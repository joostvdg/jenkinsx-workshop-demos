{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Jenkins X Demos Workshop material for Jenkins X . From Kubernetis basics, to Jenkins X Serverless and a lot of the stuff in between. Made possible by CloudBees.com","title":"Home"},{"location":"#jenkins-x-demos","text":"Workshop material for Jenkins X . From Kubernetis basics, to Jenkins X Serverless and a lot of the stuff in between. Made possible by CloudBees.com","title":"Jenkins X Demos"},{"location":"go-demo/","text":"Go Demo This is a minimal HelloWorld demo! First, make sure you have a Kubernetes cluster with Jenkins X installe Warning It is easiest to use all the default values, such as a dns on nip.io and Static Jenkins . Create Quickstart Let's examine what quickstart does. 1 jx create quickstart # Cancel with ctrl+c Cancel it with ctr+c , as it will be very interactive. Let's create the Go (lang) demo! 1 jx create quickstart -l go -p jx-go -b Info Go to github.com/jenkins-x-quickstarts to see all the available quickstarts. Open repo Replace ? with your GitHub user. 1 export GH_USER = ? 1 open https://github.com/ $GH_USER /jx-go View created files Let's take a look at what was created: 1 ls -l jx-go Dockerfile 1 cat jx-go/Dockerfile pipeline 1 cat jx-go/jenkins-x.yml Makefile 1 cat jx-go/Makefile Skaffold 1 cat jx-go/skaffold.yaml And let's take a loot at the Helm charts. Charts root 1 ls -l jx-go/charts Application Chart 1 ls -l jx-go/charts/jx-go Preview Chart 1 ls -l jx-go/charts/preview Webhook Jenkins X works with Git and wants to work event based. This means there should be a webhook, which will be send to our Jenkins X's cluster. 1 open https://github.com/ $GH_USER /jx-go/settings/hooks Releases Jenkins X will create releases for you in your Git repository (where applicable). To view them: 1 open https://github.com/ $GH_USER /jx-go/releases Explore Application in JX Jenkins UI 1 jx console Activities 1 jx get activities Acitivites jx-go 1 jx get activities -f jx-go -w # Cancel with ctrl+c Build Logs 1 jx get build logs # Cancel with ctrl+c Build Logs jx-go 1 jx get build logs -f jx-go # Cancel with ctrl+c Build Logs of Job 1 jx get build logs $GH_USER /jx-go/master General Jenkins X listings Pipelines 1 jx get pipelines Applications 1 jx get applications Applications in Env 1 jx get applications -e staging Environments 1 jx get env Update the application First, make sure the application has been build successfully and is running in our staging environment. Confirm we're ready 1 jx get activities -f jx-go -w You should see something like, after which we can continue the next step. 1 2 3 4 5 6 7 STEP STARTED AGO DURATION STATUS joostvdg/jx-go/master #1 Running Version: 0.0.1 Release 4m23s 1m0s Succeeded Promote: staging 3m23s 2m26s Succeeded PullRequest 3m23s 1m25s Succeeded PullRequest: https://github.com/joostvdg/environment-jx-staging/pull/1 Merge SHA: f602fd78694fcfef7b59b27469e0e2b8538e1bb7 Update 1m58s 1m1s Succeeded Status: Success at: http://jenkins.jx.35.231.11.119.nip.io/job/joostvdg/job/environment-jx-staging/job/master/2/display/redirect Promoted 1m58s 1m1s Succeeded Application is at: http://jx-go.jx-staging.35.231.11.119.nip.io 1 2 APP_ADDR = $( kubectl get ing -n jx-staging jx-go -o jsonpath = {.spec.rules[0].host} ) open http:// $APP_ADDR You should see a very fancy (for 1992) page which says Hello from: Jenkins X golang http example . Make the change We will now create a WIP branch. 1 git checkout -b wip Now edit our main.go file using your favorite editor - or VIM if you want. Change the title variable: title := \"Jenkins X golang http example\" to a value you like. For example: title := \"Jenkins X Is Awesome!\" . 1 2 3 git add main.go git commit -m changed our message to be awesome git push origin wip Create PR We will have to create PR for our change. When pushing to Git, you should have received a link to create a pr. If not, see below: 1 open https://github.com/ ${ GH_USER } /jx-go/pull/new/wip Keep the PR page open, you will see why! We will watch the activities to see when our preview is ready! 1 jx get activities -f jx-go -w Once we see something like Preview Application 0s http://jx-go.jx-joostvdg-jx-go-pr-1.35.231.11.119.nip.io We can go back to our PR page, which should now the link to the preview as well! Confirm your change is successful and merge the pull request by clicking the merge button. Go back to the activities feed - in case you closed it. And wait for the PR to land in staging. 1 jx get activities -f jx-go -w Once the activity Promote: staging is succeeded, we can confirm our application is updated. 1 2 APP_ADDR = $( kubectl get ing -n jx-staging jx-go -o jsonpath = {.spec.rules[0].host} ) curl http:// $APP_ADDR To wrap up, go back to the master branch and pull the changes from the PR. 1 2 git checkout master git pull Promote to Production Applications will be automatically promoted to staging, to promote them to production we have to take manual action. How do you promote manually To manually Promote a version of your application to an environment use the jx promote command. 1 jx promote --app myapp --version 1 .2.3 --env production The command waits for the promotion to complete, logging details of its progress. You can specify the timeout to wait for the promotion to complete via the --timeout argument. e.g. to wait for 5 hours 1 jx promote --app myapp --version 1 .2.3 --env production --timeout 5h You can use terms like 20m or 10h30m for the various duration expressions. To promote our jx-go application, run the following command. Promote jx-go to production 1 jx promote --app jx-go --version 0 .0.1 --env production --timeout 1h Info You will get a warning message stating Failed to query the Pull Request last commit status for , which is at this time (April 2019) expected behavior. Once the promotion is completed successfully, you should be returned to your console. Let's confirm our application landed in Production! 1 2 APP_ADDR = $( kubectl get ing -n jx-production jx-go -o jsonpath = {.spec.rules[0].host} ) curl http:// $APP_ADDR","title":"Quickstart Demo"},{"location":"go-demo/#go-demo","text":"This is a minimal HelloWorld demo! First, make sure you have a Kubernetes cluster with Jenkins X installe Warning It is easiest to use all the default values, such as a dns on nip.io and Static Jenkins .","title":"Go Demo"},{"location":"go-demo/#create-quickstart","text":"Let's examine what quickstart does. 1 jx create quickstart # Cancel with ctrl+c Cancel it with ctr+c , as it will be very interactive. Let's create the Go (lang) demo! 1 jx create quickstart -l go -p jx-go -b Info Go to github.com/jenkins-x-quickstarts to see all the available quickstarts.","title":"Create Quickstart"},{"location":"go-demo/#open-repo","text":"Replace ? with your GitHub user. 1 export GH_USER = ? 1 open https://github.com/ $GH_USER /jx-go","title":"Open repo"},{"location":"go-demo/#view-created-files","text":"Let's take a look at what was created: 1 ls -l jx-go Dockerfile 1 cat jx-go/Dockerfile pipeline 1 cat jx-go/jenkins-x.yml Makefile 1 cat jx-go/Makefile Skaffold 1 cat jx-go/skaffold.yaml And let's take a loot at the Helm charts. Charts root 1 ls -l jx-go/charts Application Chart 1 ls -l jx-go/charts/jx-go Preview Chart 1 ls -l jx-go/charts/preview","title":"View created files"},{"location":"go-demo/#webhook","text":"Jenkins X works with Git and wants to work event based. This means there should be a webhook, which will be send to our Jenkins X's cluster. 1 open https://github.com/ $GH_USER /jx-go/settings/hooks","title":"Webhook"},{"location":"go-demo/#releases","text":"Jenkins X will create releases for you in your Git repository (where applicable). To view them: 1 open https://github.com/ $GH_USER /jx-go/releases","title":"Releases"},{"location":"go-demo/#explore-application-in-jx","text":"Jenkins UI 1 jx console Activities 1 jx get activities Acitivites jx-go 1 jx get activities -f jx-go -w # Cancel with ctrl+c Build Logs 1 jx get build logs # Cancel with ctrl+c Build Logs jx-go 1 jx get build logs -f jx-go # Cancel with ctrl+c Build Logs of Job 1 jx get build logs $GH_USER /jx-go/master","title":"Explore Application in JX"},{"location":"go-demo/#general-jenkins-x-listings","text":"Pipelines 1 jx get pipelines Applications 1 jx get applications Applications in Env 1 jx get applications -e staging Environments 1 jx get env","title":"General Jenkins X listings"},{"location":"go-demo/#update-the-application","text":"First, make sure the application has been build successfully and is running in our staging environment.","title":"Update the application"},{"location":"go-demo/#confirm-were-ready","text":"1 jx get activities -f jx-go -w You should see something like, after which we can continue the next step. 1 2 3 4 5 6 7 STEP STARTED AGO DURATION STATUS joostvdg/jx-go/master #1 Running Version: 0.0.1 Release 4m23s 1m0s Succeeded Promote: staging 3m23s 2m26s Succeeded PullRequest 3m23s 1m25s Succeeded PullRequest: https://github.com/joostvdg/environment-jx-staging/pull/1 Merge SHA: f602fd78694fcfef7b59b27469e0e2b8538e1bb7 Update 1m58s 1m1s Succeeded Status: Success at: http://jenkins.jx.35.231.11.119.nip.io/job/joostvdg/job/environment-jx-staging/job/master/2/display/redirect Promoted 1m58s 1m1s Succeeded Application is at: http://jx-go.jx-staging.35.231.11.119.nip.io 1 2 APP_ADDR = $( kubectl get ing -n jx-staging jx-go -o jsonpath = {.spec.rules[0].host} ) open http:// $APP_ADDR You should see a very fancy (for 1992) page which says Hello from: Jenkins X golang http example .","title":"Confirm we're ready"},{"location":"go-demo/#make-the-change","text":"We will now create a WIP branch. 1 git checkout -b wip Now edit our main.go file using your favorite editor - or VIM if you want. Change the title variable: title := \"Jenkins X golang http example\" to a value you like. For example: title := \"Jenkins X Is Awesome!\" . 1 2 3 git add main.go git commit -m changed our message to be awesome git push origin wip","title":"Make the change"},{"location":"go-demo/#create-pr","text":"We will have to create PR for our change. When pushing to Git, you should have received a link to create a pr. If not, see below: 1 open https://github.com/ ${ GH_USER } /jx-go/pull/new/wip Keep the PR page open, you will see why! We will watch the activities to see when our preview is ready! 1 jx get activities -f jx-go -w Once we see something like Preview Application 0s http://jx-go.jx-joostvdg-jx-go-pr-1.35.231.11.119.nip.io We can go back to our PR page, which should now the link to the preview as well! Confirm your change is successful and merge the pull request by clicking the merge button. Go back to the activities feed - in case you closed it. And wait for the PR to land in staging. 1 jx get activities -f jx-go -w Once the activity Promote: staging is succeeded, we can confirm our application is updated. 1 2 APP_ADDR = $( kubectl get ing -n jx-staging jx-go -o jsonpath = {.spec.rules[0].host} ) curl http:// $APP_ADDR To wrap up, go back to the master branch and pull the changes from the PR. 1 2 git checkout master git pull","title":"Create PR"},{"location":"go-demo/#promote-to-production","text":"Applications will be automatically promoted to staging, to promote them to production we have to take manual action.","title":"Promote to Production"},{"location":"go-demo/#how-do-you-promote-manually","text":"To manually Promote a version of your application to an environment use the jx promote command. 1 jx promote --app myapp --version 1 .2.3 --env production The command waits for the promotion to complete, logging details of its progress. You can specify the timeout to wait for the promotion to complete via the --timeout argument. e.g. to wait for 5 hours 1 jx promote --app myapp --version 1 .2.3 --env production --timeout 5h You can use terms like 20m or 10h30m for the various duration expressions. To promote our jx-go application, run the following command.","title":"How do you promote manually"},{"location":"go-demo/#promote-jx-go-to-production","text":"1 jx promote --app jx-go --version 0 .0.1 --env production --timeout 1h Info You will get a warning message stating Failed to query the Pull Request last commit status for , which is at this time (April 2019) expected behavior. Once the promotion is completed successfully, you should be returned to your console. Let's confirm our application landed in Production! 1 2 APP_ADDR = $( kubectl get ing -n jx-production jx-go -o jsonpath = {.spec.rules[0].host} ) curl http:// $APP_ADDR","title":"Promote jx-go to production"},{"location":"install/","text":"Install Jenkins X Install Prerequisites Git GitBash if Windows Kubectl Helm Public cloud CLI such as gcloud Info For Windows, we recommend using the Chocolatey package manager. Info For Linux, we recommend using the Snap package manager. Info For MacOS we recommend using the Homebrew package manager. Git Debian 1 2 3 sudo apt-get update sudo apt-get upgrade sudo apt-get install git RHEL 1 2 sudo yum upgrade sudo yum install git Homebrew 1 brew install git Windows 1 2 choco install git.install choco install hub Kubectl Snap 1 2 sudo snap install kubectl --classic kubectl version Debian 1 2 3 4 5 sudo apt-get update sudo apt-get install -y apt-transport-https curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - echo deb https://apt.kubernetes.io/ kubernetes-xenial main | sudo tee -a /etc/apt/sources.list.d/kubernetes.list sudo apt-get update sudo apt-get install -y kubectl RHEL 1 2 3 4 5 6 7 8 9 10 cat EOF /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg EOF yum install -y kubectl Homebrew 1 brew install kubernetes-cli Windows 1 choco install kubernetes-cli Curl 1 2 3 curl -LO https://storage.googleapis.com/kubernetes-release/release/ $( curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt ) /bin/linux/amd64/kubectl chmod +x ./kubectl sudo mv ./kubectl /usr/local/bin/kubectl Helm Snap 1 sudo snap install helm --classic Homebrew 1 brew install kubernetes-helm Windows 1 choco install kubernetes-helm For other options, visit the install guide . Cloud CLI's AWS Snap 1 sudo snap install aws-cli --classic Homebrew 1 brew install awscli Windows 1 choco install awscli For other options, visit the install guide . EKS CTL Linux 1 2 curl --silent --location https://github.com/weaveworks/eksctl/releases/download/latest_release/eksctl_ $( uname -s ) _amd64.tar.gz | tar xz -C /tmp sudo mv /tmp/eksctl /usr/local/bin Homebrew 1 2 brew tap weaveworks/tap brew install weaveworks/tap/eksctl Windows 1 chocolatey install eksctl For other options, visit the install guide . Google Snap 1 sudo snap install google-cloud-sdk --classic Homebrew 1 brew cask install google-cloud-sdk. Windows 1 choco install gcloudsdk For other options, visit the install guide . Azure Snap 1 sudo snap install aws-cli --classic Homebrew 1 brew install awscli Windows 1 choco install azure-cli For other options, visit the install guide . Install JX Binary The jx binary is the main vehicle for Jenkins X to manage anything related to Jenkins X and your applications. Warning It is recommended to always update your jx binary at the start of your workday. It gets updates and fixes several times a day, so don't stay behind! Info If you are running on Mac OS X, Jenkins X is using Homebrew to install the various CLI. It will install it if not present. Install Linux 1 2 3 4 mkdir -p ~/.jx/bin curl -L https://github.com/jenkins-x/jx/releases/download/v1.3.1068/jx-linux-amd64.tar.gz | tar xzv -C ~/.jx/bin export PATH = $PATH :~/.jx/bin echo export PATH=$PATH:~/.jx/bin ~/.bashrc RHEL 1 2 sudo yum upgrade sudo yum install git Homebrew 1 2 brew tap jenkins-x/jx brew install jx Windows 1 choco install jenkins-x Install JX w/ Cluster You can install Jenkins X in an existing Kubernetes cluster, or let it install a cluster for you. Below are the examples for installing a cluster via Jenkins X before it installs itself into it. Info For all the installation options, please consult the jx CLI. jx create cluster ${clusterType} --help When in doubt, accept the default value! Variables Set these variables in your console, for use with the install commands. CLUSTER_NAME your desired cluster name (can be anything) PROJECT is your Google Cloud project-id you can retrieve this via your console or via the gcloud CLI: gcloud config list 1 2 3 4 5 6 7 REGION = us-east1 ZONE = ${ REGION } -b PREFIX = jx MACHINE_TYPE = n1-standard-2 ADMIN_PSW = admin CLUSTER_NAME = PROJECT = Configuration Warning If you are using Java with Maven or Gradle, you'd want to install Nexus. Else, you can disable it as per example below. Disable Nexus Copy below text into a file called myvalues.yaml . 1 2 nexus: enabled: false Install Info EKS option will download and use the eksctl tool to create a new EKS cluster, then it\u2019ll install Jenkins X on top. Info When you're creating your first cluster with GKE, you will need to login for authorization. If you have authorization taken care of, you can add --skip-login to prevent the process. GKE 1 2 3 4 jx create cluster gke -n $CLUSTER_NAME -p $PROJECT -z ${ ZONE } \\ -m ${ MACHINE_TYPE } --min-num-nodes 3 --max-num-nodes 5 \\ --default-admin-password = ${ ADMIN_PSW } \\ --default-environment-prefix ${ PREFIX } --no-tiller EKS 1 jx create cluster eks Kops 1 jx create cluster aws Azure 1 jx create cluster aks For more options and information, read the documentation . Test install 1 kubectl -n jx get pods Install Serverless Variables 1 2 3 4 5 6 7 CLUSTER_NAME = PROJECT = ADMIN_PSW = admin PREFIX = jx REGION = us-east1 ZONE = ${ REGION } -a MACHINE_TYPE = n1-standard-1 Install GKE 1 2 3 4 5 jx create cluster gke -n $CLUSTER_NAME -p $PROJECT -z ${ ZONE } \\ -m n1-standard-2 --min-num-nodes 3 --max-num-nodes 5 \\ --default-admin-password = ${ ADMIN_PSW } \\ --default-environment-prefix \\ --prow --tekton --no-tiller","title":"Install"},{"location":"install/#install-jenkins-x","text":"","title":"Install Jenkins X"},{"location":"install/#install-prerequisites","text":"Git GitBash if Windows Kubectl Helm Public cloud CLI such as gcloud Info For Windows, we recommend using the Chocolatey package manager. Info For Linux, we recommend using the Snap package manager. Info For MacOS we recommend using the Homebrew package manager.","title":"Install Prerequisites"},{"location":"install/#git","text":"Debian 1 2 3 sudo apt-get update sudo apt-get upgrade sudo apt-get install git RHEL 1 2 sudo yum upgrade sudo yum install git Homebrew 1 brew install git Windows 1 2 choco install git.install choco install hub","title":"Git"},{"location":"install/#kubectl","text":"Snap 1 2 sudo snap install kubectl --classic kubectl version Debian 1 2 3 4 5 sudo apt-get update sudo apt-get install -y apt-transport-https curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - echo deb https://apt.kubernetes.io/ kubernetes-xenial main | sudo tee -a /etc/apt/sources.list.d/kubernetes.list sudo apt-get update sudo apt-get install -y kubectl RHEL 1 2 3 4 5 6 7 8 9 10 cat EOF /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg EOF yum install -y kubectl Homebrew 1 brew install kubernetes-cli Windows 1 choco install kubernetes-cli Curl 1 2 3 curl -LO https://storage.googleapis.com/kubernetes-release/release/ $( curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt ) /bin/linux/amd64/kubectl chmod +x ./kubectl sudo mv ./kubectl /usr/local/bin/kubectl","title":"Kubectl"},{"location":"install/#helm","text":"Snap 1 sudo snap install helm --classic Homebrew 1 brew install kubernetes-helm Windows 1 choco install kubernetes-helm For other options, visit the install guide .","title":"Helm"},{"location":"install/#cloud-clis","text":"","title":"Cloud CLI's"},{"location":"install/#aws","text":"Snap 1 sudo snap install aws-cli --classic Homebrew 1 brew install awscli Windows 1 choco install awscli For other options, visit the install guide .","title":"AWS"},{"location":"install/#eks-ctl","text":"Linux 1 2 curl --silent --location https://github.com/weaveworks/eksctl/releases/download/latest_release/eksctl_ $( uname -s ) _amd64.tar.gz | tar xz -C /tmp sudo mv /tmp/eksctl /usr/local/bin Homebrew 1 2 brew tap weaveworks/tap brew install weaveworks/tap/eksctl Windows 1 chocolatey install eksctl For other options, visit the install guide .","title":"EKS CTL"},{"location":"install/#google","text":"Snap 1 sudo snap install google-cloud-sdk --classic Homebrew 1 brew cask install google-cloud-sdk. Windows 1 choco install gcloudsdk For other options, visit the install guide .","title":"Google"},{"location":"install/#azure","text":"Snap 1 sudo snap install aws-cli --classic Homebrew 1 brew install awscli Windows 1 choco install azure-cli For other options, visit the install guide .","title":"Azure"},{"location":"install/#install-jx-binary","text":"The jx binary is the main vehicle for Jenkins X to manage anything related to Jenkins X and your applications. Warning It is recommended to always update your jx binary at the start of your workday. It gets updates and fixes several times a day, so don't stay behind! Info If you are running on Mac OS X, Jenkins X is using Homebrew to install the various CLI. It will install it if not present.","title":"Install JX Binary"},{"location":"install/#install","text":"Linux 1 2 3 4 mkdir -p ~/.jx/bin curl -L https://github.com/jenkins-x/jx/releases/download/v1.3.1068/jx-linux-amd64.tar.gz | tar xzv -C ~/.jx/bin export PATH = $PATH :~/.jx/bin echo export PATH=$PATH:~/.jx/bin ~/.bashrc RHEL 1 2 sudo yum upgrade sudo yum install git Homebrew 1 2 brew tap jenkins-x/jx brew install jx Windows 1 choco install jenkins-x","title":"Install"},{"location":"install/#install-jx-w-cluster","text":"You can install Jenkins X in an existing Kubernetes cluster, or let it install a cluster for you. Below are the examples for installing a cluster via Jenkins X before it installs itself into it. Info For all the installation options, please consult the jx CLI. jx create cluster ${clusterType} --help When in doubt, accept the default value!","title":"Install JX w/ Cluster"},{"location":"install/#variables","text":"Set these variables in your console, for use with the install commands. CLUSTER_NAME your desired cluster name (can be anything) PROJECT is your Google Cloud project-id you can retrieve this via your console or via the gcloud CLI: gcloud config list 1 2 3 4 5 6 7 REGION = us-east1 ZONE = ${ REGION } -b PREFIX = jx MACHINE_TYPE = n1-standard-2 ADMIN_PSW = admin CLUSTER_NAME = PROJECT =","title":"Variables"},{"location":"install/#configuration","text":"Warning If you are using Java with Maven or Gradle, you'd want to install Nexus. Else, you can disable it as per example below.","title":"Configuration"},{"location":"install/#disable-nexus","text":"Copy below text into a file called myvalues.yaml . 1 2 nexus: enabled: false","title":"Disable Nexus"},{"location":"install/#install_1","text":"Info EKS option will download and use the eksctl tool to create a new EKS cluster, then it\u2019ll install Jenkins X on top. Info When you're creating your first cluster with GKE, you will need to login for authorization. If you have authorization taken care of, you can add --skip-login to prevent the process. GKE 1 2 3 4 jx create cluster gke -n $CLUSTER_NAME -p $PROJECT -z ${ ZONE } \\ -m ${ MACHINE_TYPE } --min-num-nodes 3 --max-num-nodes 5 \\ --default-admin-password = ${ ADMIN_PSW } \\ --default-environment-prefix ${ PREFIX } --no-tiller EKS 1 jx create cluster eks Kops 1 jx create cluster aws Azure 1 jx create cluster aks For more options and information, read the documentation .","title":"Install"},{"location":"install/#test-install","text":"1 kubectl -n jx get pods","title":"Test install"},{"location":"install/#install-serverless","text":"","title":"Install Serverless"},{"location":"install/#variables_1","text":"1 2 3 4 5 6 7 CLUSTER_NAME = PROJECT = ADMIN_PSW = admin PREFIX = jx REGION = us-east1 ZONE = ${ REGION } -a MACHINE_TYPE = n1-standard-1","title":"Variables"},{"location":"install/#install_2","text":"GKE 1 2 3 4 5 jx create cluster gke -n $CLUSTER_NAME -p $PROJECT -z ${ ZONE } \\ -m n1-standard-2 --min-num-nodes 3 --max-num-nodes 5 \\ --default-admin-password = ${ ADMIN_PSW } \\ --default-environment-prefix \\ --prow --tekton --no-tiller","title":"Install"},{"location":"to-be-processed/","text":"Jenkins X Introduction Notes from Slack Tips Tricks https://github.com/jenkinsci/kubernetes-credentials-provider-plugin volume storage with Heptio's Valerio debugging: https://jenkins-x.io/contribute/development/#debugging --gitops mode have to check this jx upgrade platform knative authentication --git-username --org are complimentary, --organisations is unrelated username = the user for the repo's (apps env) org = the organization for the repo's (apps env) e.g.: --git-username joostvdg --org demomon --organisations is used to query GitHub for Quickstarts (no need to specify unless you have alternatives) Jenkins configurations do not persist you can specify them in a ConfigMap though https://github.com/jenkins-x/charts/blob/jenkins/stable/jenkins/templates/config.yaml#L8 credentials used for config can be found here: ~/.jx/jenkinsAuth.yam PodTemplates (static Jenkins) are from the Jenkins Kubernetes Plugin cleanup GKE jx gc gke you can create separate teams with --no-tiller even if the installation was done with Tiller jx init to \"fix\" a outdated ~/.jx folder https://jenkins-x.io/commands/jx_step_credential/ don't do mono repo's why? - https://medium.com/@mattklein123/monorepos-please-dont-e9a279be011b but if you do, https://fuchsia.googlesource.com/jiri/ using a different git provider --git-provider-url .... --git-provider-kind bitbucketserver --git-username foo --git-api-token whatever https://jenkins-x.io/developing/git/#using-a-different-git-provider-for-environments jx start pipeline to manually trigger a pipeline (I assume static Jenkins only) enable GCS for chartmuseum backend https://github.com/jenkins-x/cloud-environments/blob/master/env-jx-infra/myvalues.yaml#L10-L17 jx wraps kubectx tool, so you can use jx ns namespace to change your context to a different namespace faq for diagnosing exposecontroller issues: https://jenkins-x.io/faq/issues/#how-can-i-diagnose-exposecontroller-issues controller is used to generate ingress resources https://github.com/lvlstudio/jenkins-x-builders/tree/master/builder-nodejs-mysql https://github.com/jenkins-x/jx/issues/2550 https://github.com/jenkins-x/jenkins-x-platform/issues/4768 Tillerless: https://jenkins-x.io/news/helm-without-tiller/ difference between jx create quickstart and selecting spring vs. jx create spring jx create spring is an interactive wizard that uses the spring initialiser https://start.spring.io/ jx create quickstart uses a configurable github org to list available existing quickstarts i.e. https://github.com/jenkins-x-quickstarts (edited) multi-cluster support: https://github.com/jenkins-x-charts/environment-controller https://github.com/jenkins-x/jx/issues/479 jx create user ? https://jenkins-x.io/commands/jx_create_jenkins_token/ for problems with wild card certificates doing only one segment (i.e., *.example.com instead of *.*.example.com ) no - we can tweak that. It\u2019s easiest with wildcard - then any exposed service at svc.ns.domain just works - but you could register each namespace in DNS we\u2019ve not exposed that property to the jx install CLI yet - but you could try kubectl edit cm ingress-config urltemplate: \"{{.Service}}-{{.Namespace}}.{{.Domain}}\" and then jx upgrade ingress doesn't seem to work yet? (customization gets reverted) alternative, add dns entries to the ingress resources https://jenkins-x.io/getting-started/install-on-cluster/#installing-jenkins-x-on-premise Helm tips tricks for changing secrets https://github.com/helm/helm/blob/master/docs/charts_tips_and_tricks.md#user-content-automatically-roll-deployments-when-configmaps-or-secrets-change how to run integration tests I've answered this in a previous thread. Basically you use the helm chart (which includes the service dependencies as requirements). You create a preview but set the replicaCount for your service to 0 (that way, just the requirements are started). Then just run your tests against the requirements and delete the preview afterwards. Search the channel history for my messages and replicaCount. You should find it. https://jenkins-x.io/faq/develop/#how-do-i-add-other-services-into-a-preview managing static jenkins config has some issues https://github.com/jenkins-x/jx/issues/2991 https://github.com/jenkins-x/jx-docs/issues/1039 https://github.com/helm/charts/pull/9296/files we\u2019re super close from recommending folks use jx create cluster ... --vault --gitops which uses a git repository to store all the configuration changes + versions of stuff - and uses Vault to store all secrets we just merged the last few fixes so it should work for static jenkins servers with the latest jx binary - feb 7 Info It is Ready: jx create cluster gke --vault --gitops --no-tiller Multiple Micro-services if 1 microservice was 1 helm chart with a few different containers for example; you may have a few repos that just make binaries/docker images - then 1 repo which contains the helm chart of the microservice - you can also easily combine microservices together into an uber helm chart - James Strachan we prefer to use multiple repositories so that things are more microservice based. The problem with monorepos is everything gets released on every change; with separate repos its easier to manage change etc to handle changing versions of things across repositories we use updatebot ourselves to do \u2018CI/CD of dependencies\u2019 - we kinda think of it as promotion of dependencies like we promote microservices into environments - its PRs generated as part of the release process the main decision to make really is, , if you have, say, 3 microservices that are fairly tightly coupled; do you combine 3 versions of them all into 1 chart and then release that 1 chart when its all tested together; or do you release the 3 things totally independently - it depeends on coupling and team structure really you can switch from one to the other at any time really https://github.com/jenkins-x/updatebot 1 2 3 4 5 6 7 8 9 10 11 12 # Setup: $ jx create cluster gke -n team1 --default-environment-prefix team1 $ jx create quickstart -p app1 $ jx create quickstart -p app2 $ jx create quickstart -p app3 # The 5 repos: environment-team1-staging environment-team1-production app1 app2 app3 Practices Principles Principles Faster time to market Improved deployment frequency Shorter time between fixes Lower failure rate of releases Faster Mean Time To Recovery Practices Loosely-coupled Architectures Self-service Configuration Automated Provisioning Continuous Build / Integration and Delivery Automated Release Management Incremental Testing Infrastructure Configuration as Code Comprehensive configuration management Trunk based development and feature flags Prerequisites Primary jx Git Docker Kubernetes Helm Secondary Nexus GitOps Skaffold KNative Prow KSync JX Git Docker Kubernetes any public cloud will do GKE is recommended Validate Cluster for Jenkins X Compliance You can read this article from Viktor Farcic on how to confirm if your cluster is compliant with Jenkins X's requirements. Jenkins X has a binary, called jx , which includes some facilities from the Sonobuoy SDK to provide some validation capabilities. Run compliance check To run the compliance check, just use the jx command below. 1 jx compliance run Warning The compliance check will run for about one hour! Check Compliance run status 1 jx compliance status Check Compliance run logs 1 jx compliance logs -f See Compliance run results 1 jx compliance results Cleanup Once you're done with the compliance run, you can clean up any resources it created for the run. 1 jx compliance delete Helm package manager for Kubernetes quivalent to yum install package Kubernetes manifest template (Go templates), packaged and versioned, reffered to as Charts Use Cases manages packages, repositories and installations manage dependencies to other Charts meaning: you can create bundles allows for customizing standardized installations on Kubernetes for example: MySQL database with specific database, users and settings used as building blocks in other tools separate packaging (Chart) from runtime (application image) there's the chart version and the application version Architecture Helm repository = a hosted index.yaml client ( helm ) and server ( tiller ) component Chartmuseum lightweight Helm chart repository alternatives are Artifactory, Nexus 3, but are heavy \"support all package types\" kind of tools supports different storage options local (e.g., Kubernetes volume) S3 / Minio Google Cloud Storage Azure Blob Store Alibaba Cloud OSS storage Openstack Object Storage Monocular UI for Helm repositories includes documentation and searching can give insight into charts installed in the cluster via helm GitOps https://www.weave.works/blog/gitops-operations-by-pull-request https://www.weave.works/blog/what-is-gitops-really https://www.cloudbees.com/blog/gitops-dev-dash-ops https://developer.atlassian.com/blog/2017/07/kubernetes-workflow/ Thoughts declarative specification for each environment auditable changes ability to verify live what is vs. what should be aside from autoscaling and the like for example, with Kubediff reproducable environments helps automate and spead up Segragation of Duties use PullRequests for change management eventual consistency via event based reconciliation e.g., Git commit event - pipeline - update based on spec Jenkins X - Introduction The challenges Jenkins X tries to solve: good way to setup Kubernetes environments containerize your applications deploy containerized applications to Kubernetes adopt Continuous Delivery / Progressive Delivery base platform for automation keeping focus on delivering value instead of HOW to deliver the value help embed proven practices from the State of DevOps Report How automates installations of all the basic building blocks for CI, CD and PD helm, skaffold, kaniko, jenkins, ksync, knative, nexus, monocular, chartmuseum,... pre-configured, ready to go automates CI/CD setup (PD setup is coming) Docker image Helm chart (Jenkins) Pipeline Event trigger management (e.g., GitHub event triggers for Pipelines) pre-configured GitOps environments + pipelines for managing environment promotion feedback interaction logs, notification hooks, caches labels and comments on GitHub issue / PR PR chatbot Jenkins X - Installation There's four ways to install Jenkins X. install in an existing public cloud Kubernetes cluster jx install create cluster and installation via public cloud's CLI (e.g., gcloud ) create cluster and installation via Terraform (where applicable) install Jenkins X in an on-premise Kubernetes cluster (requirements apply) During the installation, the following things will be done via the jx binary. If you're using Mac OS X, the jx binary will install any missing tool via homebrew . install Helm install cloud provider cli install kubectl create cluster (unless an install only option is used) create Jenkins X namespace install Tiller (unless helm 3 or tillerless is specified) setup basic ingress controller install several CI/CD tools chartmuseum docker-registry jenkins monocular nexus configure git source repository create admin secrets The installation can be configured with flags in order to customize how each step is executed. Default Terraform Install only On-Premis Next Steps create and app via quickstart import and existing app customize your Jenkins X installation Jenkins X - IDE Integration You read all about Jenkins X's IDE integration in the docs . Visual Studio Code Visual Studio Code is a popular open source IDE from Microsoft. The Jenkins X team created the vscode-jx-tools extension for VS Code. Intelli J There's a plugin for IntelliJ and the associated IDEs like WebStorm, GoLand, PyCharm et al from JetBrains . You can find the Jenkins X plugin for IntelliJ here . Jenkins X - Customization Installation Parameters Component Configuration During installation (incl. or excl. cluster creation) Jenkins X will read a myvalues.yaml file in the current directory to configure its core components. You can read more about all the options here , but below are some examples. Nexus You might not want to include Nexus in your installation, the snippet below will exclude it from being installed. 1 2 3 4 5 nexus : enabled : false nexusServiceLink : enabled : true externalName : nexus.jx.svc.cluster.local Chartmuseum Add the below snippet in order to skip installing Chartmuseum. 1 2 3 4 5 chartmuseum : enabled : false chartmuseumServiceLink : enabled : true externalName : jenkins-x-chartmuseum.jx.svc.cluster.local Jenkins image used When using the static Jenkins type of installation, Jenkins X uses the jenkinsxio/jenkinsx docker image. You specify an alternative image in the myvalues.yaml . 1 2 3 4 jenkins : Master : Image : acme/my-jenkinsx ImageTag : 1.2.3 For how to create your custom Jenkins image, read the Jenkins X docs . Alternative Docker Registry Currently - March 2019 - you can only specify the docker registry during the installation ( jx create cluster or jx install ) via a flag. 1 jx create cluster gke --docker-registry eu.gcr.io You will have to configure the docker authentication secret as well. How to do this, you can read in the Jenkins X documentation . Jenkins X - Basic Usage import quickstart console pipelines promote environments applications teams Jenkins X - Other Features Teams https://jenkins-x.io/about/features/#teams JX Shell Create a sub shell so that changes to the Kubernetes context, namespace or environment remain local to the shell 1 2 # create a new shell using a specific named context jx shell prod-cluster JX Prompt Generate the command line prompt for the current team and environment current 1 2 # Generate the current prompt jx prompt bash 1 2 # Enable the prompt for bash PS1 = [\\u@\\h \\W \\$(jx prompt)]\\$ zsh 1 2 # Enable the prompt for zsh PROMPT = $(jx prompt) $PROMPT Issue Tracker Jenkins X can work with issue trackers such as GitHub issues and Jira. By default, Jenkins X will use GitHub for projects and issues. So if you haven't specified anything for either Git provider or Issue tracker, it will use GitHub for issues. GitHub 1 jx create issue -t lets make things more awesome 1 jx get issues Jira In order to configure Jenkins X to use Jira as issue tracker, you have to do three steps. create a tracker configuration jx create tracker server ${trackerName} https://mycompany.atlassian.net/ create a tracker login jx create tracker token -n ${trackerName} myEmailAddress configure your Jenkins X managed project to use this issue tracker instead jx edit config -k issues Info A file called jenkins-x.yml will be modified in your project source code which should be added to your git repository. Security Jenkins X has direct support for some security analysis . Anchore image scanning The Anchore Engine is used to provide image security, by examining contents of containers either in pull request/review state, or on running containers. This was introduced in this blog post . Here is a video demonstrating it live. 1 jx create addon anchore To see if it found any problems in a specific environment: 1 jx get cve --environment = staging OWASP ZAP ZAP or Zed Attack Proxy allows you to scan the public surface of your application for any known vulnerability. 1 jx create addon owasp-zap Jenkins X - Development Cycle Versioning semver semantics using git tags https://github.com/jenkins-x/jx-release-version https://www.cloudbees.com/blog/automatically-versioning-your-application-jenkins-x DevPod https://jenkins-x.io/developing/devpods/ Custom Builder https://jenkins-x.io/getting-started/create-custom-builder/ Preview A new preview is created when you create a PullRequest (PR) on a Jenkins X managed application. builds the application packages it into a Helm chart creates a unique Kubernetes namespace -only on first build deploys the application into the namespace adds a pull request comment with preview environment URL https://jenkins-x.io/about/features/#preview-environments https://jenkins-x.io/developing/preview/#adding-more-resources https://medium.com/@vbehar/zero-cost-preview-environments-on-kubernetes-with-jenkins-x-and-osiris-bd9ce0148d03 https://medium.com/@MichalFoksa/jenkins-x-preview-environment-3bf2424a05e4 Get current preview environments 1 jx get previews Post Preview hook Jenkins X allows you to extend it at several points. One such extension point is the preview process. You can extends the preview process with a post preview hook . Add dependencies Your application might depend on other services or facilities, that are generally present in your staging and production environment. With the move to clusters, these can have become a cluster function, These cluster functions might not be available in your temporary preview environment. Or you want to test your application against multiple versions of its dependencies. So you need the ability to specify these dependencies for the preview of your app which should only be used for the preview environment. This can be done in two ways. link Kubernetes service from other environment /namespace create an instance of a dependency in the preview environment through Helm chart. Link to a service Via the jx step link service you connect your preview app with a Jenkins X managed service elsewhere in the same cluster. Example: 1 jx step link services --from-namespace jx-staging --includes * --excludes cheese* Add helm chart dependency The preview environment has its own helm chart, in the folder charts/preview . You can add dependencies in here, just like in any other helm chart. Just make sure the chart ends with the dependency on your app (in charts/appName ) and an empty line, as the file's comment says. Promotion https://jenkins-x.io/faq/develop/#how-does-promotion-actually-work https://jenkins-x.io/developing/promote/ Jenkins X Secrets https://github.com/futuresimple/helm-secrets https://developer.epages.com/blog/tech-stories/kubernetes-deployments-with-helm-secrets/ vault operator / addon? secrets for preview example: https://github.com/Zenika/snowcamp-2019-sncf-timesheet-reader or this: https://github.com/Riduidel/snowcamp-2019 thanks! helm secrets was the only choice available initially but we\u2019re moving more towards using the vault operator instead - though we need some more docs and demos to show how to use secrets in vault from a Preview or Staging environment - James Strachan Extending Jenkins X https://jenkins-x.io/extending/ Jenkins X CloudBees CodeShip Jenkins X The Future Serverless Jenkins Tekton aka Next Gen(eration) Pipeline Serverless https://medium.com/@jdrawlings/serverless-jenkins-with-jenkins-x-9134cbfe6870 https://github.com/jenkinsci/jenkinsfile-runner Next Gen Pipeline https://www.cloudbees.com/blog/move-toward-next-generation-pipelines https://jenkins-x.io/news/jenkins-x-next-gen-pipeline-engine/ https://github.com/tektoncd/pipeline#-tekton-pipelines https://github.com/jenkins-x/jx/issues/3225 https://jenkins-x.io/getting-started/next-gen-pipeline/ https://github.com/jenkins-x/jx/issues/3223","title":"Jenkins X Introduction"},{"location":"to-be-processed/#jenkins-x-introduction","text":"","title":"Jenkins X Introduction"},{"location":"to-be-processed/#notes-from-slack","text":"","title":"Notes from Slack"},{"location":"to-be-processed/#tips-tricks","text":"https://github.com/jenkinsci/kubernetes-credentials-provider-plugin volume storage with Heptio's Valerio debugging: https://jenkins-x.io/contribute/development/#debugging --gitops mode have to check this jx upgrade platform knative authentication --git-username --org are complimentary, --organisations is unrelated username = the user for the repo's (apps env) org = the organization for the repo's (apps env) e.g.: --git-username joostvdg --org demomon --organisations is used to query GitHub for Quickstarts (no need to specify unless you have alternatives) Jenkins configurations do not persist you can specify them in a ConfigMap though https://github.com/jenkins-x/charts/blob/jenkins/stable/jenkins/templates/config.yaml#L8 credentials used for config can be found here: ~/.jx/jenkinsAuth.yam PodTemplates (static Jenkins) are from the Jenkins Kubernetes Plugin cleanup GKE jx gc gke you can create separate teams with --no-tiller even if the installation was done with Tiller jx init to \"fix\" a outdated ~/.jx folder https://jenkins-x.io/commands/jx_step_credential/ don't do mono repo's why? - https://medium.com/@mattklein123/monorepos-please-dont-e9a279be011b but if you do, https://fuchsia.googlesource.com/jiri/ using a different git provider --git-provider-url .... --git-provider-kind bitbucketserver --git-username foo --git-api-token whatever https://jenkins-x.io/developing/git/#using-a-different-git-provider-for-environments jx start pipeline to manually trigger a pipeline (I assume static Jenkins only) enable GCS for chartmuseum backend https://github.com/jenkins-x/cloud-environments/blob/master/env-jx-infra/myvalues.yaml#L10-L17 jx wraps kubectx tool, so you can use jx ns namespace to change your context to a different namespace faq for diagnosing exposecontroller issues: https://jenkins-x.io/faq/issues/#how-can-i-diagnose-exposecontroller-issues controller is used to generate ingress resources https://github.com/lvlstudio/jenkins-x-builders/tree/master/builder-nodejs-mysql https://github.com/jenkins-x/jx/issues/2550 https://github.com/jenkins-x/jenkins-x-platform/issues/4768 Tillerless: https://jenkins-x.io/news/helm-without-tiller/ difference between jx create quickstart and selecting spring vs. jx create spring jx create spring is an interactive wizard that uses the spring initialiser https://start.spring.io/ jx create quickstart uses a configurable github org to list available existing quickstarts i.e. https://github.com/jenkins-x-quickstarts (edited) multi-cluster support: https://github.com/jenkins-x-charts/environment-controller https://github.com/jenkins-x/jx/issues/479 jx create user ? https://jenkins-x.io/commands/jx_create_jenkins_token/ for problems with wild card certificates doing only one segment (i.e., *.example.com instead of *.*.example.com ) no - we can tweak that. It\u2019s easiest with wildcard - then any exposed service at svc.ns.domain just works - but you could register each namespace in DNS we\u2019ve not exposed that property to the jx install CLI yet - but you could try kubectl edit cm ingress-config urltemplate: \"{{.Service}}-{{.Namespace}}.{{.Domain}}\" and then jx upgrade ingress doesn't seem to work yet? (customization gets reverted) alternative, add dns entries to the ingress resources https://jenkins-x.io/getting-started/install-on-cluster/#installing-jenkins-x-on-premise Helm tips tricks for changing secrets https://github.com/helm/helm/blob/master/docs/charts_tips_and_tricks.md#user-content-automatically-roll-deployments-when-configmaps-or-secrets-change how to run integration tests I've answered this in a previous thread. Basically you use the helm chart (which includes the service dependencies as requirements). You create a preview but set the replicaCount for your service to 0 (that way, just the requirements are started). Then just run your tests against the requirements and delete the preview afterwards. Search the channel history for my messages and replicaCount. You should find it. https://jenkins-x.io/faq/develop/#how-do-i-add-other-services-into-a-preview managing static jenkins config has some issues https://github.com/jenkins-x/jx/issues/2991 https://github.com/jenkins-x/jx-docs/issues/1039 https://github.com/helm/charts/pull/9296/files we\u2019re super close from recommending folks use jx create cluster ... --vault --gitops which uses a git repository to store all the configuration changes + versions of stuff - and uses Vault to store all secrets we just merged the last few fixes so it should work for static jenkins servers with the latest jx binary - feb 7 Info It is Ready: jx create cluster gke --vault --gitops --no-tiller","title":"Tips &amp; Tricks"},{"location":"to-be-processed/#multiple-micro-services","text":"if 1 microservice was 1 helm chart with a few different containers for example; you may have a few repos that just make binaries/docker images - then 1 repo which contains the helm chart of the microservice - you can also easily combine microservices together into an uber helm chart - James Strachan we prefer to use multiple repositories so that things are more microservice based. The problem with monorepos is everything gets released on every change; with separate repos its easier to manage change etc to handle changing versions of things across repositories we use updatebot ourselves to do \u2018CI/CD of dependencies\u2019 - we kinda think of it as promotion of dependencies like we promote microservices into environments - its PRs generated as part of the release process the main decision to make really is, , if you have, say, 3 microservices that are fairly tightly coupled; do you combine 3 versions of them all into 1 chart and then release that 1 chart when its all tested together; or do you release the 3 things totally independently - it depeends on coupling and team structure really you can switch from one to the other at any time really https://github.com/jenkins-x/updatebot 1 2 3 4 5 6 7 8 9 10 11 12 # Setup: $ jx create cluster gke -n team1 --default-environment-prefix team1 $ jx create quickstart -p app1 $ jx create quickstart -p app2 $ jx create quickstart -p app3 # The 5 repos: environment-team1-staging environment-team1-production app1 app2 app3","title":"Multiple Micro-services"},{"location":"to-be-processed/#practices-principles","text":"","title":"Practices &amp; Principles"},{"location":"to-be-processed/#principles","text":"Faster time to market Improved deployment frequency Shorter time between fixes Lower failure rate of releases Faster Mean Time To Recovery","title":"Principles"},{"location":"to-be-processed/#practices","text":"Loosely-coupled Architectures Self-service Configuration Automated Provisioning Continuous Build / Integration and Delivery Automated Release Management Incremental Testing Infrastructure Configuration as Code Comprehensive configuration management Trunk based development and feature flags","title":"Practices"},{"location":"to-be-processed/#prerequisites","text":"Primary jx Git Docker Kubernetes Helm Secondary Nexus GitOps Skaffold KNative Prow KSync","title":"Prerequisites"},{"location":"to-be-processed/#jx","text":"","title":"JX"},{"location":"to-be-processed/#git","text":"","title":"Git"},{"location":"to-be-processed/#docker","text":"","title":"Docker"},{"location":"to-be-processed/#kubernetes","text":"any public cloud will do GKE is recommended","title":"Kubernetes"},{"location":"to-be-processed/#validate-cluster-for-jenkins-x-compliance","text":"You can read this article from Viktor Farcic on how to confirm if your cluster is compliant with Jenkins X's requirements. Jenkins X has a binary, called jx , which includes some facilities from the Sonobuoy SDK to provide some validation capabilities.","title":"Validate Cluster for Jenkins X Compliance"},{"location":"to-be-processed/#run-compliance-check","text":"To run the compliance check, just use the jx command below. 1 jx compliance run Warning The compliance check will run for about one hour!","title":"Run compliance check"},{"location":"to-be-processed/#check-compliance-run-status","text":"1 jx compliance status","title":"Check Compliance run status"},{"location":"to-be-processed/#check-compliance-run-logs","text":"1 jx compliance logs -f","title":"Check Compliance run logs"},{"location":"to-be-processed/#see-compliance-run-results","text":"1 jx compliance results","title":"See Compliance run results"},{"location":"to-be-processed/#cleanup","text":"Once you're done with the compliance run, you can clean up any resources it created for the run. 1 jx compliance delete","title":"Cleanup"},{"location":"to-be-processed/#helm","text":"package manager for Kubernetes quivalent to yum install package Kubernetes manifest template (Go templates), packaged and versioned, reffered to as Charts","title":"Helm"},{"location":"to-be-processed/#use-cases","text":"manages packages, repositories and installations manage dependencies to other Charts meaning: you can create bundles allows for customizing standardized installations on Kubernetes for example: MySQL database with specific database, users and settings used as building blocks in other tools separate packaging (Chart) from runtime (application image) there's the chart version and the application version","title":"Use Cases"},{"location":"to-be-processed/#architecture","text":"Helm repository = a hosted index.yaml client ( helm ) and server ( tiller ) component","title":"Architecture"},{"location":"to-be-processed/#chartmuseum","text":"lightweight Helm chart repository alternatives are Artifactory, Nexus 3, but are heavy \"support all package types\" kind of tools supports different storage options local (e.g., Kubernetes volume) S3 / Minio Google Cloud Storage Azure Blob Store Alibaba Cloud OSS storage Openstack Object Storage","title":"Chartmuseum"},{"location":"to-be-processed/#monocular","text":"UI for Helm repositories includes documentation and searching can give insight into charts installed in the cluster via helm","title":"Monocular"},{"location":"to-be-processed/#gitops","text":"https://www.weave.works/blog/gitops-operations-by-pull-request https://www.weave.works/blog/what-is-gitops-really https://www.cloudbees.com/blog/gitops-dev-dash-ops https://developer.atlassian.com/blog/2017/07/kubernetes-workflow/","title":"GitOps"},{"location":"to-be-processed/#thoughts","text":"declarative specification for each environment auditable changes ability to verify live what is vs. what should be aside from autoscaling and the like for example, with Kubediff reproducable environments helps automate and spead up Segragation of Duties use PullRequests for change management eventual consistency via event based reconciliation e.g., Git commit event - pipeline - update based on spec","title":"Thoughts"},{"location":"to-be-processed/#jenkins-x-introduction_1","text":"The challenges Jenkins X tries to solve: good way to setup Kubernetes environments containerize your applications deploy containerized applications to Kubernetes adopt Continuous Delivery / Progressive Delivery base platform for automation keeping focus on delivering value instead of HOW to deliver the value help embed proven practices from the State of DevOps Report","title":"Jenkins X - Introduction"},{"location":"to-be-processed/#how","text":"automates installations of all the basic building blocks for CI, CD and PD helm, skaffold, kaniko, jenkins, ksync, knative, nexus, monocular, chartmuseum,... pre-configured, ready to go automates CI/CD setup (PD setup is coming) Docker image Helm chart (Jenkins) Pipeline Event trigger management (e.g., GitHub event triggers for Pipelines) pre-configured GitOps environments + pipelines for managing environment promotion feedback interaction logs, notification hooks, caches labels and comments on GitHub issue / PR PR chatbot","title":"How"},{"location":"to-be-processed/#jenkins-x-installation","text":"There's four ways to install Jenkins X. install in an existing public cloud Kubernetes cluster jx install create cluster and installation via public cloud's CLI (e.g., gcloud ) create cluster and installation via Terraform (where applicable) install Jenkins X in an on-premise Kubernetes cluster (requirements apply) During the installation, the following things will be done via the jx binary. If you're using Mac OS X, the jx binary will install any missing tool via homebrew . install Helm install cloud provider cli install kubectl create cluster (unless an install only option is used) create Jenkins X namespace install Tiller (unless helm 3 or tillerless is specified) setup basic ingress controller install several CI/CD tools chartmuseum docker-registry jenkins monocular nexus configure git source repository create admin secrets The installation can be configured with flags in order to customize how each step is executed.","title":"Jenkins X - Installation"},{"location":"to-be-processed/#default","text":"","title":"Default"},{"location":"to-be-processed/#terraform","text":"","title":"Terraform"},{"location":"to-be-processed/#install-only","text":"","title":"Install only"},{"location":"to-be-processed/#on-premis","text":"","title":"On-Premis"},{"location":"to-be-processed/#next-steps","text":"create and app via quickstart import and existing app customize your Jenkins X installation","title":"Next Steps"},{"location":"to-be-processed/#jenkins-x-ide-integration","text":"You read all about Jenkins X's IDE integration in the docs .","title":"Jenkins X - IDE Integration"},{"location":"to-be-processed/#visual-studio-code","text":"Visual Studio Code is a popular open source IDE from Microsoft. The Jenkins X team created the vscode-jx-tools extension for VS Code.","title":"Visual Studio Code"},{"location":"to-be-processed/#intelli-j","text":"There's a plugin for IntelliJ and the associated IDEs like WebStorm, GoLand, PyCharm et al from JetBrains . You can find the Jenkins X plugin for IntelliJ here .","title":"Intelli J"},{"location":"to-be-processed/#jenkins-x-customization","text":"","title":"Jenkins X - Customization"},{"location":"to-be-processed/#installation-parameters","text":"","title":"Installation Parameters"},{"location":"to-be-processed/#component-configuration","text":"During installation (incl. or excl. cluster creation) Jenkins X will read a myvalues.yaml file in the current directory to configure its core components. You can read more about all the options here , but below are some examples.","title":"Component Configuration"},{"location":"to-be-processed/#nexus","text":"You might not want to include Nexus in your installation, the snippet below will exclude it from being installed. 1 2 3 4 5 nexus : enabled : false nexusServiceLink : enabled : true externalName : nexus.jx.svc.cluster.local","title":"Nexus"},{"location":"to-be-processed/#chartmuseum_1","text":"Add the below snippet in order to skip installing Chartmuseum. 1 2 3 4 5 chartmuseum : enabled : false chartmuseumServiceLink : enabled : true externalName : jenkins-x-chartmuseum.jx.svc.cluster.local","title":"Chartmuseum"},{"location":"to-be-processed/#jenkins-image-used","text":"When using the static Jenkins type of installation, Jenkins X uses the jenkinsxio/jenkinsx docker image. You specify an alternative image in the myvalues.yaml . 1 2 3 4 jenkins : Master : Image : acme/my-jenkinsx ImageTag : 1.2.3 For how to create your custom Jenkins image, read the Jenkins X docs .","title":"Jenkins image used"},{"location":"to-be-processed/#alternative-docker-registry","text":"Currently - March 2019 - you can only specify the docker registry during the installation ( jx create cluster or jx install ) via a flag. 1 jx create cluster gke --docker-registry eu.gcr.io You will have to configure the docker authentication secret as well. How to do this, you can read in the Jenkins X documentation .","title":"Alternative Docker Registry"},{"location":"to-be-processed/#jenkins-x-basic-usage","text":"import quickstart console pipelines promote environments applications teams","title":"Jenkins X - Basic Usage"},{"location":"to-be-processed/#jenkins-x-other-features","text":"","title":"Jenkins X - Other Features"},{"location":"to-be-processed/#teams","text":"https://jenkins-x.io/about/features/#teams","title":"Teams"},{"location":"to-be-processed/#jx-shell","text":"Create a sub shell so that changes to the Kubernetes context, namespace or environment remain local to the shell 1 2 # create a new shell using a specific named context jx shell prod-cluster","title":"JX Shell"},{"location":"to-be-processed/#jx-prompt","text":"Generate the command line prompt for the current team and environment current 1 2 # Generate the current prompt jx prompt bash 1 2 # Enable the prompt for bash PS1 = [\\u@\\h \\W \\$(jx prompt)]\\$ zsh 1 2 # Enable the prompt for zsh PROMPT = $(jx prompt) $PROMPT","title":"JX Prompt"},{"location":"to-be-processed/#issue-tracker","text":"Jenkins X can work with issue trackers such as GitHub issues and Jira. By default, Jenkins X will use GitHub for projects and issues. So if you haven't specified anything for either Git provider or Issue tracker, it will use GitHub for issues.","title":"Issue Tracker"},{"location":"to-be-processed/#github","text":"1 jx create issue -t lets make things more awesome 1 jx get issues","title":"GitHub"},{"location":"to-be-processed/#jira","text":"In order to configure Jenkins X to use Jira as issue tracker, you have to do three steps. create a tracker configuration jx create tracker server ${trackerName} https://mycompany.atlassian.net/ create a tracker login jx create tracker token -n ${trackerName} myEmailAddress configure your Jenkins X managed project to use this issue tracker instead jx edit config -k issues Info A file called jenkins-x.yml will be modified in your project source code which should be added to your git repository.","title":"Jira"},{"location":"to-be-processed/#security","text":"Jenkins X has direct support for some security analysis .","title":"Security"},{"location":"to-be-processed/#anchore-image-scanning","text":"The Anchore Engine is used to provide image security, by examining contents of containers either in pull request/review state, or on running containers. This was introduced in this blog post . Here is a video demonstrating it live. 1 jx create addon anchore To see if it found any problems in a specific environment: 1 jx get cve --environment = staging","title":"Anchore image scanning"},{"location":"to-be-processed/#owasp-zap","text":"ZAP or Zed Attack Proxy allows you to scan the public surface of your application for any known vulnerability. 1 jx create addon owasp-zap","title":"OWASP ZAP"},{"location":"to-be-processed/#jenkins-x-development-cycle","text":"","title":"Jenkins X - Development Cycle"},{"location":"to-be-processed/#versioning","text":"semver semantics using git tags https://github.com/jenkins-x/jx-release-version https://www.cloudbees.com/blog/automatically-versioning-your-application-jenkins-x","title":"Versioning"},{"location":"to-be-processed/#devpod","text":"https://jenkins-x.io/developing/devpods/","title":"DevPod"},{"location":"to-be-processed/#custom-builder","text":"https://jenkins-x.io/getting-started/create-custom-builder/","title":"Custom Builder"},{"location":"to-be-processed/#preview","text":"A new preview is created when you create a PullRequest (PR) on a Jenkins X managed application. builds the application packages it into a Helm chart creates a unique Kubernetes namespace -only on first build deploys the application into the namespace adds a pull request comment with preview environment URL https://jenkins-x.io/about/features/#preview-environments https://jenkins-x.io/developing/preview/#adding-more-resources https://medium.com/@vbehar/zero-cost-preview-environments-on-kubernetes-with-jenkins-x-and-osiris-bd9ce0148d03 https://medium.com/@MichalFoksa/jenkins-x-preview-environment-3bf2424a05e4","title":"Preview"},{"location":"to-be-processed/#get-current-preview-environments","text":"1 jx get previews","title":"Get current preview environments"},{"location":"to-be-processed/#post-preview-hook","text":"Jenkins X allows you to extend it at several points. One such extension point is the preview process. You can extends the preview process with a post preview hook .","title":"Post Preview hook"},{"location":"to-be-processed/#add-dependencies","text":"Your application might depend on other services or facilities, that are generally present in your staging and production environment. With the move to clusters, these can have become a cluster function, These cluster functions might not be available in your temporary preview environment. Or you want to test your application against multiple versions of its dependencies. So you need the ability to specify these dependencies for the preview of your app which should only be used for the preview environment. This can be done in two ways. link Kubernetes service from other environment /namespace create an instance of a dependency in the preview environment through Helm chart.","title":"Add dependencies"},{"location":"to-be-processed/#link-to-a-service","text":"Via the jx step link service you connect your preview app with a Jenkins X managed service elsewhere in the same cluster. Example: 1 jx step link services --from-namespace jx-staging --includes * --excludes cheese*","title":"Link to a service"},{"location":"to-be-processed/#add-helm-chart-dependency","text":"The preview environment has its own helm chart, in the folder charts/preview . You can add dependencies in here, just like in any other helm chart. Just make sure the chart ends with the dependency on your app (in charts/appName ) and an empty line, as the file's comment says.","title":"Add helm chart dependency"},{"location":"to-be-processed/#promotion","text":"https://jenkins-x.io/faq/develop/#how-does-promotion-actually-work https://jenkins-x.io/developing/promote/","title":"Promotion"},{"location":"to-be-processed/#jenkins-x-secrets","text":"https://github.com/futuresimple/helm-secrets https://developer.epages.com/blog/tech-stories/kubernetes-deployments-with-helm-secrets/ vault operator / addon? secrets for preview example: https://github.com/Zenika/snowcamp-2019-sncf-timesheet-reader or this: https://github.com/Riduidel/snowcamp-2019 thanks! helm secrets was the only choice available initially but we\u2019re moving more towards using the vault operator instead - though we need some more docs and demos to show how to use secrets in vault from a Preview or Staging environment - James Strachan","title":"Jenkins X &amp; Secrets"},{"location":"to-be-processed/#extending-jenkins-x","text":"https://jenkins-x.io/extending/","title":"Extending Jenkins X"},{"location":"to-be-processed/#jenkins-x-cloudbees-codeship","text":"","title":"Jenkins X &amp; CloudBees CodeShip"},{"location":"to-be-processed/#jenkins-x-the-future","text":"Serverless Jenkins Tekton aka Next Gen(eration) Pipeline","title":"Jenkins X &amp; The Future"},{"location":"to-be-processed/#serverless","text":"https://medium.com/@jdrawlings/serverless-jenkins-with-jenkins-x-9134cbfe6870 https://github.com/jenkinsci/jenkinsfile-runner","title":"Serverless"},{"location":"to-be-processed/#next-gen-pipeline","text":"https://www.cloudbees.com/blog/move-toward-next-generation-pipelines https://jenkins-x.io/news/jenkins-x-next-gen-pipeline-engine/ https://github.com/tektoncd/pipeline#-tekton-pipelines https://github.com/jenkins-x/jx/issues/3225 https://jenkins-x.io/getting-started/next-gen-pipeline/ https://github.com/jenkins-x/jx/issues/3223","title":"Next Gen Pipeline"},{"location":"buildpack/","text":"BuildPack notes location of jx default packs: https://github.com/jenkins-x-buildpacks/jenkins-x-kubernetes location of local ones we can edit: ~/.jx/draft/packs/github.com/jenkins-x-buildpacks/jenkins-x-kubernetes Copy existing First, make a fork of https://github.com/$GH_USER/jenkins-x-kubernetes in your own repo. 1 GH_USER = ? 1 2 git clone https://github.com/ $GH_USER /jenkins-x-kubernetes cd jenkins-x-kubernetes Confirm the packs are there: 1 ls -1 packs Let's look at the Gradle one - the one closest to what we need. 1 ls -1 packs/gradle 1 cp -R packs/gradle packs/micronaut-gradle-redis 1 ls -1 packs/micronaut-gradle-redis Fix Dockerfile Raw Dockerfile 1 2 3 4 FROM openjdk:8u171-alpine3.7 RUN apk --no-cache add curl COPY build/libs/*-all.jar complete.jar CMD java ${ JAVA_OPTS } -jar complete.jar CommandLine magic 1 2 3 4 5 rm packs/micronaut-gradle-redis/Dockerfile echo FROM openjdk:8u171-alpine3.7 RUN apk --no-cache add curl COPY build/libs/*-all.jar complete.jar CMD java ${ JAVA_OPTS } -jar complete.jar | tee packs/micronaut-gradle-redis/Dockerfile Fix health check We have to change the value of probePath , from /actuator/health to /health . So please edit packs/micronaut-gradle-redis/charts/values.yaml to reflect the change or use the below yq command. yq 1 yq w packs/micronaut-gradle-redis/charts/values.yaml --inplace probePath /health Configure Redis deployment.yaml packs/micronaut-gradle-redis/charts/templates/deployment.yaml Raw YAML 1 2 3 env : - name : REDIS_HOST value : {{ template fullname . }} -redis-master Command Line magic 1 2 3 4 5 cat packs/micronaut-gradle-redis/charts/templates/deployment.yaml | sed -e \\ s@env:@env:\\ - name: REDIS_HOST\\ value: {{ template fullname . }}-redis-master@g \\ | tee packs/micronaut-gradle-redis/charts/templates/deployment.yaml values.yaml Raw YAML 1 2 REPLACE_ME_APP_NAME-redis : usePassword : false CommandLine Magic 1 2 3 echo REPLACE_ME_APP_NAME-redis: usePassword: false | tee -a packs/micronaut-gradle-redis/charts/values.yaml requirements.yaml packs/micronaut-gradle-redis/charts/requirements.yaml Warning Please note the usage of the REPLACE_ME_APP_NAME string. Today (April 2019), that is still one of the features that are not documented. When the build pack is applied, it'll replace that string with the actual name of the application. After all, it would be silly to hard-code the name of the application since this pack should be reusable across many. Raw YAML 1 2 3 4 5 dependencies : - alias : REPLACE_ME_APP_NAME-redis name : redis repository : https://kubernetes-charts.storage.googleapis.com version : 6.1.0 CommandLine magic 1 2 3 4 5 6 echo dependencies: - alias: REPLACE_ME_APP_NAME-redis name: redis repository: https://kubernetes-charts.storage.googleapis.com version: 6.1.0 | tee packs/micronaut-gradle-redis/charts/requirements.yaml Preview requirements packs/micronaut-gradle-redis/preview/requirements.yaml Info The file states: \"alias: preview\" must be last entry in dependencies array Place custom dependencies above We will have to change the file to include our Redis requirement. Which means, this part: 1 2 3 4 5 # !! alias: preview must be last entry in dependencies array !! # !! Place custom dependencies above !! - alias : preview name : REPLACE_ME_APP_NAME repository : file://../REPLACE_ME_APP_NAME Should look like: Expected End Result 1 2 3 4 5 6 7 8 9 10 - alias : preview-redis name : REPLACE_ME_APP_NAME-redis repository : https://kubernetes-charts.storage.googleapis.com version : 6.1.0 # !! alias: preview must be last entry in dependencies array !! # !! Place custom dependencies above !! - alias : preview name : REPLACE_ME_APP_NAME repository : file://../REPLACE_ME_APP_NAME CommandLine Magic 1 2 3 4 5 6 7 8 9 10 11 12 cat packs/micronaut-gradle-redis/preview/requirements.yaml \\ | sed -e \\ s@ # !! alias@- name: REPLACE_ME_APP_NAME-redis\\ alias: preview-redis\\ version: 6.1.0\\ repository: https://kubernetes-charts.storage.googleapis.com\\ \\ # !! alias@g \\ | tee packs/micronaut-gradle-redis/preview/requirements.yaml echo | tee -a packs/micronaut-gradle-redis/preview/requirements.yaml Commit and go 1 2 3 4 5 git add . git commit -m Added micronaut-gradle-redis build pack git push Add to known buildpacks With the new build pack safely stored, we should let Jenkins X know that we want to use the forked repository. We can use jx edit buildpack to change the location of our kubernetes-workloads packs. However, at the time of this writing (February 2019), there is a bug that prevents us from doing that ( issue 2955 ). The good news is that there is a workaround. If we omit the name ( -n or --name ), Jenkins X will add the new packs location, instead of editing the one dedicated to kubernetes-workloads packs. 1 2 3 4 jx edit buildpack \\ -u https://github.com/ $GH_USER /jenkins-x-kubernetes \\ -r master \\ -b Test new BuildPack make sure we're back at jx-micronaut-seed project lets reset it Reset application 1 2 3 4 5 6 7 8 9 10 11 git checkout orig git merge -s ours master --no-edit git checkout master git merge orig rm -rf charts git push Remove application from Jenkins X Removes the application from Jenkins X. 1 jx delete application $GH_USER /jx-micronaut-seed -b And the following removes the activities of the application from Kubernetes. 1 2 kubectl -n jx delete act -l owner = $GH_USER \\ -l sourcerepository = $GH_USER -jx-micronaut-seed Import 1 2 3 4 jx import --pack micronaut-gradle-redis -b ls -1 \\ ~/.jx/draft/packs/github.com/ $GH_USER /jenkins-x-kubernetes/packs Info If you run into the problem that the build fails, because the Helm Chart already exists in Chartmuseum: Received 409 response: {\"error\":\"file already exists\"} . You can solve this by creating and pushing git tag with a higher version via jx binary. 1 2 jx step tag --version 0 .2.0 git push Info For more information on how to version your application, please consult Jenkins X's jx-release-version tool . Or read CloudBees' blog on automatic versioning . Confirm it works Let's watch the activity stream, to see when our application lands in staging. 1 jx get activity -f jx-micronaut-seed -w Once it succeeds, we can see if the applications does run now. 1 kubectl get pods -n jx-staging -l app = jx-jx-micronaut-seed It should now be running: 1 2 NAME READY STATUS RESTARTS AGE jx-jx-micronaut-seed-75ffd4fbc4-66sgr 1 /1 Running 0 9m Let's see if we can talk to it. curl 1 2 APP_ADDR = $( kubectl get ing -n jx-staging jx-micronaut-seed -o jsonpath = {.spec.rules[0].host} ) curl http:// $APP_ADDR /health httpie 1 2 APP_ADDR = $( kubectl get ing -n jx-staging jx-micronaut-seed -o jsonpath = {.spec.rules[0].host} ) http $APP_ADDR /health The answer is: 1 2 3 { status : UP }","title":"BuildPack"},{"location":"buildpack/#buildpack-notes","text":"location of jx default packs: https://github.com/jenkins-x-buildpacks/jenkins-x-kubernetes location of local ones we can edit: ~/.jx/draft/packs/github.com/jenkins-x-buildpacks/jenkins-x-kubernetes","title":"BuildPack notes"},{"location":"buildpack/#copy-existing","text":"First, make a fork of https://github.com/$GH_USER/jenkins-x-kubernetes in your own repo. 1 GH_USER = ? 1 2 git clone https://github.com/ $GH_USER /jenkins-x-kubernetes cd jenkins-x-kubernetes Confirm the packs are there: 1 ls -1 packs Let's look at the Gradle one - the one closest to what we need. 1 ls -1 packs/gradle 1 cp -R packs/gradle packs/micronaut-gradle-redis 1 ls -1 packs/micronaut-gradle-redis","title":"Copy existing"},{"location":"buildpack/#fix-dockerfile","text":"Raw Dockerfile 1 2 3 4 FROM openjdk:8u171-alpine3.7 RUN apk --no-cache add curl COPY build/libs/*-all.jar complete.jar CMD java ${ JAVA_OPTS } -jar complete.jar CommandLine magic 1 2 3 4 5 rm packs/micronaut-gradle-redis/Dockerfile echo FROM openjdk:8u171-alpine3.7 RUN apk --no-cache add curl COPY build/libs/*-all.jar complete.jar CMD java ${ JAVA_OPTS } -jar complete.jar | tee packs/micronaut-gradle-redis/Dockerfile","title":"Fix Dockerfile"},{"location":"buildpack/#fix-health-check","text":"We have to change the value of probePath , from /actuator/health to /health . So please edit packs/micronaut-gradle-redis/charts/values.yaml to reflect the change or use the below yq command. yq 1 yq w packs/micronaut-gradle-redis/charts/values.yaml --inplace probePath /health","title":"Fix health check"},{"location":"buildpack/#configure-redis","text":"","title":"Configure Redis"},{"location":"buildpack/#deploymentyaml","text":"packs/micronaut-gradle-redis/charts/templates/deployment.yaml Raw YAML 1 2 3 env : - name : REDIS_HOST value : {{ template fullname . }} -redis-master Command Line magic 1 2 3 4 5 cat packs/micronaut-gradle-redis/charts/templates/deployment.yaml | sed -e \\ s@env:@env:\\ - name: REDIS_HOST\\ value: {{ template fullname . }}-redis-master@g \\ | tee packs/micronaut-gradle-redis/charts/templates/deployment.yaml","title":"deployment.yaml"},{"location":"buildpack/#valuesyaml","text":"Raw YAML 1 2 REPLACE_ME_APP_NAME-redis : usePassword : false CommandLine Magic 1 2 3 echo REPLACE_ME_APP_NAME-redis: usePassword: false | tee -a packs/micronaut-gradle-redis/charts/values.yaml","title":"values.yaml"},{"location":"buildpack/#requirementsyaml","text":"packs/micronaut-gradle-redis/charts/requirements.yaml Warning Please note the usage of the REPLACE_ME_APP_NAME string. Today (April 2019), that is still one of the features that are not documented. When the build pack is applied, it'll replace that string with the actual name of the application. After all, it would be silly to hard-code the name of the application since this pack should be reusable across many. Raw YAML 1 2 3 4 5 dependencies : - alias : REPLACE_ME_APP_NAME-redis name : redis repository : https://kubernetes-charts.storage.googleapis.com version : 6.1.0 CommandLine magic 1 2 3 4 5 6 echo dependencies: - alias: REPLACE_ME_APP_NAME-redis name: redis repository: https://kubernetes-charts.storage.googleapis.com version: 6.1.0 | tee packs/micronaut-gradle-redis/charts/requirements.yaml","title":"requirements.yaml"},{"location":"buildpack/#preview-requirements","text":"packs/micronaut-gradle-redis/preview/requirements.yaml Info The file states: \"alias: preview\" must be last entry in dependencies array Place custom dependencies above We will have to change the file to include our Redis requirement. Which means, this part: 1 2 3 4 5 # !! alias: preview must be last entry in dependencies array !! # !! Place custom dependencies above !! - alias : preview name : REPLACE_ME_APP_NAME repository : file://../REPLACE_ME_APP_NAME Should look like: Expected End Result 1 2 3 4 5 6 7 8 9 10 - alias : preview-redis name : REPLACE_ME_APP_NAME-redis repository : https://kubernetes-charts.storage.googleapis.com version : 6.1.0 # !! alias: preview must be last entry in dependencies array !! # !! Place custom dependencies above !! - alias : preview name : REPLACE_ME_APP_NAME repository : file://../REPLACE_ME_APP_NAME CommandLine Magic 1 2 3 4 5 6 7 8 9 10 11 12 cat packs/micronaut-gradle-redis/preview/requirements.yaml \\ | sed -e \\ s@ # !! alias@- name: REPLACE_ME_APP_NAME-redis\\ alias: preview-redis\\ version: 6.1.0\\ repository: https://kubernetes-charts.storage.googleapis.com\\ \\ # !! alias@g \\ | tee packs/micronaut-gradle-redis/preview/requirements.yaml echo | tee -a packs/micronaut-gradle-redis/preview/requirements.yaml","title":"Preview requirements"},{"location":"buildpack/#commit-and-go","text":"1 2 3 4 5 git add . git commit -m Added micronaut-gradle-redis build pack git push","title":"Commit and go"},{"location":"buildpack/#add-to-known-buildpacks","text":"With the new build pack safely stored, we should let Jenkins X know that we want to use the forked repository. We can use jx edit buildpack to change the location of our kubernetes-workloads packs. However, at the time of this writing (February 2019), there is a bug that prevents us from doing that ( issue 2955 ). The good news is that there is a workaround. If we omit the name ( -n or --name ), Jenkins X will add the new packs location, instead of editing the one dedicated to kubernetes-workloads packs. 1 2 3 4 jx edit buildpack \\ -u https://github.com/ $GH_USER /jenkins-x-kubernetes \\ -r master \\ -b","title":"Add to known buildpacks"},{"location":"buildpack/#test-new-buildpack","text":"make sure we're back at jx-micronaut-seed project lets reset it","title":"Test new BuildPack"},{"location":"buildpack/#reset-application","text":"1 2 3 4 5 6 7 8 9 10 11 git checkout orig git merge -s ours master --no-edit git checkout master git merge orig rm -rf charts git push","title":"Reset application"},{"location":"buildpack/#remove-application-from-jenkins-x","text":"Removes the application from Jenkins X. 1 jx delete application $GH_USER /jx-micronaut-seed -b And the following removes the activities of the application from Kubernetes. 1 2 kubectl -n jx delete act -l owner = $GH_USER \\ -l sourcerepository = $GH_USER -jx-micronaut-seed","title":"Remove application from Jenkins X"},{"location":"buildpack/#import","text":"1 2 3 4 jx import --pack micronaut-gradle-redis -b ls -1 \\ ~/.jx/draft/packs/github.com/ $GH_USER /jenkins-x-kubernetes/packs Info If you run into the problem that the build fails, because the Helm Chart already exists in Chartmuseum: Received 409 response: {\"error\":\"file already exists\"} . You can solve this by creating and pushing git tag with a higher version via jx binary. 1 2 jx step tag --version 0 .2.0 git push Info For more information on how to version your application, please consult Jenkins X's jx-release-version tool . Or read CloudBees' blog on automatic versioning .","title":"Import"},{"location":"buildpack/#confirm-it-works","text":"Let's watch the activity stream, to see when our application lands in staging. 1 jx get activity -f jx-micronaut-seed -w Once it succeeds, we can see if the applications does run now. 1 kubectl get pods -n jx-staging -l app = jx-jx-micronaut-seed It should now be running: 1 2 NAME READY STATUS RESTARTS AGE jx-jx-micronaut-seed-75ffd4fbc4-66sgr 1 /1 Running 0 9m Let's see if we can talk to it. curl 1 2 APP_ADDR = $( kubectl get ing -n jx-staging jx-micronaut-seed -o jsonpath = {.spec.rules[0].host} ) curl http:// $APP_ADDR /health httpie 1 2 APP_ADDR = $( kubectl get ing -n jx-staging jx-micronaut-seed -o jsonpath = {.spec.rules[0].host} ) http $APP_ADDR /health The answer is: 1 2 3 { status : UP }","title":"Confirm it works"},{"location":"buildpack/import/","text":"Import Existing Project Tip As we will have to edit yaml files, I can recommend using a commandline yaml editor. One such is mikefarah's yq . Snap 1 snap install yq Homebrew 1 brew install yq Another tip, for testing URL's, instead of using CURL, I would recommend HTTPie . Debian based 1 apt-get install httpie RHEL based 1 yum install httpie Homebrew 1 brew install httpie Windows via Python 1 2 pip install --upgrade pip setuptools pip install --upgrade httpie Config Replace ? with your GitHub user where you will be working from. 1 GH_USER = ? Fork example project Fork github.com/demomon/jx-micronaut-seed as a start. And then checkout your version of the project and go into the project's directory. SSH 1 2 git clone git@github.com: ${ GH_USER } /jx-micronaut-seed.git \\ cd jx-micronaut-seed https 1 2 git clone https://github.com/ ${ GH_USER } /jx-micronaut-seed.git \\ cd jx-micronaut-seed Import in JX Info When in doubt, accept the defaults of the prompts asking questions. 1 jx import You should see, among other things, the following logs. 1 2 3 selected pack: /Users/joostvdg/.jx/draft/packs/github.com/jenkins-x-buildpacks/jenkins-x-kubernetes/packs/gradle replacing placeholders in directory /Users/joostvdg/Projects/Personal/Github/jx-micronaut-seed app name: jx-micronaut-seed, git server: github.com, org: joostvdg, Docker registry org: joostvdg Jenkins X - via Draft - automatically detected this application is being build with Gradle . As you can see in the highlighted section in the code snippet above. Confirm the application is building The application will fail to build, as the default Dockerfile is not correct. Use the below code to replace the Dockerfile. 1 2 3 4 FROM openjdk:8u171-alpine3.7 RUN apk --no-cache add curl COPY build/libs/*-all.jar complete.jar CMD java ${ JAVA_OPTS } -jar complete.jar Commit your change and watch the activity. 1 2 3 4 git add Dockerfile git commit -m fix Dockerfile git push jx get activity -f jx-micronaut-seed -w Once the Promote: staging is completed successfully, we should be able to test if the application is running! 1 2 APP_ADDR = $( kubectl get ing -n jx-staging jx-micronaut-seed -o jsonpath = {.spec.rules[0].host} ) curl http:// $APP_ADDR You should see something like this: 1 2 3 4 5 6 7 html head title 503 Service Temporarily Unavailable / title / head body center h1 503 Service Temporarily Unavailable / h1 / center hr center nginx/1.15.8 / center / body / html Something is wrong... Find out what is wrong To find out what is wrong, lets check the pod status. 1 kubectl get pods -n jx-staging -l app = jx-jx-micronaut-seed We should see something as follows. 1 2 NAME READY STATUS RESTARTS AGE jx-jx-micronaut-seed-68f4bffb7b-mpb57 0 /1 CrashLoopBackOff 41 1h Let's describe the pod and see what is causing the CrashLoopBackOff . 1 kubectl describe pods -n jx-staging -l app = jx-jx-micronaut-seed If we look at the Events: section, we will see something like this: 1 2 3 4 5 6 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning Unhealthy 13m ( x1833 over 16h ) kubelet, gke-joostvdg-default-pool-6f512cf8-7l4l Readiness probe failed: HTTP probe failed with statuscode: 404 Normal Pulled 8m43s ( x240 over 16h ) kubelet, gke-joostvdg-default-pool-6f512cf8-7l4l Container image 10.23.250.118:5000/joostvdg/jx-micronaut-seed:0.0.3 already present on machine Warning BackOff 3m34s ( x2866 over 16h ) kubelet, gke-joostvdg-default-pool-6f512cf8-7l4l Back-off restarting failed container One of the things you can spot, is Readiness probe failed: HTTP probe failed with statuscode: 404 . Assuming our application is flawless - it passed it build test phase - it's likely that Kubernetes is looking at a non-existing endpoint for a health check. Fix Health Check As good as Jenkins X is, it isn't clairvoyant and cannot detect that our Micronaut application has a different health check endpoint. You might not know what the Micronaut framework gives you, but I can tell you. The health check endpoint is located at /health , not a bad place to put it. As the Gradle BuildPack is designed with Spring Boot in mind, it directs Kubernetes health check to /actuator/health . So we have to change this. Our application is packaged by Helm and the values for our Kubernetes Deployment - where the health check is configured - are located in /charts/jx-micronaut-seed/values.yaml . We have to change the value of probePath , from /actuator/health to /health . So please edit /charts/jx-micronaut-seed/values.yaml to reflect the change or use the below yq command. This should be the end result: 1 2 3 4 5 6 7 8 9 10 11 12 13 ... cpu : 500m memory : 512Mi requests : cpu : 400m memory : 512Mi probePath : /health livenessProbe : initialDelaySeconds : 60 periodSeconds : 10 successThreshold : 1 timeoutSeconds : 1 ... yq 1 yq w charts/jx-micronaut-seed/values.yaml --inplace probePath /health Now commit and push our change to fix our deployment! 1 2 3 4 git add charts/jx-micronaut-seed/values.yaml git commit -m fix health check endpoint git push jx get activity -f jx-micronaut-seed -w Once the applications is successfully promoted to staging, we can try again! 1 kubectl get pods -n jx-staging -l app = jx-jx-micronaut-seed Oh no, the application is still not running! 1 2 NAME READY STATUS RESTARTS AGE jx-jx-micronaut-seed-d5498679f-55b84 0 /1 Running 1 2m Still broken Let's describe the pod and see what is wrong this time. 1 kubectl describe pods -n jx-staging -l app = jx-jx-micronaut-seed 1 2 3 Warning Unhealthy 94s ( x2 over 3m14s ) kubelet, gke-joostvdg-default-pool-6f512cf8-41l4 Readiness probe failed: Get http://10.20.0.24:8080/health: dial tcp 10 .20.0.24:8080: connect: connection refused Warning Unhealthy 83s ( x2 over 3m3s ) kubelet, gke-joostvdg-default-pool-6f512cf8-41l4 Readiness probe failed: Get http://10.20.0.24:8080/health: net/http: request canceled ( Client.Timeout exceeded while awaiting headers ) Warning Unhealthy 54s ( x11 over 2m54s ) kubelet, gke-joostvdg-default-pool-6f512cf8-41l4 Readiness probe failed: HTTP probe failed with statuscode: 500 It seems our application is not getting into ready state: Readiness probe failed: HTTP probe failed with statuscode: 500 . Now this is a bit of cheat, because this application actually requires a connection with a Redis database in order to function. It can be build without it fine, and it will run fine, but Micronaut 's health check endpoint will incorporate the Redis connection into it's health status. Configure Redis database This means we must make sure our application can talk to a Redis database! Add Redis dependency The easiest way to do this with Jenkins X , is to add a dependency to our Helm Chart . If our dependency exists as a health chart, that is. Just our luck, looking at Helm Stable Charts , there's a Redis chart we can add. To do so, we add a requirements.yaml to our Chart. Create a file charts/jx-micronaut-seed/requirements.yaml and fill in the below details. Raw YAML 1 2 3 4 5 dependencies : - alias : jx-micronaut-seed-redis name : redis repository : https://kubernetes-charts.storage.googleapis.com version : 6.1.0 CommandLine magic 1 2 3 4 5 6 echo dependencies: - alias: jx-micronaut-seed-redis name: redis repository: https://kubernetes-charts.storage.googleapis.com version: 6.1.0 | tee charts/jx-micronaut-seed/requirements.yaml Application Redis config To be safe that whenever our application gets deployed via Helm it can find our database, we need to make sure the location it looks for is a variable. We can add this in three places, either in the application itself, in our values.yaml or in our deployment.yaml template. Our application will get deployed via Helm, which means the name it gets and the Redis dependency gets, will depend on the Helm release name. So in this particular case, it's best to add an environment variable to the deployment template. With a default value, that derives its value from the Helm release name. This makes the default install from Helm work and allows users of our Helm chart, to use a different Redis instance. To do so, we have to add the environment variable in charts/jx-micronaut-seed/templates/deployment.yaml . Add the below snippet to the spec.template.spec.containers[0] section between imagePullPolicy: {{ .Values.image.pullPolicy }} and ports: . Raw YAML 1 2 3 env : - name : REDIS_HOST value : {{ template fullname . }} -redis-master Command Line magic 1 2 3 4 5 cat charts/jx-micronaut-seed/templates/deployment.yaml | sed -e \\ s@env:@env:\\ - name: REDIS_HOST\\ value: {{ template fullname . }}-redis-master@g \\ | tee charts/jx-micronaut-seed/templates/deployment.yaml The end result should look like this: 1 2 3 4 5 6 7 8 9 spec : containers : - name : {{ .Chart.Name }} image : {{ .Values.image.repository }}:{{ .Values.image.tag }} imagePullPolicy : {{ .Values.image.pullPolicy }} env : - name : REDIS_HOST value : {{ template fullname . }} -redis-master ports : Redis Chart config We need to do one last thing. The Redis chart by default generates a unique password on startup. This is nice and secure, but makes it difficult for our application to connect to it. Let's configure our Redis chart to not use a password for now. Add the below snippet at the bottom of charts/jx-micronaut-seed/values.yaml or use the command line magic for automation. Raw YAML 1 2 jx-micronaut-seed-redis : usePassword : false Command Line magic 1 2 3 echo jx-micronaut-seed-redis: usePassword: false | tee -a charts/jx-micronaut-seed/values.yaml Commit and confirm Let's commit and push our changes and see if this was enough! 1 2 3 4 5 6 git add charts/jx-micronaut-seed/templates/deployment.yaml git add charts/jx-micronaut-seed/values.yaml git add charts/jx-micronaut-seed/requirements.yaml git commit -m add and configure redis dependency git push jx get activity -f jx-micronaut-seed -w Confirm it works Once it succeeds, we can see if the applications does run now. 1 kubectl get pods -n jx-staging -l app = jx-jx-micronaut-seed It should now be running: 1 2 NAME READY STATUS RESTARTS AGE jx-jx-micronaut-seed-75ffd4fbc4-66sgr 1 /1 Running 0 9m Let's see if we can talk to it. curl 1 2 APP_ADDR = $( kubectl get ing -n jx-staging jx-micronaut-seed -o jsonpath = {.spec.rules[0].host} ) curl http:// $APP_ADDR /health httpie 1 2 APP_ADDR = $( kubectl get ing -n jx-staging jx-micronaut-seed -o jsonpath = {.spec.rules[0].host} ) http $APP_ADDR /health The answer is: 1 2 3 { status : UP } We've done it! Now lets use the Redis database in a wholefully inappropriate way. curl 1 2 3 4 5 APP_ADDR = $( kubectl get ing -n jx-staging jx-micronaut-seed -o jsonpath = {.spec.rules[0].host} ) curl --header Content-Type: application/json \\ --request POST --data { body : Something curl , sender : Joost } \\ http:// $APP_ADDR /message curl http:// $APP_ADDR /message httpie 1 2 3 APP_ADDR = $( kubectl get ing -n jx-staging jx-micronaut-seed -o jsonpath = {.spec.rules[0].host} ) http POST ${ APP_ADDR } /message body = Something httpie sender = Joost http ${ APP_ADDR } /message Now, in order to avoid having to this kind of ritual for every Micronaut based application, we should probably make a better starting point. Let's move on to create a BuildPack Info For more information on Jenkins X's jx import command, please consult the documentation at jenkins-x.io","title":"Import"},{"location":"buildpack/import/#import-existing-project","text":"","title":"Import Existing Project"},{"location":"buildpack/import/#tip","text":"As we will have to edit yaml files, I can recommend using a commandline yaml editor. One such is mikefarah's yq . Snap 1 snap install yq Homebrew 1 brew install yq Another tip, for testing URL's, instead of using CURL, I would recommend HTTPie . Debian based 1 apt-get install httpie RHEL based 1 yum install httpie Homebrew 1 brew install httpie Windows via Python 1 2 pip install --upgrade pip setuptools pip install --upgrade httpie","title":"Tip"},{"location":"buildpack/import/#config","text":"Replace ? with your GitHub user where you will be working from. 1 GH_USER = ?","title":"Config"},{"location":"buildpack/import/#fork-example-project","text":"Fork github.com/demomon/jx-micronaut-seed as a start. And then checkout your version of the project and go into the project's directory. SSH 1 2 git clone git@github.com: ${ GH_USER } /jx-micronaut-seed.git \\ cd jx-micronaut-seed https 1 2 git clone https://github.com/ ${ GH_USER } /jx-micronaut-seed.git \\ cd jx-micronaut-seed","title":"Fork example project"},{"location":"buildpack/import/#import-in-jx","text":"Info When in doubt, accept the defaults of the prompts asking questions. 1 jx import You should see, among other things, the following logs. 1 2 3 selected pack: /Users/joostvdg/.jx/draft/packs/github.com/jenkins-x-buildpacks/jenkins-x-kubernetes/packs/gradle replacing placeholders in directory /Users/joostvdg/Projects/Personal/Github/jx-micronaut-seed app name: jx-micronaut-seed, git server: github.com, org: joostvdg, Docker registry org: joostvdg Jenkins X - via Draft - automatically detected this application is being build with Gradle . As you can see in the highlighted section in the code snippet above.","title":"Import in JX"},{"location":"buildpack/import/#confirm-the-application-is-building","text":"The application will fail to build, as the default Dockerfile is not correct. Use the below code to replace the Dockerfile. 1 2 3 4 FROM openjdk:8u171-alpine3.7 RUN apk --no-cache add curl COPY build/libs/*-all.jar complete.jar CMD java ${ JAVA_OPTS } -jar complete.jar Commit your change and watch the activity. 1 2 3 4 git add Dockerfile git commit -m fix Dockerfile git push jx get activity -f jx-micronaut-seed -w Once the Promote: staging is completed successfully, we should be able to test if the application is running! 1 2 APP_ADDR = $( kubectl get ing -n jx-staging jx-micronaut-seed -o jsonpath = {.spec.rules[0].host} ) curl http:// $APP_ADDR You should see something like this: 1 2 3 4 5 6 7 html head title 503 Service Temporarily Unavailable / title / head body center h1 503 Service Temporarily Unavailable / h1 / center hr center nginx/1.15.8 / center / body / html Something is wrong...","title":"Confirm the application is building"},{"location":"buildpack/import/#find-out-what-is-wrong","text":"To find out what is wrong, lets check the pod status. 1 kubectl get pods -n jx-staging -l app = jx-jx-micronaut-seed We should see something as follows. 1 2 NAME READY STATUS RESTARTS AGE jx-jx-micronaut-seed-68f4bffb7b-mpb57 0 /1 CrashLoopBackOff 41 1h Let's describe the pod and see what is causing the CrashLoopBackOff . 1 kubectl describe pods -n jx-staging -l app = jx-jx-micronaut-seed If we look at the Events: section, we will see something like this: 1 2 3 4 5 6 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning Unhealthy 13m ( x1833 over 16h ) kubelet, gke-joostvdg-default-pool-6f512cf8-7l4l Readiness probe failed: HTTP probe failed with statuscode: 404 Normal Pulled 8m43s ( x240 over 16h ) kubelet, gke-joostvdg-default-pool-6f512cf8-7l4l Container image 10.23.250.118:5000/joostvdg/jx-micronaut-seed:0.0.3 already present on machine Warning BackOff 3m34s ( x2866 over 16h ) kubelet, gke-joostvdg-default-pool-6f512cf8-7l4l Back-off restarting failed container One of the things you can spot, is Readiness probe failed: HTTP probe failed with statuscode: 404 . Assuming our application is flawless - it passed it build test phase - it's likely that Kubernetes is looking at a non-existing endpoint for a health check.","title":"Find out what is wrong"},{"location":"buildpack/import/#fix-health-check","text":"As good as Jenkins X is, it isn't clairvoyant and cannot detect that our Micronaut application has a different health check endpoint. You might not know what the Micronaut framework gives you, but I can tell you. The health check endpoint is located at /health , not a bad place to put it. As the Gradle BuildPack is designed with Spring Boot in mind, it directs Kubernetes health check to /actuator/health . So we have to change this. Our application is packaged by Helm and the values for our Kubernetes Deployment - where the health check is configured - are located in /charts/jx-micronaut-seed/values.yaml . We have to change the value of probePath , from /actuator/health to /health . So please edit /charts/jx-micronaut-seed/values.yaml to reflect the change or use the below yq command. This should be the end result: 1 2 3 4 5 6 7 8 9 10 11 12 13 ... cpu : 500m memory : 512Mi requests : cpu : 400m memory : 512Mi probePath : /health livenessProbe : initialDelaySeconds : 60 periodSeconds : 10 successThreshold : 1 timeoutSeconds : 1 ... yq 1 yq w charts/jx-micronaut-seed/values.yaml --inplace probePath /health Now commit and push our change to fix our deployment! 1 2 3 4 git add charts/jx-micronaut-seed/values.yaml git commit -m fix health check endpoint git push jx get activity -f jx-micronaut-seed -w Once the applications is successfully promoted to staging, we can try again! 1 kubectl get pods -n jx-staging -l app = jx-jx-micronaut-seed Oh no, the application is still not running! 1 2 NAME READY STATUS RESTARTS AGE jx-jx-micronaut-seed-d5498679f-55b84 0 /1 Running 1 2m","title":"Fix Health Check"},{"location":"buildpack/import/#still-broken","text":"Let's describe the pod and see what is wrong this time. 1 kubectl describe pods -n jx-staging -l app = jx-jx-micronaut-seed 1 2 3 Warning Unhealthy 94s ( x2 over 3m14s ) kubelet, gke-joostvdg-default-pool-6f512cf8-41l4 Readiness probe failed: Get http://10.20.0.24:8080/health: dial tcp 10 .20.0.24:8080: connect: connection refused Warning Unhealthy 83s ( x2 over 3m3s ) kubelet, gke-joostvdg-default-pool-6f512cf8-41l4 Readiness probe failed: Get http://10.20.0.24:8080/health: net/http: request canceled ( Client.Timeout exceeded while awaiting headers ) Warning Unhealthy 54s ( x11 over 2m54s ) kubelet, gke-joostvdg-default-pool-6f512cf8-41l4 Readiness probe failed: HTTP probe failed with statuscode: 500 It seems our application is not getting into ready state: Readiness probe failed: HTTP probe failed with statuscode: 500 . Now this is a bit of cheat, because this application actually requires a connection with a Redis database in order to function. It can be build without it fine, and it will run fine, but Micronaut 's health check endpoint will incorporate the Redis connection into it's health status.","title":"Still broken"},{"location":"buildpack/import/#configure-redis-database","text":"This means we must make sure our application can talk to a Redis database!","title":"Configure Redis database"},{"location":"buildpack/import/#add-redis-dependency","text":"The easiest way to do this with Jenkins X , is to add a dependency to our Helm Chart . If our dependency exists as a health chart, that is. Just our luck, looking at Helm Stable Charts , there's a Redis chart we can add. To do so, we add a requirements.yaml to our Chart. Create a file charts/jx-micronaut-seed/requirements.yaml and fill in the below details. Raw YAML 1 2 3 4 5 dependencies : - alias : jx-micronaut-seed-redis name : redis repository : https://kubernetes-charts.storage.googleapis.com version : 6.1.0 CommandLine magic 1 2 3 4 5 6 echo dependencies: - alias: jx-micronaut-seed-redis name: redis repository: https://kubernetes-charts.storage.googleapis.com version: 6.1.0 | tee charts/jx-micronaut-seed/requirements.yaml","title":"Add Redis dependency"},{"location":"buildpack/import/#application-redis-config","text":"To be safe that whenever our application gets deployed via Helm it can find our database, we need to make sure the location it looks for is a variable. We can add this in three places, either in the application itself, in our values.yaml or in our deployment.yaml template. Our application will get deployed via Helm, which means the name it gets and the Redis dependency gets, will depend on the Helm release name. So in this particular case, it's best to add an environment variable to the deployment template. With a default value, that derives its value from the Helm release name. This makes the default install from Helm work and allows users of our Helm chart, to use a different Redis instance. To do so, we have to add the environment variable in charts/jx-micronaut-seed/templates/deployment.yaml . Add the below snippet to the spec.template.spec.containers[0] section between imagePullPolicy: {{ .Values.image.pullPolicy }} and ports: . Raw YAML 1 2 3 env : - name : REDIS_HOST value : {{ template fullname . }} -redis-master Command Line magic 1 2 3 4 5 cat charts/jx-micronaut-seed/templates/deployment.yaml | sed -e \\ s@env:@env:\\ - name: REDIS_HOST\\ value: {{ template fullname . }}-redis-master@g \\ | tee charts/jx-micronaut-seed/templates/deployment.yaml The end result should look like this: 1 2 3 4 5 6 7 8 9 spec : containers : - name : {{ .Chart.Name }} image : {{ .Values.image.repository }}:{{ .Values.image.tag }} imagePullPolicy : {{ .Values.image.pullPolicy }} env : - name : REDIS_HOST value : {{ template fullname . }} -redis-master ports :","title":"Application Redis config"},{"location":"buildpack/import/#redis-chart-config","text":"We need to do one last thing. The Redis chart by default generates a unique password on startup. This is nice and secure, but makes it difficult for our application to connect to it. Let's configure our Redis chart to not use a password for now. Add the below snippet at the bottom of charts/jx-micronaut-seed/values.yaml or use the command line magic for automation. Raw YAML 1 2 jx-micronaut-seed-redis : usePassword : false Command Line magic 1 2 3 echo jx-micronaut-seed-redis: usePassword: false | tee -a charts/jx-micronaut-seed/values.yaml","title":"Redis Chart config"},{"location":"buildpack/import/#commit-and-confirm","text":"Let's commit and push our changes and see if this was enough! 1 2 3 4 5 6 git add charts/jx-micronaut-seed/templates/deployment.yaml git add charts/jx-micronaut-seed/values.yaml git add charts/jx-micronaut-seed/requirements.yaml git commit -m add and configure redis dependency git push jx get activity -f jx-micronaut-seed -w","title":"Commit and confirm"},{"location":"buildpack/import/#confirm-it-works","text":"Once it succeeds, we can see if the applications does run now. 1 kubectl get pods -n jx-staging -l app = jx-jx-micronaut-seed It should now be running: 1 2 NAME READY STATUS RESTARTS AGE jx-jx-micronaut-seed-75ffd4fbc4-66sgr 1 /1 Running 0 9m Let's see if we can talk to it. curl 1 2 APP_ADDR = $( kubectl get ing -n jx-staging jx-micronaut-seed -o jsonpath = {.spec.rules[0].host} ) curl http:// $APP_ADDR /health httpie 1 2 APP_ADDR = $( kubectl get ing -n jx-staging jx-micronaut-seed -o jsonpath = {.spec.rules[0].host} ) http $APP_ADDR /health The answer is: 1 2 3 { status : UP } We've done it! Now lets use the Redis database in a wholefully inappropriate way. curl 1 2 3 4 5 APP_ADDR = $( kubectl get ing -n jx-staging jx-micronaut-seed -o jsonpath = {.spec.rules[0].host} ) curl --header Content-Type: application/json \\ --request POST --data { body : Something curl , sender : Joost } \\ http:// $APP_ADDR /message curl http:// $APP_ADDR /message httpie 1 2 3 APP_ADDR = $( kubectl get ing -n jx-staging jx-micronaut-seed -o jsonpath = {.spec.rules[0].host} ) http POST ${ APP_ADDR } /message body = Something httpie sender = Joost http ${ APP_ADDR } /message Now, in order to avoid having to this kind of ritual for every Micronaut based application, we should probably make a better starting point. Let's move on to create a BuildPack Info For more information on Jenkins X's jx import command, please consult the documentation at jenkins-x.io","title":"Confirm it works"},{"location":"cat/","text":"Central Application Tracker Ideas minimal flexible data structure stored in Git should include: source reference ( git:// , svn:// , github:// , etc.), multiple possible packaging reference ( docker:// , helm:// ), multiple possible, should reference source reference hashes of reference and hashes of signof (pgp?) can scan applications in jx can scan applications in helm can scan applications from jx 's Environments","title":"Central Application Tracker"},{"location":"cat/#central-application-tracker","text":"","title":"Central Application Tracker"},{"location":"cat/#ideas","text":"minimal flexible data structure stored in Git should include: source reference ( git:// , svn:// , github:// , etc.), multiple possible packaging reference ( docker:// , helm:// ), multiple possible, should reference source reference hashes of reference and hashes of signof (pgp?) can scan applications in jx can scan applications in helm can scan applications from jx 's Environments","title":"Ideas"},{"location":"devoxx/create-cluster/","text":"Create Cluster Terraform Get Kubeconfig 1 gcloud container clusters get-credentials ${ CLUSTER_NAME } --region ${ REGION } JX Boot kubectl create secret generic external-dns-gcp-sa --from-file=credentials.json download jx brew install jx configure cloud dns see: https://joostvdg.github.io/jenkinsx/aks-boot-core/ change cloud dns' values.tmpl.yaml (gcp project) change cert manager prod issues' cert-manager-prod-issuer.yaml (gcp project) run cjx boot to initialize update jx-requirements.yaml run cjx boot again to start the rest JX Boot - CJXD configure cloud dns create CloudDNS Zone - create a zone for each subdomain \"forward\" domain to Zone's Domain Servers (NS entry, add the four servers) kubectl create secret generic external-dns-gcp-sa --from-file=credentials.json download cjxd https://docs.cloudbees.com/docs/cloudbees-jenkins-x-distribution/latest/install-guide/macos name this jx run jx profile cloudbees run jx boot to initialize update jx-requirements.yaml replace environments replace ingress replace cluster change google project for cloud dns config (different project) see: https://joostvdg.github.io/jenkinsx/aks-boot-core/ change cloud dns' values.tmpl.yaml (gcp project) change cert manager prod issues' cert-manager-prod-issuer.yaml (gcp project) CJXD's cloudbees-jenkins-x-boot-config version is 1.0.14 https://github.com/cloudbees/cloudbees-jenkins-x-boot-config/releases/tag/v1.0.14 run jx boot again to start the rest do not upgrade seems to crash immediatly create storage buckets log report chart repo confirm it is correct: jx status Clone Faillure 1 2 3 4 5 6 7 8 ? Do you want to clone the Jenkins X Boot Git repository? Yes Cloning https://github.com/cloudbees/cloudbees-jenkins-x-boot-config.git @ 1.0.36 to cloudbees-jenkins-x-boot-config error: setting HEAD to origin/1.0.36: git output: fatal: ambiguous argument origin/1.0.36 : unknown revision or path not in the working tree. Use -- to separate paths from revisions, like this: git command [ revision ...] -- [ file ...] : failed to run git reset --hard origin/1.0.36 command in directory cloudbees-jenkins-x-boot-config , output: fatal: ambiguous argument origin/1.0.36 : unknown revision or path not in the working tree. Use -- to separate paths from revisions, like this: git command [ revision ...] -- [ file ...] Helm Apply Issue 1 2 3 4 5 6 Modified file /Users/joostvdg/Projects/Personal/Github/vasimos/jx/cjxd/gke/jxboot/cloudbees-jenkins-x-boot-config/env/Chart.yaml to set the chart to version 1 Ignoring templates/.gitignore Applying the kubernetes overrides at ../kubeProviders/gke/values.tmpl.yaml Verifying the helm requirements versions in dir: . using version stream URL: https://github.com/cloudbees/cloudbees-jenkins-x-versions.git and git ref: v0.0.15 error: failed to lint the chart /var/folders/f_/mpwdv8s16r7_zt43r3r7k20w0000gn/T/jx-helm-apply-578543501/env : failed to run helm lint command in directory /var/folders/f_/mpwdv8s16r7_zt43r3r7k20w0000gn/T/jx-helm-apply-578543501/env , output: == Linting . [ERROR] Chart.yaml: apiVersion is required TLS - CloudDNS For each environment we have to setup the issuer and certificate. Easiest way I found, was to copy the yaml from the issuer and certificate in the jx namespace. You then remove the unnecesary elements, those generated by Kubernetes itself (such as creation date, status, etc). You have to change the domain name and hosts values, as they should now point to the subdomain corresponding to this environment (unless its production). Once the files are good, you add them to your environment. You do so, by adding them to the templates folder - env/templates. 1 kubectl -n jx get issuer letsencrypt-prod -o yaml 1 kubectl -n jx get certificate tls- unique to your cluster -p -o yaml Add Support For Multiple Subdomains If you want to support multiple subdomains, such as dev.cjxd.example.com and staging.cjxd.example.com you need multiple entries in the external dns controller for the domain. ImagePullBackup 1 2 3 4 5 6 7 8 9 events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 18m default-scheduler Successfully assigned jx-staging/jx-jx-qs-spring-boot-6-58b75446b4-pkd7x to gke-joost-cjxd-pool2-54e21b2f-hlhd Normal Pulling 16m (x4 over 18m) kubelet, gke-joost-cjxd-pool2-54e21b2f-hlhd Pulling image gcr.io/ps-dev-201405/jx-qs-spring-boot-6:0.0.1 Warning Failed 16m (x4 over 18m) kubelet, gke-joost-cjxd-pool2-54e21b2f-hlhd Failed to pull image gcr.io/ps-dev-201405/jx-qs-spring-boot-6:0.0.1 : rpc error: code = Unknown desc = Error response from daemon: unauthorized: You don t have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication Warning Failed 16m (x4 over 18m) kubelet, gke-joost-cjxd-pool2-54e21b2f-hlhd Error: ErrImagePull Normal BackOff 8m24s (x42 over 18m) kubelet, gke-joost-cjxd-pool2-54e21b2f-hlhd Back-off pulling image gcr.io/ps-dev-201405/jx-qs-spring-boot-6:0.0.1 Warning Failed 3m18s (x64 over 18m) kubelet, gke-joost-cjxd-pool2-54e21b2f-hlhd Error: ImagePullBackOff Then you're missing scopes in your GKE Node's. 1 2 3 4 5 6 7 8 9 10 11 12 13 resource google_container_node_pool nodepool2 { ... node_config { machine_type = n2-standard-2 oauth_scopes = [ https://www.googleapis.com/auth/compute , https://www.googleapis.com/auth/devstorage.read_only , https://www.googleapis.com/auth/logging.write , https://www.googleapis.com/auth/monitoring , ] } ... } Add CJXD UI 1 jx add app jx-app-ui --version=0.1.2 This will make a PR, which, when merged launches a Master Promotion build. 1 jx get activities --watch --filter environment-joost-cjxd-dev 1 jx get build log 1 jx ui -p 8081 Unable To Enable DNS API 1 2 valid: there is a Secret: external-dns-gcp-sa in namespace: jx error: unable to enable dns api: failed to run gcloud services list --enabled --project XXXXXX command in directory , output: ERROR: (gcloud.services.list) User [XXXXXX857-compute@developer.gserviceaccount.com] does not have permission to access project [XXXXXX] (or it may not exist): Request had insufficient authentication scopes. 1 2 3 4 valid: there is a Secret: external-dns-gcp-sa in namespace: jx error: unable to enable dns api: failed to run gcloud services list --enabled --project GCP PROJECT B command in directory , output: ERROR: (gcloud.services.list) User [389413650857-compute@developer.gserviceaccount.com] does not have permission to access project [GCP PROJECT B] (or it may not exist): Request had insufficient authentication scopes. Pipeline failed on stage release : container step-create-install-values . The execution of the pipeline has stopped. Have to enable \"Service Usage API\": https://console.developers.google.com/apis/api/serviceusage.googleapis.com/overview?project= node pools (for example in terraform) need access to the security scope https://www.googleapis.com/auth/cloud-platform 1 2 3 oauth_scopes = [ https://www.googleapis.com/auth/cloud-platform ] Reset Installation Remove Environment Repos go to git and remove environment repo clear jx boot config folder clear cloud dns config (if used) clear requirements from env repo's if re-used Clean GCP Resources Disks Buckets Others? Clean Environment Information 1 ./clear-clusters.sh Clear Local Data 1 rm -rf ~/.jx/ Recreate Git Token If for some reason the git token is invalid, you can recreate it with the commands below . 1 2 jx delete git token -n github yourUserName jx create git token -n github yourUserName Debug TLS Configuration Cert Manager 1 kubectl get pods -n cert-manager 1 kubectl logs -f -n cert-manager cm-cert-manager-885695c9f-bxhvk External DNS Controller kubectl logs -f exdns-external-dns-56948bff8-fq692 1 kubectl get ingress -A 1 2 NAMESPACE NAME HOSTS ADDRESS PORTS AGE jx jx-vault-joost-cjxd vault.dev.cjxd.kearos.net 35.204.54.193 80, 443 5m46s The address from the ingress resource should match the address returned from the nslookup . 1 nslookup vault.dev.cjxd.kearos.net 1 2 3 4 5 6 Server: 192.168.178.1 Address: 192.168.178.1#53 Non-authoritative answer: Name: vault.dev.cjxd.kearos.net Address: 35.204.54.193 Certificate Resources certificate issuer kubectl get issuer -n jx certificate kubectl get cert -n jx certificate secret kubectl get secret cloud dns secret ( credentials.json ) If the certificate is correct, it looks like this: 1 2 NAME READY SECRET AGE tls-dev-cjxd-kearos-net-p True tls-dev-cjxd-kearos-net-p 4m31s Create new Environment https://jenkins-x.io/docs/managing-jx/faq/boot/#how-do-i-add-new-environments Create CloudBees Environment copy existing resources into new ( env/templates/ ) update values to suit new environment execute jx boot configure cloud dns cloud dns zone external dns secret dns domain forward add cert and issuer to env repo configure secrets add requirements to repo edit exdns-external-dns deployment kubectl edit deployment -n jx exdns-external-dns Get Resources 1 2 kubectl get env staging -oyaml env/templates/cb.yaml kubectl get sr joostvdg-env-cjxd-staging -oyaml env/templates/cb-sr.yaml Configure TLS Create Cloud DNS Zone 1 2 3 ZONE_NAME=cb-cjxd-kearos-net DESCRIPTION= joostvdg - cb env for cjxd DNS_NAME=cb.cjxd.kearos.net 1 gcloud dns managed-zones create ${ZONE_NAME} --description=${DESCRIPTION} --dns-name=${DNS_NAME} Configure Certificate Issuer 1 2 kubectl get issuer -n jx letsencrypt-prod -oyaml env/templates/issuer.yaml kubectl get cert -n jx tls-dev-cjxd-kearos-net-p -oyaml env/templates/certificate.yaml Rename the namespace to your namespace remove status segment remove kubernetes managed fields ( uuid , timestamps, etc) 1 2 git add env/ git commit -m add certs 1 2 git push jx get activities -w 1 kubectl get namespace Configure DNS Secrets 1 2 kubectl get secret -n jx exdns-external-dns-token-cq5mv -oyaml exdns-external-dns-token-env-cb.yaml kubectl get secret -n jx external-dns-gcp-sa -oyaml external-dns-gcp-sa-env-cb.yaml Rename the namespace to your namespace remove status segment remove kubernetes managed fields ( uuid , timestamps, etc) 1 2 kubectl apply -f exdns-external-dns-token-env-cb.yaml kubectl apply -f external-dns-gcp-sa-env-cb.yaml Confirm Certificate Works 1 kubectl get cert -n cloudbees Add CloudBees Core https://charts.cloudbees.com/public/cloudbees/api/charts env/requirements.yaml 1 2 3 4 - name : cloudbees-core version : 3.6.0+4d2e34de1e86 repository : https://charts.cloudbees.com/public/cloudbees alias : core env/values.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 core : OperationsCenter : HostName : core.cb.cjxd.kearos.net Ingress : Annotations : kubernetes.io/ingress.class : nginx kubernetes.io/tls-acme : true nginx.ingress.kubernetes.io/app-root : https://$best_http_host/cjoc/teams-check/ nginx.ingress.kubernetes.io/proxy-body-size : 50m nginx.ingress.kubernetes.io/proxy-request-buffering : off nginx.ingress.kubernetes.io/ssl-redirect : true tls : Enable : true Host : core.cb.cjxd.kearos.net SecretName : tls-cb-cjxd-kearos-net-p ServiceType : ClusterIP nginx-ingress : Enabled : false cb env 1 cb source repository 1 jx-requirements 1 2 3 4 5 6 7 8 9 10 - ingress : domain : cb.cjxd.kearos.net externalDNS : true namespaceSubDomain : . tls : email : joostvdg@gmail.com enabled : true production : true key : cb repository : env-cjxd-cb TLS Unique DNS If you want to make sure each environment has its own unique address, the external dns controller needs to filter on multiple domains. Luckily, it is able to do so. Unfortunately, it seems Jenkins X (with jx boot ) doesn't seem to do this out of the box. https://github.com/kubernetes-incubator/external-dns/pull/252 To do so, make sure each environment has its own domain, as shown in jx-requirements . Then, edit the exdns deployment via kubectl edit exdn... and add an additional - --domain-filter= line at the args for each domain. jx-requirements 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 - ingress: domain: staging.gke.kearos.net externalDNS: true namespaceSubDomain: tls: email: joostvdg@gmail.com enabled: true production: true key: staging repository: env-gke-staging - ingress: domain: prod.gke.kearos.net externalDNS: true namespaceSubDomain: tls: email: joostvdg@gmail.com enabled: true production: true key: production repository: env-gke-production gitops: true ingress: cloud_dns_secret_name: external-dns-gcp-sa domain: jx.gke.kearos.net externalDNS: true namespaceSubDomain: -jx. tls: email: joostvdg@gmail.com enabled: true production: true - ingress: domain: staging.gke.kearos.net externalDNS: true namespaceSubDomain: tls: email: joostvdg@gmail.com enabled: true production: true key: staging repository: env-gke-staging - ingress: domain: prod.gke.kearos.net externalDNS: true namespaceSubDomain: tls: email: joostvdg@gmail.com enabled: true production: true key: production repository: env-gke-production gitops: true ingress: cloud_dns_secret_name: external-dns-gcp-sa domain: jx.gke.kearos.net externalDNS: true namespaceSubDomain: -jx. tls: email: joostvdg@gmail.com enabled: true production: true exdns deployment config 1 2 3 4 5 6 7 8 9 10 - args: - --log-level=info - --domain-filter=jx.gke.kearos.net - --domain-filter=staging.gke.kearos.net - --policy=upsert-only - --provider=google - --registry=txt - --interval=1m - --source=ingress - --google-project=kearos-gcp Certmanager complaining about the wrong domain In case Cert-Manager is complaining that while validating x.y.example.com it cannot find example.com . See: https://github.com/jetstack/cert-manager/issues/1507 1 2 3 I1104 09:13:33.884549 1 base_controller.go:187] cert-manager/controller/challenges level =0 msg = syncing item key = cloudbees/tls-cb-cjxd-kearos-net-p-2383487961-0 I1104 09:13:33.884966 1 dns.go:104] cert-manager/controller/challenges/Present level =0 msg = presenting DNS01 challenge for domain dnsName = cloudbees.cjxd.kearos.net domain = cloudbees.cjxd.kearos.net resource_kind = Challenge resource_name = tls-cb-cjxd-kearos-net-p-2383487961-0 resource_namespace = cloudbees type = dns-01 E1104 09:13:34.141122 1 base_controller.go:189] cert-manager/controller/challenges msg = re-queuing item due to error processing error = No matching GoogleCloud domain found for domain kearos.net. key = cloudbees/tls-cb-cjxd-kearos-net-p-2383487961-0 References Jenkins X - Managing FAQ Jenkins X - Using FAQ Jenkins X - Pipeline FAQ Jenkins X - Boot FAQ Jenkins X - Configure CloudDNS","title":"Init"},{"location":"devoxx/create-cluster/#create-cluster","text":"","title":"Create Cluster"},{"location":"devoxx/create-cluster/#terraform","text":"","title":"Terraform"},{"location":"devoxx/create-cluster/#get-kubeconfig","text":"1 gcloud container clusters get-credentials ${ CLUSTER_NAME } --region ${ REGION }","title":"Get Kubeconfig"},{"location":"devoxx/create-cluster/#jx-boot","text":"kubectl create secret generic external-dns-gcp-sa --from-file=credentials.json download jx brew install jx configure cloud dns see: https://joostvdg.github.io/jenkinsx/aks-boot-core/ change cloud dns' values.tmpl.yaml (gcp project) change cert manager prod issues' cert-manager-prod-issuer.yaml (gcp project) run cjx boot to initialize update jx-requirements.yaml run cjx boot again to start the rest","title":"JX Boot"},{"location":"devoxx/create-cluster/#jx-boot-cjxd","text":"configure cloud dns create CloudDNS Zone - create a zone for each subdomain \"forward\" domain to Zone's Domain Servers (NS entry, add the four servers) kubectl create secret generic external-dns-gcp-sa --from-file=credentials.json download cjxd https://docs.cloudbees.com/docs/cloudbees-jenkins-x-distribution/latest/install-guide/macos name this jx run jx profile cloudbees run jx boot to initialize update jx-requirements.yaml replace environments replace ingress replace cluster change google project for cloud dns config (different project) see: https://joostvdg.github.io/jenkinsx/aks-boot-core/ change cloud dns' values.tmpl.yaml (gcp project) change cert manager prod issues' cert-manager-prod-issuer.yaml (gcp project) CJXD's cloudbees-jenkins-x-boot-config version is 1.0.14 https://github.com/cloudbees/cloudbees-jenkins-x-boot-config/releases/tag/v1.0.14 run jx boot again to start the rest do not upgrade seems to crash immediatly create storage buckets log report chart repo confirm it is correct: jx status","title":"JX Boot - CJXD"},{"location":"devoxx/create-cluster/#clone-faillure","text":"1 2 3 4 5 6 7 8 ? Do you want to clone the Jenkins X Boot Git repository? Yes Cloning https://github.com/cloudbees/cloudbees-jenkins-x-boot-config.git @ 1.0.36 to cloudbees-jenkins-x-boot-config error: setting HEAD to origin/1.0.36: git output: fatal: ambiguous argument origin/1.0.36 : unknown revision or path not in the working tree. Use -- to separate paths from revisions, like this: git command [ revision ...] -- [ file ...] : failed to run git reset --hard origin/1.0.36 command in directory cloudbees-jenkins-x-boot-config , output: fatal: ambiguous argument origin/1.0.36 : unknown revision or path not in the working tree. Use -- to separate paths from revisions, like this: git command [ revision ...] -- [ file ...]","title":"Clone Faillure"},{"location":"devoxx/create-cluster/#helm-apply-issue","text":"1 2 3 4 5 6 Modified file /Users/joostvdg/Projects/Personal/Github/vasimos/jx/cjxd/gke/jxboot/cloudbees-jenkins-x-boot-config/env/Chart.yaml to set the chart to version 1 Ignoring templates/.gitignore Applying the kubernetes overrides at ../kubeProviders/gke/values.tmpl.yaml Verifying the helm requirements versions in dir: . using version stream URL: https://github.com/cloudbees/cloudbees-jenkins-x-versions.git and git ref: v0.0.15 error: failed to lint the chart /var/folders/f_/mpwdv8s16r7_zt43r3r7k20w0000gn/T/jx-helm-apply-578543501/env : failed to run helm lint command in directory /var/folders/f_/mpwdv8s16r7_zt43r3r7k20w0000gn/T/jx-helm-apply-578543501/env , output: == Linting . [ERROR] Chart.yaml: apiVersion is required","title":"Helm Apply Issue"},{"location":"devoxx/create-cluster/#tls-clouddns","text":"For each environment we have to setup the issuer and certificate. Easiest way I found, was to copy the yaml from the issuer and certificate in the jx namespace. You then remove the unnecesary elements, those generated by Kubernetes itself (such as creation date, status, etc). You have to change the domain name and hosts values, as they should now point to the subdomain corresponding to this environment (unless its production). Once the files are good, you add them to your environment. You do so, by adding them to the templates folder - env/templates. 1 kubectl -n jx get issuer letsencrypt-prod -o yaml 1 kubectl -n jx get certificate tls- unique to your cluster -p -o yaml","title":"TLS - CloudDNS"},{"location":"devoxx/create-cluster/#add-support-for-multiple-subdomains","text":"If you want to support multiple subdomains, such as dev.cjxd.example.com and staging.cjxd.example.com you need multiple entries in the external dns controller for the domain.","title":"Add Support For Multiple Subdomains"},{"location":"devoxx/create-cluster/#imagepullbackup","text":"1 2 3 4 5 6 7 8 9 events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 18m default-scheduler Successfully assigned jx-staging/jx-jx-qs-spring-boot-6-58b75446b4-pkd7x to gke-joost-cjxd-pool2-54e21b2f-hlhd Normal Pulling 16m (x4 over 18m) kubelet, gke-joost-cjxd-pool2-54e21b2f-hlhd Pulling image gcr.io/ps-dev-201405/jx-qs-spring-boot-6:0.0.1 Warning Failed 16m (x4 over 18m) kubelet, gke-joost-cjxd-pool2-54e21b2f-hlhd Failed to pull image gcr.io/ps-dev-201405/jx-qs-spring-boot-6:0.0.1 : rpc error: code = Unknown desc = Error response from daemon: unauthorized: You don t have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication Warning Failed 16m (x4 over 18m) kubelet, gke-joost-cjxd-pool2-54e21b2f-hlhd Error: ErrImagePull Normal BackOff 8m24s (x42 over 18m) kubelet, gke-joost-cjxd-pool2-54e21b2f-hlhd Back-off pulling image gcr.io/ps-dev-201405/jx-qs-spring-boot-6:0.0.1 Warning Failed 3m18s (x64 over 18m) kubelet, gke-joost-cjxd-pool2-54e21b2f-hlhd Error: ImagePullBackOff Then you're missing scopes in your GKE Node's. 1 2 3 4 5 6 7 8 9 10 11 12 13 resource google_container_node_pool nodepool2 { ... node_config { machine_type = n2-standard-2 oauth_scopes = [ https://www.googleapis.com/auth/compute , https://www.googleapis.com/auth/devstorage.read_only , https://www.googleapis.com/auth/logging.write , https://www.googleapis.com/auth/monitoring , ] } ... }","title":"ImagePullBackup"},{"location":"devoxx/create-cluster/#add-cjxd-ui","text":"1 jx add app jx-app-ui --version=0.1.2 This will make a PR, which, when merged launches a Master Promotion build. 1 jx get activities --watch --filter environment-joost-cjxd-dev 1 jx get build log 1 jx ui -p 8081","title":"Add CJXD UI"},{"location":"devoxx/create-cluster/#unable-to-enable-dns-api","text":"1 2 valid: there is a Secret: external-dns-gcp-sa in namespace: jx error: unable to enable dns api: failed to run gcloud services list --enabled --project XXXXXX command in directory , output: ERROR: (gcloud.services.list) User [XXXXXX857-compute@developer.gserviceaccount.com] does not have permission to access project [XXXXXX] (or it may not exist): Request had insufficient authentication scopes. 1 2 3 4 valid: there is a Secret: external-dns-gcp-sa in namespace: jx error: unable to enable dns api: failed to run gcloud services list --enabled --project GCP PROJECT B command in directory , output: ERROR: (gcloud.services.list) User [389413650857-compute@developer.gserviceaccount.com] does not have permission to access project [GCP PROJECT B] (or it may not exist): Request had insufficient authentication scopes. Pipeline failed on stage release : container step-create-install-values . The execution of the pipeline has stopped. Have to enable \"Service Usage API\": https://console.developers.google.com/apis/api/serviceusage.googleapis.com/overview?project= node pools (for example in terraform) need access to the security scope https://www.googleapis.com/auth/cloud-platform 1 2 3 oauth_scopes = [ https://www.googleapis.com/auth/cloud-platform ]","title":"Unable To Enable DNS API"},{"location":"devoxx/create-cluster/#reset-installation","text":"","title":"Reset Installation"},{"location":"devoxx/create-cluster/#remove-environment-repos","text":"go to git and remove environment repo clear jx boot config folder clear cloud dns config (if used) clear requirements from env repo's if re-used","title":"Remove Environment Repos"},{"location":"devoxx/create-cluster/#clean-gcp-resources","text":"","title":"Clean GCP Resources"},{"location":"devoxx/create-cluster/#disks","text":"","title":"Disks"},{"location":"devoxx/create-cluster/#buckets","text":"","title":"Buckets"},{"location":"devoxx/create-cluster/#others","text":"","title":"Others?"},{"location":"devoxx/create-cluster/#clean-environment-information","text":"1 ./clear-clusters.sh","title":"Clean Environment Information"},{"location":"devoxx/create-cluster/#clear-local-data","text":"1 rm -rf ~/.jx/","title":"Clear Local Data"},{"location":"devoxx/create-cluster/#recreate-git-token","text":"If for some reason the git token is invalid, you can recreate it with the commands below . 1 2 jx delete git token -n github yourUserName jx create git token -n github yourUserName","title":"Recreate Git Token"},{"location":"devoxx/create-cluster/#debug","text":"","title":"Debug"},{"location":"devoxx/create-cluster/#tls-configuration","text":"","title":"TLS Configuration"},{"location":"devoxx/create-cluster/#cert-manager","text":"1 kubectl get pods -n cert-manager 1 kubectl logs -f -n cert-manager cm-cert-manager-885695c9f-bxhvk","title":"Cert Manager"},{"location":"devoxx/create-cluster/#external-dns-controller","text":"kubectl logs -f exdns-external-dns-56948bff8-fq692 1 kubectl get ingress -A 1 2 NAMESPACE NAME HOSTS ADDRESS PORTS AGE jx jx-vault-joost-cjxd vault.dev.cjxd.kearos.net 35.204.54.193 80, 443 5m46s The address from the ingress resource should match the address returned from the nslookup . 1 nslookup vault.dev.cjxd.kearos.net 1 2 3 4 5 6 Server: 192.168.178.1 Address: 192.168.178.1#53 Non-authoritative answer: Name: vault.dev.cjxd.kearos.net Address: 35.204.54.193","title":"External DNS Controller"},{"location":"devoxx/create-cluster/#certificate-resources","text":"certificate issuer kubectl get issuer -n jx certificate kubectl get cert -n jx certificate secret kubectl get secret cloud dns secret ( credentials.json ) If the certificate is correct, it looks like this: 1 2 NAME READY SECRET AGE tls-dev-cjxd-kearos-net-p True tls-dev-cjxd-kearos-net-p 4m31s","title":"Certificate Resources"},{"location":"devoxx/create-cluster/#create-new-environment","text":"https://jenkins-x.io/docs/managing-jx/faq/boot/#how-do-i-add-new-environments","title":"Create new Environment"},{"location":"devoxx/create-cluster/#create-cloudbees-environment","text":"copy existing resources into new ( env/templates/ ) update values to suit new environment execute jx boot configure cloud dns cloud dns zone external dns secret dns domain forward add cert and issuer to env repo configure secrets add requirements to repo edit exdns-external-dns deployment kubectl edit deployment -n jx exdns-external-dns","title":"Create CloudBees Environment"},{"location":"devoxx/create-cluster/#get-resources","text":"1 2 kubectl get env staging -oyaml env/templates/cb.yaml kubectl get sr joostvdg-env-cjxd-staging -oyaml env/templates/cb-sr.yaml","title":"Get Resources"},{"location":"devoxx/create-cluster/#configure-tls","text":"","title":"Configure TLS"},{"location":"devoxx/create-cluster/#create-cloud-dns-zone","text":"1 2 3 ZONE_NAME=cb-cjxd-kearos-net DESCRIPTION= joostvdg - cb env for cjxd DNS_NAME=cb.cjxd.kearos.net 1 gcloud dns managed-zones create ${ZONE_NAME} --description=${DESCRIPTION} --dns-name=${DNS_NAME}","title":"Create Cloud DNS Zone"},{"location":"devoxx/create-cluster/#configure-certificate-issuer","text":"1 2 kubectl get issuer -n jx letsencrypt-prod -oyaml env/templates/issuer.yaml kubectl get cert -n jx tls-dev-cjxd-kearos-net-p -oyaml env/templates/certificate.yaml Rename the namespace to your namespace remove status segment remove kubernetes managed fields ( uuid , timestamps, etc) 1 2 git add env/ git commit -m add certs 1 2 git push jx get activities -w 1 kubectl get namespace","title":"Configure Certificate &amp; Issuer"},{"location":"devoxx/create-cluster/#configure-dns-secrets","text":"1 2 kubectl get secret -n jx exdns-external-dns-token-cq5mv -oyaml exdns-external-dns-token-env-cb.yaml kubectl get secret -n jx external-dns-gcp-sa -oyaml external-dns-gcp-sa-env-cb.yaml Rename the namespace to your namespace remove status segment remove kubernetes managed fields ( uuid , timestamps, etc) 1 2 kubectl apply -f exdns-external-dns-token-env-cb.yaml kubectl apply -f external-dns-gcp-sa-env-cb.yaml","title":"Configure DNS Secrets"},{"location":"devoxx/create-cluster/#confirm-certificate-works","text":"1 kubectl get cert -n cloudbees","title":"Confirm Certificate Works"},{"location":"devoxx/create-cluster/#add-cloudbees-core","text":"https://charts.cloudbees.com/public/cloudbees/api/charts env/requirements.yaml 1 2 3 4 - name : cloudbees-core version : 3.6.0+4d2e34de1e86 repository : https://charts.cloudbees.com/public/cloudbees alias : core env/values.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 core : OperationsCenter : HostName : core.cb.cjxd.kearos.net Ingress : Annotations : kubernetes.io/ingress.class : nginx kubernetes.io/tls-acme : true nginx.ingress.kubernetes.io/app-root : https://$best_http_host/cjoc/teams-check/ nginx.ingress.kubernetes.io/proxy-body-size : 50m nginx.ingress.kubernetes.io/proxy-request-buffering : off nginx.ingress.kubernetes.io/ssl-redirect : true tls : Enable : true Host : core.cb.cjxd.kearos.net SecretName : tls-cb-cjxd-kearos-net-p ServiceType : ClusterIP nginx-ingress : Enabled : false","title":"Add CloudBees Core"},{"location":"devoxx/create-cluster/#cb-env","text":"1","title":"cb env"},{"location":"devoxx/create-cluster/#cb-source-repository","text":"1","title":"cb source repository"},{"location":"devoxx/create-cluster/#jx-requirements","text":"1 2 3 4 5 6 7 8 9 10 - ingress : domain : cb.cjxd.kearos.net externalDNS : true namespaceSubDomain : . tls : email : joostvdg@gmail.com enabled : true production : true key : cb repository : env-cjxd-cb","title":"jx-requirements"},{"location":"devoxx/create-cluster/#tls-unique-dns","text":"If you want to make sure each environment has its own unique address, the external dns controller needs to filter on multiple domains. Luckily, it is able to do so. Unfortunately, it seems Jenkins X (with jx boot ) doesn't seem to do this out of the box. https://github.com/kubernetes-incubator/external-dns/pull/252 To do so, make sure each environment has its own domain, as shown in jx-requirements . Then, edit the exdns deployment via kubectl edit exdn... and add an additional - --domain-filter= line at the args for each domain.","title":"TLS Unique DNS"},{"location":"devoxx/create-cluster/#jx-requirements_1","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 - ingress: domain: staging.gke.kearos.net externalDNS: true namespaceSubDomain: tls: email: joostvdg@gmail.com enabled: true production: true key: staging repository: env-gke-staging - ingress: domain: prod.gke.kearos.net externalDNS: true namespaceSubDomain: tls: email: joostvdg@gmail.com enabled: true production: true key: production repository: env-gke-production gitops: true ingress: cloud_dns_secret_name: external-dns-gcp-sa domain: jx.gke.kearos.net externalDNS: true namespaceSubDomain: -jx. tls: email: joostvdg@gmail.com enabled: true production: true - ingress: domain: staging.gke.kearos.net externalDNS: true namespaceSubDomain: tls: email: joostvdg@gmail.com enabled: true production: true key: staging repository: env-gke-staging - ingress: domain: prod.gke.kearos.net externalDNS: true namespaceSubDomain: tls: email: joostvdg@gmail.com enabled: true production: true key: production repository: env-gke-production gitops: true ingress: cloud_dns_secret_name: external-dns-gcp-sa domain: jx.gke.kearos.net externalDNS: true namespaceSubDomain: -jx. tls: email: joostvdg@gmail.com enabled: true production: true","title":"jx-requirements"},{"location":"devoxx/create-cluster/#exdns-deployment-config","text":"1 2 3 4 5 6 7 8 9 10 - args: - --log-level=info - --domain-filter=jx.gke.kearos.net - --domain-filter=staging.gke.kearos.net - --policy=upsert-only - --provider=google - --registry=txt - --interval=1m - --source=ingress - --google-project=kearos-gcp","title":"exdns deployment config"},{"location":"devoxx/create-cluster/#certmanager-complaining-about-the-wrong-domain","text":"In case Cert-Manager is complaining that while validating x.y.example.com it cannot find example.com . See: https://github.com/jetstack/cert-manager/issues/1507 1 2 3 I1104 09:13:33.884549 1 base_controller.go:187] cert-manager/controller/challenges level =0 msg = syncing item key = cloudbees/tls-cb-cjxd-kearos-net-p-2383487961-0 I1104 09:13:33.884966 1 dns.go:104] cert-manager/controller/challenges/Present level =0 msg = presenting DNS01 challenge for domain dnsName = cloudbees.cjxd.kearos.net domain = cloudbees.cjxd.kearos.net resource_kind = Challenge resource_name = tls-cb-cjxd-kearos-net-p-2383487961-0 resource_namespace = cloudbees type = dns-01 E1104 09:13:34.141122 1 base_controller.go:189] cert-manager/controller/challenges msg = re-queuing item due to error processing error = No matching GoogleCloud domain found for domain kearos.net. key = cloudbees/tls-cb-cjxd-kearos-net-p-2383487961-0","title":"Certmanager complaining about the wrong domain"},{"location":"devoxx/create-cluster/#references","text":"Jenkins X - Managing FAQ Jenkins X - Using FAQ Jenkins X - Pipeline FAQ Jenkins X - Boot FAQ Jenkins X - Configure CloudDNS","title":"References"},{"location":"devoxx/environment/","text":"Environment Show Environments 1 jx get environments Make a PR Create New Branch 1 git checkout -b helloworld-controller Add Model 1 2 3 4 5 6 7 8 9 10 11 12 public class Greeting { private String greeting ; public Greeting ( String greeting ) { this . greeting = greeting ; } public String getGreeting () { return greeting ; } } Add controller 1 2 3 4 5 6 7 8 @RestController @RequestMapping ( /hello ) public class HelloWorldController { @GetMapping public Greeting hello () { return new Greeting ( Hello Devoxx! ); } } Make the PR 1 2 git add src/ git commit -m add helloworld controller 1 git push --set-upstream origin helloworld-controller 1 jx create pullrequest --title my PR --body What are we doing --batch-mode 1 jx get activity -f jx-qs-spring-boot -w 1 jx get build log 1 http http://jx-qs-spring-boot-7.jx-joostvdg-jx-qs-spring-boot-7-pr-1.dev.cjxd.kearos.net/hello Promote to production show environments: jx get environments look at KIND and PROMOTE show repository 1 jx get applications -e staging 1 jx get applications -e production 1 jx promote jx-qs-spring-boot-7 --env production --version 1 jx get activity -f env-cjxd-prod -w 1 jx ui -p 8082 1 jx get applications -e production Add PR show applications: jx get applications show environments: jx get environments git checkout -b helloworld-controller make changes git push --set-upstream origin helloworld-controller create pull request jx create pullrequest --title \"my PR\" --body \"What are we doing\" --batch-mode open URL of pull request add /meow add /assign @joostvdg add /lgtm watch activity jx get activity -f jx-qs-spring-boot-1 -w look for Preview in GitHub PR page show namespaces: kubectl get namespaces approve PR merge References https://docs.cloudbees.com/docs/cloudbees-jenkins-x-distribution/latest/developer-guide/spring-boot","title":"Environment"},{"location":"devoxx/environment/#environment","text":"","title":"Environment"},{"location":"devoxx/environment/#show-environments","text":"1 jx get environments","title":"Show Environments"},{"location":"devoxx/environment/#make-a-pr","text":"","title":"Make a PR"},{"location":"devoxx/environment/#create-new-branch","text":"1 git checkout -b helloworld-controller","title":"Create New Branch"},{"location":"devoxx/environment/#add-model","text":"1 2 3 4 5 6 7 8 9 10 11 12 public class Greeting { private String greeting ; public Greeting ( String greeting ) { this . greeting = greeting ; } public String getGreeting () { return greeting ; } }","title":"Add Model"},{"location":"devoxx/environment/#add-controller","text":"1 2 3 4 5 6 7 8 @RestController @RequestMapping ( /hello ) public class HelloWorldController { @GetMapping public Greeting hello () { return new Greeting ( Hello Devoxx! ); } }","title":"Add controller"},{"location":"devoxx/environment/#make-the-pr","text":"1 2 git add src/ git commit -m add helloworld controller 1 git push --set-upstream origin helloworld-controller 1 jx create pullrequest --title my PR --body What are we doing --batch-mode 1 jx get activity -f jx-qs-spring-boot -w 1 jx get build log 1 http http://jx-qs-spring-boot-7.jx-joostvdg-jx-qs-spring-boot-7-pr-1.dev.cjxd.kearos.net/hello","title":"Make the PR"},{"location":"devoxx/environment/#promote-to-production","text":"show environments: jx get environments look at KIND and PROMOTE show repository 1 jx get applications -e staging 1 jx get applications -e production 1 jx promote jx-qs-spring-boot-7 --env production --version 1 jx get activity -f env-cjxd-prod -w 1 jx ui -p 8082 1 jx get applications -e production","title":"Promote to production"},{"location":"devoxx/environment/#add-pr","text":"show applications: jx get applications show environments: jx get environments git checkout -b helloworld-controller make changes git push --set-upstream origin helloworld-controller create pull request jx create pullrequest --title \"my PR\" --body \"What are we doing\" --batch-mode open URL of pull request add /meow add /assign @joostvdg add /lgtm watch activity jx get activity -f jx-qs-spring-boot-1 -w look for Preview in GitHub PR page show namespaces: kubectl get namespaces approve PR merge","title":"Add PR"},{"location":"devoxx/environment/#references","text":"https://docs.cloudbees.com/docs/cloudbees-jenkins-x-distribution/latest/developer-guide/spring-boot","title":"References"},{"location":"devoxx/quickstart/","text":"Quickstart ENVs 1 2 3 GH_USER = joostvdg APP_NAME = jx-qs-spring-boot-1 GH_TOKEN = Check Existing Applications 1 jx get applications Create default 1 jx create spring preconfigured 1 jx create spring --git-username = ${ GH_USER } --git-api-token = ${ GH_TOKEN } See Application in JX Watch Activity 1 jx get activity -f jx-qs-spring-boot -w Get Applications 1 jx get applications -e staging Test Application 1 http https://jx-qs-spring-boot-7-jx-staging.staging.cjxd.kearos.net Get Build Log 1 jx get build log CJXD UI 1 jx ui -p 8082 Add Pipeline step Open Application with Intelli J open jenkins-x.yml Build Packs https://github.com/jenkins-x-buildpacks/jenkins-x-kubernetes/tree/master/packs https://github.com/jenkins-x-buildpacks/jenkins-x-classic/blob/master/packs/maven/pipeline.yaml Add SonarCloud scan https://sonarcloud.io/projects/create Add env vars 1 2 3 4 5 6 7 8 9 pipelineConfig : env : - name : example value : someValue - name : fromSecret valueFrom : secretKeyRef : key : SONARCLOUD_APIKEY name : sonarcloud-apikey Add step 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 pipelineConfig : pipelines : overrides : - name : mvn-deploy pipeline : release stage : build step : name : sonar command : sonar-scanner image : fabiopotame/sonar-scanner-cli # newtmitch/sonar-scanner for JDK 10+? dir : /workspace/source/ args : - -Dsonar.projectName=jx-qs-spring-boot - -Dsonar.projectKey=jx-qs-spring-boot - -Dsonar.organization=joostvdg-github - -Dsonar.sources=./src/main/java/ - -Dsonar.language=java - -Dsonar.java.binaries=./target/classes - -Dsonar.host.url=https://sonarcloud.io - -Dsonar.login=bebe633ad6599cbf52f7e0b9ee1bc2bbd3cd9c80 type : after Verify existing pipeline 1 jx step syntax effective Make the Change 1 2 git add jenkins-x.yml git commit -m add sonarqube scan 1 git push Confirm build 1 jx get activity -f jx-qs-spring-boot-7 -w 1 jx get build logs","title":"Quickstart"},{"location":"devoxx/quickstart/#quickstart","text":"","title":"Quickstart"},{"location":"devoxx/quickstart/#envs","text":"1 2 3 GH_USER = joostvdg APP_NAME = jx-qs-spring-boot-1 GH_TOKEN =","title":"ENVs"},{"location":"devoxx/quickstart/#check-existing-applications","text":"1 jx get applications","title":"Check Existing Applications"},{"location":"devoxx/quickstart/#create","text":"default 1 jx create spring preconfigured 1 jx create spring --git-username = ${ GH_USER } --git-api-token = ${ GH_TOKEN }","title":"Create"},{"location":"devoxx/quickstart/#see-application-in-jx","text":"","title":"See Application in JX"},{"location":"devoxx/quickstart/#watch-activity","text":"1 jx get activity -f jx-qs-spring-boot -w","title":"Watch Activity"},{"location":"devoxx/quickstart/#get-applications","text":"1 jx get applications -e staging","title":"Get Applications"},{"location":"devoxx/quickstart/#test-application","text":"1 http https://jx-qs-spring-boot-7-jx-staging.staging.cjxd.kearos.net","title":"Test Application"},{"location":"devoxx/quickstart/#get-build-log","text":"1 jx get build log","title":"Get Build Log"},{"location":"devoxx/quickstart/#cjxd-ui","text":"1 jx ui -p 8082","title":"CJXD UI"},{"location":"devoxx/quickstart/#add-pipeline-step","text":"Open Application with Intelli J open jenkins-x.yml","title":"Add Pipeline step"},{"location":"devoxx/quickstart/#build-packs","text":"https://github.com/jenkins-x-buildpacks/jenkins-x-kubernetes/tree/master/packs https://github.com/jenkins-x-buildpacks/jenkins-x-classic/blob/master/packs/maven/pipeline.yaml","title":"Build Packs"},{"location":"devoxx/quickstart/#add-sonarcloud-scan","text":"https://sonarcloud.io/projects/create","title":"Add SonarCloud scan"},{"location":"devoxx/quickstart/#add-env-vars","text":"1 2 3 4 5 6 7 8 9 pipelineConfig : env : - name : example value : someValue - name : fromSecret valueFrom : secretKeyRef : key : SONARCLOUD_APIKEY name : sonarcloud-apikey","title":"Add env vars"},{"location":"devoxx/quickstart/#add-step","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 pipelineConfig : pipelines : overrides : - name : mvn-deploy pipeline : release stage : build step : name : sonar command : sonar-scanner image : fabiopotame/sonar-scanner-cli # newtmitch/sonar-scanner for JDK 10+? dir : /workspace/source/ args : - -Dsonar.projectName=jx-qs-spring-boot - -Dsonar.projectKey=jx-qs-spring-boot - -Dsonar.organization=joostvdg-github - -Dsonar.sources=./src/main/java/ - -Dsonar.language=java - -Dsonar.java.binaries=./target/classes - -Dsonar.host.url=https://sonarcloud.io - -Dsonar.login=bebe633ad6599cbf52f7e0b9ee1bc2bbd3cd9c80 type : after","title":"Add step"},{"location":"devoxx/quickstart/#verify-existing-pipeline","text":"1 jx step syntax effective","title":"Verify existing pipeline"},{"location":"devoxx/quickstart/#make-the-change","text":"1 2 git add jenkins-x.yml git commit -m add sonarqube scan 1 git push","title":"Make the Change"},{"location":"devoxx/quickstart/#confirm-build","text":"1 jx get activity -f jx-qs-spring-boot-7 -w 1 jx get build logs","title":"Confirm build"},{"location":"k8s/","text":"Kubernetes Basics - Part I Points to cover Building Docker Images Creating Pods Scaling Pods With ReplicaSets Using Services To Enable Communication Between Pods Deploying Releases With Zero-Downtime Using Ingress To Forward Traffic Prerequisites This guide borrows heavily from the workshops created by Viktor Farcic . The examples below are build on top of his example repository, so make sure you clone that. 1 2 git clone https://github.com/vfarcic/k8s-specs.git cd k8s-specs Build docker images Docker BuildKit Alternatives Pod View Yaml 1 cat pod/db.yml Create 1 kubectl create -f pod/db.yml View Pod 1 kubectl get pods View Pod Alternatives Plural 1 kubectl get pods Singular 1 kubectl get pod Shorthand 1 kubectl get po YAML 1 kubectl get po db -o yaml JSON 1 kubectl get po db -o json JSON Path 1 kubectl get pod db -o jsonpath = {.metadata.name} Describe Pod 1 kubectl describe pod db Update Pod 1 kubectl create -f pod/db.yml This should yield an error! 1 Error from server ( AlreadyExists ) : error when creating pod/db.yml : pods db already exists The create command is imperative and in this case not idempotent. To update we would either have to patch the resource in the cluster, or apply an updated version of the yaml file. 1 kubectl apply -f pod/db.yml Enter the Pod 1 kubectl exec db ps aux 1 kubectl exec -it db sh The command below should be executed from inside the pod. If you get something like command not found: mongo , double check the above command's success. 1 echo db.stats() | mongo localhost:27017/test 1 exit 1 kubectl logs db 1 kubectl exec -it db pkill mongod 1 kubectl get pods -w Cleanup 1 kubectl delete -f pod/db.yml ReplicaSet View YAML 1 cat rs/go-demo-2.yml Create ReplicaSet 1 kubectl create -f rs/go-demo-2.yml View ReplicaSet 1 kubectl get rs 1 kubectl describe -f rs/go-demo-2.yml Which should look something like this, notice the highlighted lines? 1 2 3 4 5 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal SuccessfulCreate 9s replicaset-controller Created pod: go-demo-2-4mw79 Normal SuccessfulCreate 9s replicaset-controller Created pod: go-demo-2-bcv5n View Pods Delete Pod 1 kubectl get pods 1 2 POD_NAME = $( kubectl get pods -o name | tail -1 ) kubectl delete $POD_NAME 1 kubectl get pods Cleanup 1 kubectl delete -f rs/go-demo-2.yml Labels Create a Pods 1 2 kubectl create -f rs/go-demo-2.yml kubectl create -f pod/db.yml Get Pods 1 kubectl get pods Get it by Label 1 kubectl get pods -l type = backend Get it by multiple labels 1 kubectl get pods -l type = backend,language = go Show Labels 1 kubectl get pods --show-labels Cleanup 1 2 kubectl delete -f rs/go-demo-2.yml kubectl delete -f pod/db.yml Service View GKE/EKS 1 cat svc/go-demo-2-lb.yml Minikube 1 cat svc/go-demo-2.yml Create GKE/EKS 1 kubectl create -f svc/go-demo-2-lb.yml Minikube 1 kubectl create -f svc/go-demo-2.yml Inspect 1 kubectl get -f svc/go-demo-2.yml Get LB IP EKS 1 2 3 #!/bin/bash IP = $( kubectl get svc go-demo-2-api \\ -o jsonpath = {.status.loadBalancer.ingress[0].hostname} ) GKE 1 2 3 #!/bin/bash IP = $( kubectl get svc go-demo-2-api \\ -o jsonpath = {.status.loadBalancer.ingress[0].ip} ) Minikube 1 2 #!/bin/bash IP = $( minikube ip ) All - after 1 echo $IP Get Port GKE/EKS 1 PORT = 8080 Minikube 1 2 PORT = $( kubectl get svc go-demo-2-api \\ -o jsonpath = {.spec.ports[0].nodePort} ) Call Application curl 1 curl -i http:// $IP : $PORT /demo/hello HTTPie 1 http $IP : $PORT /demo/hello Cleanup 1 kubectl delete -f svc/go-demo-2.yml Deployment 1 cat deploy/go-demo-2.yml 1 kubectl create -f deploy/go-demo-2.yml --record --save-config 1 kubectl describe deploy go-demo-2-api 1 kubectl get all Zero downtime deployment 1 2 kubectl set image deploy go-demo-2-api api = vfarcic/go-demo-2:2.0 \\ --record 1 kubectl rollout status -w deploy go-demo-2-api 1 kubectl describe deploy go-demo-2-api 1 kubectl rollout history deploy go-demo-2-api 1 kubectl get rs Rollback / Rollforward 1 kubectl rollout undo deploy go-demo-2-api 1 kubectl describe deploy go-demo-2-api 1 kubectl rollout history deploy go-demo-2-api 1 kubectl rollout undo -f deploy/go-demo-2-api.yml --to-revision = 2 1 kubectl rollout history deploy go-demo-2-api Scaling 1 kubectl scale deployment go-demo-2-api --replicas 8 --record 1 kubectl get pods Cleanup 1 kubectl delete -f deploy/go-demo-2.yml Ingress Install ingress controller GKE 1 2 3 4 5 kubectl apply \\ -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/mandatory.yaml kubectl apply \\ -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/provider/cloud-generic.yaml Minikube 1 minikube addons enable ingress Confirm ingress works Info As we're waiting for the LoadBalancer to be created by the Cloud Provider, we might have to repeat the command until we get a valid IP address as response. GKE 1 2 3 4 IP = $( kubectl -n ingress-nginx get svc ingress-nginx \\ -o jsonpath = {.status.loadBalancer.ingress[0].ip} ) echo $IP Minikube 1 2 IP = $( minikube ip ) curl -i http:// $IP /healthz Ingress based on paths 1 cat ingress/go-demo-2.yml 1 kubectl create -f ingress/go-demo-2.yml --record --save-config 1 kubectl rollout status deployment go-demo-2-api 1 curl -i http:// $IP /demo/hello Ingress based on domains 1 cat ingress/devops-toolkit-dom.yml 1 kubectl apply -f ingress/devops-toolkit-dom.yml --record 1 kubectl rollout status deployment devops-toolkit 1 curl -I -H Host: devopstoolkitseries.com http:// $IP 1 curl -I -H Host: acme.com http:// $IP /demo/hello Ingress with default backends 1 curl -I -H Host: acme.com http:// $IP 1 cat ingress/default-backend.yml 1 kubectl create -f ingress/default-backend.yml 1 curl -I -H Host: acme.com http:// $IP 1 open http:// $IP Cleanup 1 2 3 kubectl delete -f ingress/default-backend.yml kubectl delete -f ingress/devops-toolkit-dom.yml kubectl delete -f ingress/go-demo-2.yml","title":"Basics I"},{"location":"k8s/#kubernetes-basics-part-i","text":"","title":"Kubernetes Basics - Part I"},{"location":"k8s/#points-to-cover","text":"Building Docker Images Creating Pods Scaling Pods With ReplicaSets Using Services To Enable Communication Between Pods Deploying Releases With Zero-Downtime Using Ingress To Forward Traffic","title":"Points to cover"},{"location":"k8s/#prerequisites","text":"This guide borrows heavily from the workshops created by Viktor Farcic . The examples below are build on top of his example repository, so make sure you clone that. 1 2 git clone https://github.com/vfarcic/k8s-specs.git cd k8s-specs","title":"Prerequisites"},{"location":"k8s/#build-docker-images","text":"","title":"Build docker images"},{"location":"k8s/#docker","text":"","title":"Docker"},{"location":"k8s/#buildkit","text":"","title":"BuildKit"},{"location":"k8s/#alternatives","text":"","title":"Alternatives"},{"location":"k8s/#pod","text":"","title":"Pod"},{"location":"k8s/#view-yaml","text":"1 cat pod/db.yml","title":"View Yaml"},{"location":"k8s/#create","text":"1 kubectl create -f pod/db.yml","title":"Create"},{"location":"k8s/#view-pod","text":"1 kubectl get pods","title":"View Pod"},{"location":"k8s/#view-pod-alternatives","text":"Plural 1 kubectl get pods Singular 1 kubectl get pod Shorthand 1 kubectl get po YAML 1 kubectl get po db -o yaml JSON 1 kubectl get po db -o json JSON Path 1 kubectl get pod db -o jsonpath = {.metadata.name}","title":"View Pod Alternatives"},{"location":"k8s/#describe-pod","text":"1 kubectl describe pod db","title":"Describe Pod"},{"location":"k8s/#update-pod","text":"1 kubectl create -f pod/db.yml This should yield an error! 1 Error from server ( AlreadyExists ) : error when creating pod/db.yml : pods db already exists The create command is imperative and in this case not idempotent. To update we would either have to patch the resource in the cluster, or apply an updated version of the yaml file. 1 kubectl apply -f pod/db.yml","title":"Update Pod"},{"location":"k8s/#enter-the-pod","text":"1 kubectl exec db ps aux 1 kubectl exec -it db sh The command below should be executed from inside the pod. If you get something like command not found: mongo , double check the above command's success. 1 echo db.stats() | mongo localhost:27017/test 1 exit 1 kubectl logs db 1 kubectl exec -it db pkill mongod 1 kubectl get pods -w","title":"Enter the Pod"},{"location":"k8s/#cleanup","text":"1 kubectl delete -f pod/db.yml","title":"Cleanup"},{"location":"k8s/#replicaset","text":"","title":"ReplicaSet"},{"location":"k8s/#view-yaml_1","text":"1 cat rs/go-demo-2.yml","title":"View YAML"},{"location":"k8s/#create-replicaset","text":"1 kubectl create -f rs/go-demo-2.yml","title":"Create ReplicaSet"},{"location":"k8s/#view-replicaset","text":"1 kubectl get rs 1 kubectl describe -f rs/go-demo-2.yml Which should look something like this, notice the highlighted lines? 1 2 3 4 5 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal SuccessfulCreate 9s replicaset-controller Created pod: go-demo-2-4mw79 Normal SuccessfulCreate 9s replicaset-controller Created pod: go-demo-2-bcv5n","title":"View ReplicaSet"},{"location":"k8s/#view-pods","text":"","title":"View Pods"},{"location":"k8s/#delete-pod","text":"1 kubectl get pods 1 2 POD_NAME = $( kubectl get pods -o name | tail -1 ) kubectl delete $POD_NAME 1 kubectl get pods","title":"Delete Pod"},{"location":"k8s/#cleanup_1","text":"1 kubectl delete -f rs/go-demo-2.yml","title":"Cleanup"},{"location":"k8s/#labels","text":"","title":"Labels"},{"location":"k8s/#create-a-pods","text":"1 2 kubectl create -f rs/go-demo-2.yml kubectl create -f pod/db.yml","title":"Create a Pods"},{"location":"k8s/#get-pods","text":"1 kubectl get pods","title":"Get Pods"},{"location":"k8s/#get-it-by-label","text":"1 kubectl get pods -l type = backend","title":"Get it by Label"},{"location":"k8s/#get-it-by-multiple-labels","text":"1 kubectl get pods -l type = backend,language = go","title":"Get it by multiple labels"},{"location":"k8s/#show-labels","text":"1 kubectl get pods --show-labels","title":"Show Labels"},{"location":"k8s/#cleanup_2","text":"1 2 kubectl delete -f rs/go-demo-2.yml kubectl delete -f pod/db.yml","title":"Cleanup"},{"location":"k8s/#service","text":"","title":"Service"},{"location":"k8s/#view","text":"GKE/EKS 1 cat svc/go-demo-2-lb.yml Minikube 1 cat svc/go-demo-2.yml","title":"View"},{"location":"k8s/#create_1","text":"GKE/EKS 1 kubectl create -f svc/go-demo-2-lb.yml Minikube 1 kubectl create -f svc/go-demo-2.yml","title":"Create"},{"location":"k8s/#inspect","text":"1 kubectl get -f svc/go-demo-2.yml","title":"Inspect"},{"location":"k8s/#get-lb-ip","text":"EKS 1 2 3 #!/bin/bash IP = $( kubectl get svc go-demo-2-api \\ -o jsonpath = {.status.loadBalancer.ingress[0].hostname} ) GKE 1 2 3 #!/bin/bash IP = $( kubectl get svc go-demo-2-api \\ -o jsonpath = {.status.loadBalancer.ingress[0].ip} ) Minikube 1 2 #!/bin/bash IP = $( minikube ip ) All - after 1 echo $IP","title":"Get LB IP"},{"location":"k8s/#get-port","text":"GKE/EKS 1 PORT = 8080 Minikube 1 2 PORT = $( kubectl get svc go-demo-2-api \\ -o jsonpath = {.spec.ports[0].nodePort} )","title":"Get Port"},{"location":"k8s/#call-application","text":"curl 1 curl -i http:// $IP : $PORT /demo/hello HTTPie 1 http $IP : $PORT /demo/hello","title":"Call Application"},{"location":"k8s/#cleanup_3","text":"1 kubectl delete -f svc/go-demo-2.yml","title":"Cleanup"},{"location":"k8s/#deployment","text":"1 cat deploy/go-demo-2.yml 1 kubectl create -f deploy/go-demo-2.yml --record --save-config 1 kubectl describe deploy go-demo-2-api 1 kubectl get all","title":"Deployment"},{"location":"k8s/#zero-downtime-deployment","text":"1 2 kubectl set image deploy go-demo-2-api api = vfarcic/go-demo-2:2.0 \\ --record 1 kubectl rollout status -w deploy go-demo-2-api 1 kubectl describe deploy go-demo-2-api 1 kubectl rollout history deploy go-demo-2-api 1 kubectl get rs","title":"Zero downtime deployment"},{"location":"k8s/#rollback-rollforward","text":"1 kubectl rollout undo deploy go-demo-2-api 1 kubectl describe deploy go-demo-2-api 1 kubectl rollout history deploy go-demo-2-api 1 kubectl rollout undo -f deploy/go-demo-2-api.yml --to-revision = 2 1 kubectl rollout history deploy go-demo-2-api","title":"Rollback / Rollforward"},{"location":"k8s/#scaling","text":"1 kubectl scale deployment go-demo-2-api --replicas 8 --record 1 kubectl get pods","title":"Scaling"},{"location":"k8s/#cleanup_4","text":"1 kubectl delete -f deploy/go-demo-2.yml","title":"Cleanup"},{"location":"k8s/#ingress","text":"","title":"Ingress"},{"location":"k8s/#install-ingress-controller","text":"GKE 1 2 3 4 5 kubectl apply \\ -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/mandatory.yaml kubectl apply \\ -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/provider/cloud-generic.yaml Minikube 1 minikube addons enable ingress","title":"Install ingress controller"},{"location":"k8s/#confirm-ingress-works","text":"Info As we're waiting for the LoadBalancer to be created by the Cloud Provider, we might have to repeat the command until we get a valid IP address as response. GKE 1 2 3 4 IP = $( kubectl -n ingress-nginx get svc ingress-nginx \\ -o jsonpath = {.status.loadBalancer.ingress[0].ip} ) echo $IP Minikube 1 2 IP = $( minikube ip ) curl -i http:// $IP /healthz","title":"Confirm ingress works"},{"location":"k8s/#ingress-based-on-paths","text":"1 cat ingress/go-demo-2.yml 1 kubectl create -f ingress/go-demo-2.yml --record --save-config 1 kubectl rollout status deployment go-demo-2-api 1 curl -i http:// $IP /demo/hello","title":"Ingress based on paths"},{"location":"k8s/#ingress-based-on-domains","text":"1 cat ingress/devops-toolkit-dom.yml 1 kubectl apply -f ingress/devops-toolkit-dom.yml --record 1 kubectl rollout status deployment devops-toolkit 1 curl -I -H Host: devopstoolkitseries.com http:// $IP 1 curl -I -H Host: acme.com http:// $IP /demo/hello","title":"Ingress based on domains"},{"location":"k8s/#ingress-with-default-backends","text":"1 curl -I -H Host: acme.com http:// $IP 1 cat ingress/default-backend.yml 1 kubectl create -f ingress/default-backend.yml 1 curl -I -H Host: acme.com http:// $IP 1 open http:// $IP","title":"Ingress with default backends"},{"location":"k8s/#cleanup_5","text":"1 2 3 kubectl delete -f ingress/default-backend.yml kubectl delete -f ingress/devops-toolkit-dom.yml kubectl delete -f ingress/go-demo-2.yml","title":"Cleanup"},{"location":"k8s/create-cluster/","text":"Install Kubernetes in Public Cloud GKE 1 CLUSTER_NAME =[ ... ] # Change to a random name (e.g., your user) 1 gcloud auth login 1 2 3 4 5 6 7 8 gcloud container clusters \\ create $CLUSTER_NAME \\ --region us-east1 \\ --machine-type n1-standard-1 \\ --enable-autoscaling \\ --num-nodes 1 \\ --max-nodes 3 \\ --min-nodes 1 1 2 3 4 kubectl create clusterrolebinding \\ cluster-admin-binding \\ --clusterrole cluster-admin \\ --user $( gcloud config get-value account ) 1 kubectl get nodes -o wide 1 2 3 4 gcloud container clusters \\ delete $CLUSTER_NAME \\ --region us-east1 \\ --quiet","title":"Create Cluster"},{"location":"k8s/create-cluster/#install-kubernetes-in-public-cloud","text":"","title":"Install Kubernetes in Public Cloud"},{"location":"k8s/create-cluster/#gke","text":"1 CLUSTER_NAME =[ ... ] # Change to a random name (e.g., your user) 1 gcloud auth login 1 2 3 4 5 6 7 8 gcloud container clusters \\ create $CLUSTER_NAME \\ --region us-east1 \\ --machine-type n1-standard-1 \\ --enable-autoscaling \\ --num-nodes 1 \\ --max-nodes 3 \\ --min-nodes 1 1 2 3 4 kubectl create clusterrolebinding \\ cluster-admin-binding \\ --clusterrole cluster-admin \\ --user $( gcloud config get-value account ) 1 kubectl get nodes -o wide 1 2 3 4 gcloud container clusters \\ delete $CLUSTER_NAME \\ --region us-east1 \\ --quiet","title":"GKE"},{"location":"k8s/helm/","text":"","title":"Helm"},{"location":"k8s/knative-build/","text":"","title":"Knative Build"},{"location":"k8s/part2/","text":"Kubernetes Basics - Parts II Info This workshop segment expect you to be inside a cloned repository of k8s-specs . 1 2 git clone https://github.com/vfarcic/k8s-specs.git cd k8s-specs Points to cover Using ConfigMaps To Inject Configuration Files Using Secrets To Hide Confidential Information Dividing A Cluster Into Namespaces Securing Kubernetes Clusters Managing Resources ConfigMap Create from files 1 2 kubectl create cm my-config --from-file = cm/prometheus-conf.yml \\ --from-file = cm/prometheus.yml 1 cat cm/alpine.yml 1 kubectl create -f cm/alpine.yml 1 kubectl exec -it alpine -- ls /etc/config 1 kubectl exec -it alpine -- cat /etc/config/prometheus-conf.yml 1 kubectl delete -f cm/alpine.yml 1 kubectl delete cm my-config Create from literals 1 2 kubectl create cm my-config \\ --from-literal = something = else --from-literal = weather = sunny 1 kubectl create -f cm/alpine.yml 1 kubectl exec -it alpine -- ls /etc/config 1 kubectl exec -it alpine -- cat /etc/config/something 1 kubectl delete -f cm/alpine.yml 1 kubectl delete cm my-config Create from environment files 1 cat cm/my-env-file.yml 1 kubectl create cm my-config --from-env-file = cm/my-env-file.yml 1 kubectl get cm my-config -o yaml Secrets Generic secrets 1 2 kubectl create secret generic my-creds \\ --from-literal = username = jdoe --from-literal = password = incognito 1 kubectl get secrets 1 kubectl get secret my-creds -o json 1 2 kubectl get secret my-creds -o jsonpath = {.data.username} \\ | base64 --decode 1 2 kubectl get secret my-creds -o jsonpath = {.data.password} \\ | base64 --decode 1 cat secret/jenkins.yml 1 kubectl apply -f secret/jenkins.yml 1 kubectl rollout status deploy jenkins 1 2 POD_NAME = $( kubectl get pods -l service = jenkins,type = master \\ -o jsonpath = {.items[*].metadata.name} ) 1 kubectl exec -it $POD_NAME -- ls /etc/secrets 1 kubectl exec -it $POD_NAME -- cat /etc/secrets/jenkins-user 1 IP = $( minikube ip ) # If minikube 1 open http:// $IP /jenkins Cleanup 1 2 3 kubectl delete -f secret/jenkins.yml kubectl delete secret my-creds Namespaces Create initial release 1 cat ns/go-demo-2.yml 1 2 IMG = vfarcic/go-demo-2 TAG = 1 .0 1 2 cat ns/go-demo-2.yml | sed -e s@image: $IMG @image: $IMG : $TAG @g \\ | kubectl create -f - 1 kubectl rollout status deploy go-demo-2-api Retrieve existing Namespaces Shorthand way 1 kubectl get ns Verbose way 1 kubectl get namespaces Explore existing namespaces 1 kubectl -n kube-public get all 1 kubectl -n kube-system get all 1 kubectl -n default get all 1 kubectl get all Create new namespace 1 2 3 kubectl create ns testing kubectl get ns Change default namespace JX 1 jx ns testing Kubectx 1 kubens testing GKE 1 2 3 DEFAULT_CONTEXT = $( kubectl config current-context ) kubectl config set-context testing --namespace testing \\ --cluster $DEFAULT_CONTEXT --user $DEFAULT_CONTEXT EKS 1 2 3 kubectl config set-context testing --namespace testing \\ --cluster devops24. $AWS_DEFAULT_REGION .eksctl.io \\ --user iam-root-account@devops24. $AWS_DEFAULT_REGION .eksctl.io Minikube 1 2 kubectl config set-context testing --namespace testing \\ --cluster minikube --user minikube Deploy to a new namespace 1 kubectl config view 1 kubectl config use-context testing 1 kubectl get all 1 2 TAG = 2 .0 DOM = go-demo-2.com 1 2 3 cat ns/go-demo-2.yml | sed -e s@image: $IMG @image: $IMG : $TAG @g \\ | sed -e s@host: $DOM @host: $TAG \\. $DOM @g \\ | kubectl create -f - 1 kubectl rollout status deploy go-demo-2-api 1 curl -H Host: go-demo-2.com http:// $IP /demo/hello 1 curl -H Host: 2.0.go-demo-2.com http:// $IP /demo/hello Communicate accross namespaces Make sure we use the default namespace again. JX 1 jx ns default Kubectx 1 kubens default GKE 1 kubectl config use-context $DEFAULT_CONTEXT EKS 1 kubectl config use-context iam-root-account@devops24. $AWS_DEFAULT_REGION .eksctl.io Minikube 1 kubectl config use-context minikube Now we run a new container, and make sure it has curl. 1 kubectl run test --image = alpine --restart = Never sleep 10000 1 2 3 kubectl get pod test kubectl exec -it test -- apk add -U curl Now, lets exec into the container ( kubectl exec ) and use curl to test the services' DNS. Service in Default ns 1 kubectl exec -it test -- curl http://go-demo-2-api:8080/demo/hello Service in Testing ns 1 2 kubectl exec -it test \\ -- curl http://go-demo-2-api.testing:8080/demo/hello Delete a Namespace 1 kubectl delete ns testing 1 kubectl -n testing get all 1 kubectl get all 1 curl -H Host: go-demo-2.com http:// $IP /demo/hello 1 2 kubectl set image deployment/go-demo-2-api \\ api = vfarcic/go-demo-2:2.0 --record 1 curl -H Host: go-demo-2.com http:// $IP /demo/hello Cleanup 1 2 kubectl delete -f ns/go-demo-2.yml kubectl delete pod test Resource limits requests - SHORT View resources 1 cat res/go-demo-2-random.yml 1 kubectl create -f res/go-demo-2-random.yml --record --save-config 1 kubectl rollout status deployment go-demo-2-api 1 kubectl describe deploy go-demo-2-api 1 kubectl describe nodes Too little memory 1 cat res/go-demo-2-insuf-mem.yml 1 kubectl apply -f res/go-demo-2-insuf-mem.yml --record 1 kubectl get pods 1 kubectl describe pod go-demo-2-db Too much memory 1 cat res/go-demo-2-insuf-node.yml 1 kubectl apply -f res/go-demo-2-insuf-node.yml --record 1 kubectl get pods 1 kubectl describe pod go-demo-2-db Cleanup 1 kubectl delete -f res/go-demo-2-insuf-node.yml Securing Kubernetes GKE Go to Cloud Identity and Access Management Overview And Kubernetes Engine Creating Cloud IAM Policies Create a user and a cluster named jdoe When finished, continue from the Deploying go-demo-2 slide. Resource limits requests Enable Heapster Heapster enables Container Cluster Monitoring and Performance Analysis for Kubernetes (versions v1.0.6 and higher), and platforms which include it. Info GKE has heapster installed and enabled by default. Minkube 1 minikube addons enable heapster EKS 1 2 3 kubectl apply -f https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/heapster.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/influxdb.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/rbac/heapster-rbac.yaml Warning Heapster is now EOL, but still serves a purpose for this demo. Please do NOT use this in production systems. View resources 1 cat res/go-demo-2-random.yml 1 kubectl create -f res/go-demo-2-random.yml --record --save-config 1 kubectl rollout status deployment go-demo-2-api 1 kubectl describe deploy go-demo-2-api 1 kubectl describe nodes Expose heapster api endpoint Info The version of heapster might be different. Please confirm the actual name with the command below. 1 kubectl get deployment -n kube-system GKE 1 2 kubectl -n kube-system expose deployment heapster-v1.6.0-beta.1 \\ --name heapster-api --port 8082 --type LoadBalancer EKS 1 2 kubectl -n kube-system expose deployment heapster \\ --name heapster-api --port 8082 --type LoadBalancer Minikube 1 2 kubectl -n kube-system expose rc heapster \\ --name heapster-api --port 8082 --type NodePort Measure consumption Info You can also use a tool such as Kube Capacity for easier access to these metrics. 1 kubectl -n kube-system get pods 1 kubectl -n kube-system get svc heapster-api -o json Measure consumption 2 ALL (first) 1 2 3 PORT = $( kubectl -n kube-system get svc heapster-api \\ -o jsonpath = {.spec.ports[0].nodePort} ) PORT = 8082 GKE 1 2 ADDR = $( kubectl -n kube-system get svc heapster-api \\ -o jsonpath = {.status.loadBalancer.ingress[0].ip} ) EKS 1 2 ADDR = $( kubectl -n kube-system get svc heapster-api \\ -o jsonpath = {.status.loadBalancer.ingress[0].hostname} ) Minikube 1 ADDR = $( minikube ip ) Measure consumption 3 1 2 3 BASE_URL = http:// $ADDR : $PORT /api/v1/model/namespaces/default/pods curl $BASE_URL 1 2 DB_POD_NAME = $( kubectl get pods -l service = go-demo-2 -l type = db \\ -o jsonpath = {.items[0].metadata.name} ) 1 curl $BASE_URL / $DB_POD_NAME /containers/db/metrics 1 curl $BASE_URL / $DB_POD_NAME /containers/db/metrics/memory/usage 1 curl $BASE_URL / $DB_POD_NAME /containers/db/metrics/cpu/usage_rate Resource discrepancies 1 cat res/go-demo-2-insuf-mem.yml 1 kubectl apply -f res/go-demo-2-insuf-mem.yml --record 1 kubectl get pods 1 kubectl describe pod go-demo-2-db 1 cat res/go-demo-2-insuf-node.yml 1 kubectl apply -f res/go-demo-2-insuf-node.yml --record 1 kubectl get pods 1 kubectl describe pod go-demo-2-db Resource discrepancies 2 1 kubectl apply -f res/go-demo-2-random.yml --record 1 kubectl rollout status deployment go-demo-2-db 1 kubectl rollout status deployment go-demo-2-api Adjusting resources 1 2 DB_POD_NAME = $( kubectl get pods -l service = go-demo-2 \\ -l type = db -o jsonpath = {.items[0].metadata.name} ) 1 curl $BASE_URL / $DB_POD_NAME /containers/db/metrics/memory/usage 1 curl $BASE_URL / $DB_POD_NAME /containers/db/metrics/cpu/usage_rate 1 2 API_POD_NAME = $( kubectl get pods -l service = go-demo-2 \\ -l type = api -o jsonpath = {.items[0].metadata.name} ) 1 curl $BASE_URL / $API_POD_NAME /containers/api/metrics/memory/usage 1 curl $BASE_URL / $API_POD_NAME /containers/api/metrics/cpu/usage_rate Adjusting resources 2 1 cat res/go-demo-2.yml 1 kubectl apply -f res/go-demo-2.yml --record 1 kubectl rollout status deployment go-demo-2-api QOS 1 kubectl describe pod go-demo-2-db 1 cat res/go-demo-2-qos.yml 1 kubectl apply -f res/go-demo-2-qos.yml --record 1 kubectl rollout status deployment go-demo-2-db 1 kubectl describe pod go-demo-2-db 1 kubectl describe pod go-demo-2-api 1 kubectl delete -f res/go-demo-2-qos.yml Defaults Limitations 1 kubectl create namespace test 1 cat res/limit-range.yml 1 2 kubectl -n test create -f res/limit-range.yml \\ --save-config --record 1 kubectl describe namespace test 1 cat res/go-demo-2-no-res.yml 1 2 kubectl -n test create -f res/go-demo-2-no-res.yml \\ --save-config --record 1 kubectl -n test rollout status deployment go-demo-2-api Defaults Limitations 2 1 kubectl -n test describe pod go-demo-2-db 1 cat res/go-demo-2.yml 1 kubectl -n test apply -f res/go-demo-2.yml --record 1 kubectl -n test get events -w 1 2 kubectl -n test run test --image alpine --requests memory = 100Mi \\ --restart Never sleep 10000 1 2 kubectl -n test run test --image alpine --requests memory = 1Mi \\ --restart Never sleep 10000 1 kubectl delete namespace test Resource Quotas 1 cat res/dev.yml 1 kubectl create -f res/dev.yml --record --save-config 1 kubectl -n dev describe quota dev 1 kubectl -n dev create -f res/go-demo-2.yml --save-config --record 1 kubectl -n dev rollout status deployment go-demo-2-api 1 kubectl -n dev describe quota dev Resource Quotas 2 1 cat res/go-demo-2-scaled.yml 1 kubectl -n dev apply -f res/go-demo-2-scaled.yml --record 1 kubectl -n dev get events 1 kubectl describe namespace dev 1 kubectl get pods -n dev 1 kubectl -n dev apply -f res/go-demo-2.yml --record 1 kubectl -n dev rollout status deployment go-demo-2-api Resource Quotas 3 1 cat res/go-demo-2-mem.yml 1 kubectl -n dev apply -f res/go-demo-2-mem.yml --record 1 kubectl -n dev get events | grep mem 1 kubectl describe namespace dev 1 kubectl -n dev apply -f res/go-demo-2.yml --record 1 kubectl -n dev rollout status deployment go-demo-2-api 1 2 kubectl expose deployment go-demo-2-api -n dev \\ --name go-demo-2-api --port 8080 --type NodePort Cleanup 1 kubectl delete ns dev","title":"Basics II"},{"location":"k8s/part2/#kubernetes-basics-parts-ii","text":"Info This workshop segment expect you to be inside a cloned repository of k8s-specs . 1 2 git clone https://github.com/vfarcic/k8s-specs.git cd k8s-specs","title":"Kubernetes Basics - Parts II"},{"location":"k8s/part2/#points-to-cover","text":"Using ConfigMaps To Inject Configuration Files Using Secrets To Hide Confidential Information Dividing A Cluster Into Namespaces Securing Kubernetes Clusters Managing Resources","title":"Points to cover"},{"location":"k8s/part2/#configmap","text":"","title":"ConfigMap"},{"location":"k8s/part2/#create-from-files","text":"1 2 kubectl create cm my-config --from-file = cm/prometheus-conf.yml \\ --from-file = cm/prometheus.yml 1 cat cm/alpine.yml 1 kubectl create -f cm/alpine.yml 1 kubectl exec -it alpine -- ls /etc/config 1 kubectl exec -it alpine -- cat /etc/config/prometheus-conf.yml 1 kubectl delete -f cm/alpine.yml 1 kubectl delete cm my-config","title":"Create from files"},{"location":"k8s/part2/#create-from-literals","text":"1 2 kubectl create cm my-config \\ --from-literal = something = else --from-literal = weather = sunny 1 kubectl create -f cm/alpine.yml 1 kubectl exec -it alpine -- ls /etc/config 1 kubectl exec -it alpine -- cat /etc/config/something 1 kubectl delete -f cm/alpine.yml 1 kubectl delete cm my-config","title":"Create from literals"},{"location":"k8s/part2/#create-from-environment-files","text":"1 cat cm/my-env-file.yml 1 kubectl create cm my-config --from-env-file = cm/my-env-file.yml 1 kubectl get cm my-config -o yaml","title":"Create from environment files"},{"location":"k8s/part2/#secrets","text":"","title":"Secrets"},{"location":"k8s/part2/#generic-secrets","text":"1 2 kubectl create secret generic my-creds \\ --from-literal = username = jdoe --from-literal = password = incognito 1 kubectl get secrets 1 kubectl get secret my-creds -o json 1 2 kubectl get secret my-creds -o jsonpath = {.data.username} \\ | base64 --decode 1 2 kubectl get secret my-creds -o jsonpath = {.data.password} \\ | base64 --decode 1 cat secret/jenkins.yml 1 kubectl apply -f secret/jenkins.yml 1 kubectl rollout status deploy jenkins 1 2 POD_NAME = $( kubectl get pods -l service = jenkins,type = master \\ -o jsonpath = {.items[*].metadata.name} ) 1 kubectl exec -it $POD_NAME -- ls /etc/secrets 1 kubectl exec -it $POD_NAME -- cat /etc/secrets/jenkins-user 1 IP = $( minikube ip ) # If minikube 1 open http:// $IP /jenkins","title":"Generic secrets"},{"location":"k8s/part2/#cleanup","text":"1 2 3 kubectl delete -f secret/jenkins.yml kubectl delete secret my-creds","title":"Cleanup"},{"location":"k8s/part2/#namespaces","text":"","title":"Namespaces"},{"location":"k8s/part2/#create-initial-release","text":"1 cat ns/go-demo-2.yml 1 2 IMG = vfarcic/go-demo-2 TAG = 1 .0 1 2 cat ns/go-demo-2.yml | sed -e s@image: $IMG @image: $IMG : $TAG @g \\ | kubectl create -f - 1 kubectl rollout status deploy go-demo-2-api","title":"Create initial release"},{"location":"k8s/part2/#retrieve-existing-namespaces","text":"Shorthand way 1 kubectl get ns Verbose way 1 kubectl get namespaces","title":"Retrieve existing Namespaces"},{"location":"k8s/part2/#explore-existing-namespaces","text":"1 kubectl -n kube-public get all 1 kubectl -n kube-system get all 1 kubectl -n default get all 1 kubectl get all","title":"Explore existing namespaces"},{"location":"k8s/part2/#create-new-namespace","text":"1 2 3 kubectl create ns testing kubectl get ns","title":"Create new namespace"},{"location":"k8s/part2/#change-default-namespace","text":"JX 1 jx ns testing Kubectx 1 kubens testing GKE 1 2 3 DEFAULT_CONTEXT = $( kubectl config current-context ) kubectl config set-context testing --namespace testing \\ --cluster $DEFAULT_CONTEXT --user $DEFAULT_CONTEXT EKS 1 2 3 kubectl config set-context testing --namespace testing \\ --cluster devops24. $AWS_DEFAULT_REGION .eksctl.io \\ --user iam-root-account@devops24. $AWS_DEFAULT_REGION .eksctl.io Minikube 1 2 kubectl config set-context testing --namespace testing \\ --cluster minikube --user minikube","title":"Change default namespace"},{"location":"k8s/part2/#deploy-to-a-new-namespace","text":"1 kubectl config view 1 kubectl config use-context testing 1 kubectl get all 1 2 TAG = 2 .0 DOM = go-demo-2.com 1 2 3 cat ns/go-demo-2.yml | sed -e s@image: $IMG @image: $IMG : $TAG @g \\ | sed -e s@host: $DOM @host: $TAG \\. $DOM @g \\ | kubectl create -f - 1 kubectl rollout status deploy go-demo-2-api 1 curl -H Host: go-demo-2.com http:// $IP /demo/hello 1 curl -H Host: 2.0.go-demo-2.com http:// $IP /demo/hello","title":"Deploy to a new namespace"},{"location":"k8s/part2/#communicate-accross-namespaces","text":"Make sure we use the default namespace again. JX 1 jx ns default Kubectx 1 kubens default GKE 1 kubectl config use-context $DEFAULT_CONTEXT EKS 1 kubectl config use-context iam-root-account@devops24. $AWS_DEFAULT_REGION .eksctl.io Minikube 1 kubectl config use-context minikube Now we run a new container, and make sure it has curl. 1 kubectl run test --image = alpine --restart = Never sleep 10000 1 2 3 kubectl get pod test kubectl exec -it test -- apk add -U curl Now, lets exec into the container ( kubectl exec ) and use curl to test the services' DNS. Service in Default ns 1 kubectl exec -it test -- curl http://go-demo-2-api:8080/demo/hello Service in Testing ns 1 2 kubectl exec -it test \\ -- curl http://go-demo-2-api.testing:8080/demo/hello","title":"Communicate accross namespaces"},{"location":"k8s/part2/#delete-a-namespace","text":"1 kubectl delete ns testing 1 kubectl -n testing get all 1 kubectl get all 1 curl -H Host: go-demo-2.com http:// $IP /demo/hello 1 2 kubectl set image deployment/go-demo-2-api \\ api = vfarcic/go-demo-2:2.0 --record 1 curl -H Host: go-demo-2.com http:// $IP /demo/hello","title":"Delete a Namespace"},{"location":"k8s/part2/#cleanup_1","text":"1 2 kubectl delete -f ns/go-demo-2.yml kubectl delete pod test","title":"Cleanup"},{"location":"k8s/part2/#resource-limits-requests-short","text":"","title":"Resource limits &amp; requests - SHORT"},{"location":"k8s/part2/#view-resources","text":"1 cat res/go-demo-2-random.yml 1 kubectl create -f res/go-demo-2-random.yml --record --save-config 1 kubectl rollout status deployment go-demo-2-api 1 kubectl describe deploy go-demo-2-api 1 kubectl describe nodes","title":"View resources"},{"location":"k8s/part2/#too-little-memory","text":"1 cat res/go-demo-2-insuf-mem.yml 1 kubectl apply -f res/go-demo-2-insuf-mem.yml --record 1 kubectl get pods 1 kubectl describe pod go-demo-2-db","title":"Too little memory"},{"location":"k8s/part2/#too-much-memory","text":"1 cat res/go-demo-2-insuf-node.yml 1 kubectl apply -f res/go-demo-2-insuf-node.yml --record 1 kubectl get pods 1 kubectl describe pod go-demo-2-db","title":"Too much memory"},{"location":"k8s/part2/#cleanup_2","text":"1 kubectl delete -f res/go-demo-2-insuf-node.yml","title":"Cleanup"},{"location":"k8s/part2/#securing-kubernetes","text":"","title":"Securing Kubernetes"},{"location":"k8s/part2/#gke","text":"Go to Cloud Identity and Access Management Overview And Kubernetes Engine Creating Cloud IAM Policies Create a user and a cluster named jdoe When finished, continue from the Deploying go-demo-2 slide.","title":"GKE"},{"location":"k8s/part2/#resource-limits-requests","text":"","title":"Resource limits &amp; requests"},{"location":"k8s/part2/#enable-heapster","text":"Heapster enables Container Cluster Monitoring and Performance Analysis for Kubernetes (versions v1.0.6 and higher), and platforms which include it. Info GKE has heapster installed and enabled by default. Minkube 1 minikube addons enable heapster EKS 1 2 3 kubectl apply -f https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/heapster.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/influxdb.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/rbac/heapster-rbac.yaml Warning Heapster is now EOL, but still serves a purpose for this demo. Please do NOT use this in production systems.","title":"Enable Heapster"},{"location":"k8s/part2/#view-resources_1","text":"1 cat res/go-demo-2-random.yml 1 kubectl create -f res/go-demo-2-random.yml --record --save-config 1 kubectl rollout status deployment go-demo-2-api 1 kubectl describe deploy go-demo-2-api 1 kubectl describe nodes","title":"View resources"},{"location":"k8s/part2/#expose-heapster-api-endpoint","text":"Info The version of heapster might be different. Please confirm the actual name with the command below. 1 kubectl get deployment -n kube-system GKE 1 2 kubectl -n kube-system expose deployment heapster-v1.6.0-beta.1 \\ --name heapster-api --port 8082 --type LoadBalancer EKS 1 2 kubectl -n kube-system expose deployment heapster \\ --name heapster-api --port 8082 --type LoadBalancer Minikube 1 2 kubectl -n kube-system expose rc heapster \\ --name heapster-api --port 8082 --type NodePort","title":"Expose heapster api endpoint"},{"location":"k8s/part2/#measure-consumption","text":"Info You can also use a tool such as Kube Capacity for easier access to these metrics. 1 kubectl -n kube-system get pods 1 kubectl -n kube-system get svc heapster-api -o json","title":"Measure consumption"},{"location":"k8s/part2/#measure-consumption-2","text":"ALL (first) 1 2 3 PORT = $( kubectl -n kube-system get svc heapster-api \\ -o jsonpath = {.spec.ports[0].nodePort} ) PORT = 8082 GKE 1 2 ADDR = $( kubectl -n kube-system get svc heapster-api \\ -o jsonpath = {.status.loadBalancer.ingress[0].ip} ) EKS 1 2 ADDR = $( kubectl -n kube-system get svc heapster-api \\ -o jsonpath = {.status.loadBalancer.ingress[0].hostname} ) Minikube 1 ADDR = $( minikube ip )","title":"Measure consumption 2"},{"location":"k8s/part2/#measure-consumption-3","text":"1 2 3 BASE_URL = http:// $ADDR : $PORT /api/v1/model/namespaces/default/pods curl $BASE_URL 1 2 DB_POD_NAME = $( kubectl get pods -l service = go-demo-2 -l type = db \\ -o jsonpath = {.items[0].metadata.name} ) 1 curl $BASE_URL / $DB_POD_NAME /containers/db/metrics 1 curl $BASE_URL / $DB_POD_NAME /containers/db/metrics/memory/usage 1 curl $BASE_URL / $DB_POD_NAME /containers/db/metrics/cpu/usage_rate","title":"Measure consumption 3"},{"location":"k8s/part2/#resource-discrepancies","text":"1 cat res/go-demo-2-insuf-mem.yml 1 kubectl apply -f res/go-demo-2-insuf-mem.yml --record 1 kubectl get pods 1 kubectl describe pod go-demo-2-db 1 cat res/go-demo-2-insuf-node.yml 1 kubectl apply -f res/go-demo-2-insuf-node.yml --record 1 kubectl get pods 1 kubectl describe pod go-demo-2-db","title":"Resource discrepancies"},{"location":"k8s/part2/#resource-discrepancies-2","text":"1 kubectl apply -f res/go-demo-2-random.yml --record 1 kubectl rollout status deployment go-demo-2-db 1 kubectl rollout status deployment go-demo-2-api","title":"Resource discrepancies 2"},{"location":"k8s/part2/#adjusting-resources","text":"1 2 DB_POD_NAME = $( kubectl get pods -l service = go-demo-2 \\ -l type = db -o jsonpath = {.items[0].metadata.name} ) 1 curl $BASE_URL / $DB_POD_NAME /containers/db/metrics/memory/usage 1 curl $BASE_URL / $DB_POD_NAME /containers/db/metrics/cpu/usage_rate 1 2 API_POD_NAME = $( kubectl get pods -l service = go-demo-2 \\ -l type = api -o jsonpath = {.items[0].metadata.name} ) 1 curl $BASE_URL / $API_POD_NAME /containers/api/metrics/memory/usage 1 curl $BASE_URL / $API_POD_NAME /containers/api/metrics/cpu/usage_rate","title":"Adjusting resources"},{"location":"k8s/part2/#adjusting-resources-2","text":"1 cat res/go-demo-2.yml 1 kubectl apply -f res/go-demo-2.yml --record 1 kubectl rollout status deployment go-demo-2-api","title":"Adjusting resources 2"},{"location":"k8s/part2/#qos","text":"1 kubectl describe pod go-demo-2-db 1 cat res/go-demo-2-qos.yml 1 kubectl apply -f res/go-demo-2-qos.yml --record 1 kubectl rollout status deployment go-demo-2-db 1 kubectl describe pod go-demo-2-db 1 kubectl describe pod go-demo-2-api 1 kubectl delete -f res/go-demo-2-qos.yml","title":"QOS"},{"location":"k8s/part2/#defaults-limitations","text":"1 kubectl create namespace test 1 cat res/limit-range.yml 1 2 kubectl -n test create -f res/limit-range.yml \\ --save-config --record 1 kubectl describe namespace test 1 cat res/go-demo-2-no-res.yml 1 2 kubectl -n test create -f res/go-demo-2-no-res.yml \\ --save-config --record 1 kubectl -n test rollout status deployment go-demo-2-api","title":"Defaults &amp; Limitations"},{"location":"k8s/part2/#defaults-limitations-2","text":"1 kubectl -n test describe pod go-demo-2-db 1 cat res/go-demo-2.yml 1 kubectl -n test apply -f res/go-demo-2.yml --record 1 kubectl -n test get events -w 1 2 kubectl -n test run test --image alpine --requests memory = 100Mi \\ --restart Never sleep 10000 1 2 kubectl -n test run test --image alpine --requests memory = 1Mi \\ --restart Never sleep 10000 1 kubectl delete namespace test","title":"Defaults &amp; Limitations 2"},{"location":"k8s/part2/#resource-quotas","text":"1 cat res/dev.yml 1 kubectl create -f res/dev.yml --record --save-config 1 kubectl -n dev describe quota dev 1 kubectl -n dev create -f res/go-demo-2.yml --save-config --record 1 kubectl -n dev rollout status deployment go-demo-2-api 1 kubectl -n dev describe quota dev","title":"Resource Quotas"},{"location":"k8s/part2/#resource-quotas-2","text":"1 cat res/go-demo-2-scaled.yml 1 kubectl -n dev apply -f res/go-demo-2-scaled.yml --record 1 kubectl -n dev get events 1 kubectl describe namespace dev 1 kubectl get pods -n dev 1 kubectl -n dev apply -f res/go-demo-2.yml --record 1 kubectl -n dev rollout status deployment go-demo-2-api","title":"Resource Quotas 2"},{"location":"k8s/part2/#resource-quotas-3","text":"1 cat res/go-demo-2-mem.yml 1 kubectl -n dev apply -f res/go-demo-2-mem.yml --record 1 kubectl -n dev get events | grep mem 1 kubectl describe namespace dev 1 kubectl -n dev apply -f res/go-demo-2.yml --record 1 kubectl -n dev rollout status deployment go-demo-2-api 1 2 kubectl expose deployment go-demo-2-api -n dev \\ --name go-demo-2-api --port 8080 --type NodePort","title":"Resource Quotas 3"},{"location":"k8s/part2/#cleanup_3","text":"1 kubectl delete ns dev","title":"Cleanup"},{"location":"k8s/part3/","text":"Kubernetes Basics - Part III Info This workshop segment expect you to be inside a cloned repository of k8s-specs . 1 2 git clone https://github.com/vfarcic/k8s-specs.git cd k8s-specs Points to cover Persisting State Deploying Stateful Applications At Scale Persisting State Without state preservation 1 cat pv/jenkins-no-pv.yml 1 kubectl create -f pv/jenkins-no-pv.yml --record --save-config 1 kubectl -n jenkins get events 1 2 3 kubectl -n jenkins create secret generic jenkins-creds \\ --from-literal = jenkins-user = jdoe \\ --from-literal = jenkins-pass = incognito 1 kubectl -n jenkins rollout status deployment jenkins Retrieve Jenkins Address GKE 1 2 JENKINS_ADDR = $( kubectl -n jenkins get ing jenkins \\ -o jsonpath = {.status.loadBalancer.ingress[0].ip} ) EKS 1 2 JENKINS_ADDR = $( kubectl -n jenkins get ing jenkins \\ -o jsonpath = {.status.loadBalancer.ingress[0].hostname} ) Minikube 1 JENKINS_ADDR = $( minikube ip ) Log into Jenkins Warning If you're on windows, open probably doesn't work. Copy paste the url printed and open that in a browser. 1 2 echo $JENKINS_ADDR open http:// $JENKINS_ADDR /jenkins use jdoe as username and incognito as password create a job, doesn't matter what Kill the pod 1 kubectl -n jenkins get pods --selector = app = jenkins -o json 1 2 POD_NAME = $( kubectl -n jenkins get pods --selector = app = jenkins \\ -o jsonpath = {.items[*].metadata.name} ) 1 echo $POD_NAME 1 kubectl -n jenkins exec -it $POD_NAME pkill java 1 2 echo http:// $JENKINS_ADDR /jenkins open http:// $JENKINS_ADDR /jenkins Create Volume GKE 1 CLUSTER_NAME = 1 2 gcloud compute instances list --filter = name:( ${ CLUSTER_NAME } ) \\ --format csv[no-heading](zone) | tee zones 1 AZ_1 = $( cat zones | head -n 1 ) 1 AZ_2 = $( cat zones | tail -n 2 | head -n 1 ) 1 AZ_3 = $( cat zones | tail -n 1 ) Replace ??? with your name to make sure the disk name is unique. 1 PREFIX = ??? 1 gcloud compute disks create ${ PREFIX } -disk1 --zone $AZ_1 1 gcloud compute disks create ${ PREFIX } -disk2 --zone $AZ_2 1 gcloud compute disks create ${ PREFIX } -disk3 --zone $AZ_3 Warning Later commands will depend on these variables. So stay in the same console session or make sure you recreate these! 1 2 3 VOLUME_ID_1 = ${ PREFIX } -disk1 VOLUME_ID_2 = ${ PREFIX } -disk2 VOLUME_ID_3 = ${ PREFIX } -disk3 1 gcloud compute disks describe VOLUME_ID_1 EKS ... Create Persistent Volume GKE 1 2 YAML = pv/pv-gke.yml cat $YAML 1 2 3 4 5 cat $YAML \\ | sed -e s@REPLACE_ME_1@ $VOLUME_ID_1 @g \\ | sed -e s@REPLACE_ME_2@ $VOLUME_ID_2 @g \\ | sed -e s@REPLACE_ME_3@ $VOLUME_ID_3 @g \\ | kubectl create -f - --save-config --record 1 kubectl get pv Claim Persistent Volume 1 cat pv/pvc.yml 1 kubectl create -f pv/pvc.yml --save-config --record 1 kubectl -n jenkins get pvc 1 kubectl get pv Attach PVC 1 cat pv/jenkins-pv.yml 1 kubectl apply -f pv/jenkins-pv.yml --record 1 kubectl -n jenkins rollout status deployment jenkins Demonstrate the persistent part open Jenkins open \"http://$JENKINS_ADDR/jenkins\" create a job 1 2 POD_NAME = $( kubectl -n jenkins get pod --selector = app = jenkins \\ -o jsonpath = {.items[*].metadata.name} ) 1 kubectl -n jenkins exec -it $POD_NAME pkill java Let's delete Jenkins' deployment. 1 kubectl -n jenkins delete deploy jenkins Confirm it is gone. 1 kubectl get all -n jenkins Now, let's recreate Jenkins. 1 kubectl apply -f pv/jenkins-pv.yml --record 1 kubectl -n jenkins rollout status deployment jenkins confirm job is still there open Jenkins open \"http://$JENKINS_ADDR/jenkins\" 1 kubectl -n jenkins get pvc 1 kubectl get pv Cleanup ALL 1 2 3 kubectl -n jenkins delete pvc jenkins kubectl get pv kubectl delete -f pv/pv.yml GKE 1 2 3 gcloud compute disks delete $VOLUME_ID_1 --zone $AZ_1 --quiet gcloud compute disks delete $VOLUME_ID_2 --zone $AZ_2 --quiet gcloud compute disks delete $VOLUME_ID_3 --zone $AZ_3 --quiet EKS 1 2 3 aws ec2 delete-volume --volume-id $VOLUME_ID_1 aws ec2 delete-volume --volume-id $VOLUME_ID_2 aws ec2 delete-volume --volume-id $VOLUME_ID_3 Storage Classes View ALL - before others 1 kubectl get sc GKE 1 cat pv/jenkins-dynamic-gke.yml EKS 1 cat pv/jenkins-dynamic.yml Create GKE 1 kubectl apply -f pv/jenkins-dynamic-gke.yml --record EKS 1 kubectl apply -f pv/jenkins-dynamic.yml --record ALL - after others 1 kubectl -n jenkins rollout status deployment jenkins Use ALL - before others 1 2 3 4 5 kubectl -n jenkins get events kubectl -n jenkins get pvc kubectl get pv GKE 1 2 PV_NAME = $( kubectl get pv -o jsonpath = {.items[0].metadata.name} ) gcloud compute disks list --filter = name:( $PV_NAME ) EKS 1 2 aws ec2 describe-volumes \\ --filters Name=tag-key,Values= kubernetes.io/created-for/pvc/name Use 2 ALL - before others 1 2 3 kubectl -n jenkins delete deploy,pvc jenkins kubectl get pv GKE 1 gcloud compute disks list --filter = name:( $PV_NAME ) EKS 1 2 aws ec2 describe-volumes \\ --filters Name=tag-key,Values= kubernetes.io/created-for/pvc/name Use Default 1 kubectl get sc 1 kubectl describe sc 1 cat pv/jenkins-default.yml 1 diff pv/jenkins-dynamic.yml pv/jenkins-default.yml 1 kubectl apply -f pv/jenkins-default.yml --record 1 kubectl get pv Prepare alternative SC All - before others 1 kubectl -n jenkins delete deploy,pvc jenkins GKE 1 YAML = sc-gke.yml EKS 1 YAML = sc.yml Create alternative SC 1 cat pv/ $YAML 1 kubectl create -f pv/ $YAML 1 kubectl get sc Use alternative SC All - before others 1 2 cat pv/jenkins-sc.yml kubectl apply -f pv/jenkins-sc.yml --record EKS 1 2 aws ec2 describe-volumes \\ --filters Name=tag-key,Values= kubernetes.io/created-for/pvc/name GKE 1 2 PV_NAME = $( kubectl get pv -o jsonpath = {.items[0].metadata.name} ) gcloud compute disks list --filter = name:( $PV_NAME ) Cleanup 1 kubectl delete ns jenkins 1 kubectl delete sc fast StatefulSet Create StatefulSwet 1 cat sts/jenkins.yml 1 kubectl apply -f sts/jenkins.yml --record 1 kubectl -n jenkins rollout status sts jenkins 1 kubectl -n jenkins get pvc 1 kubectl -n jenkins get pv Get Jenkins Address GKE 1 2 JENKINS_ADDR = $( kubectl -n jenkins get ing jenkins \\ -o jsonpath = {.status.loadBalancer.ingress[0].ip} ) EKS 1 2 JENKINS_ADDR = $( kubectl -n jenkins get ing jenkins \\ -o jsonpath = {.status.loadBalancer.ingress[0].hostname} ) Use Jenkins 1 open http:// $JENKINS_ADDR /jenkins 1 kubectl delete ns jenkins Use DB without STS 1 cat sts/go-demo-3-deploy.yml 1 kubectl apply -f sts/go-demo-3-deploy.yml --record 1 kubectl -n go-demo-3 rollout status deployment api 1 kubectl -n go-demo-3 get pods 1 2 DB_1 = $( kubectl -n go-demo-3 get pods -l app = db \\ -o jsonpath = {.items[0].metadata.name} ) 1 2 DB_2 = $( kubectl -n go-demo-3 get pods -l app = db \\ -o jsonpath = {.items[1].metadata.name} ) Investigate problems 1 kubectl -n go-demo-3 logs $DB_1 1 kubectl -n go-demo-3 logs $DB_2 1 kubectl get pv 1 kubectl delete ns go-demo-3 Use DB with STS 1 cat sts/go-demo-3-sts.yml 1 kubectl apply -f sts/go-demo-3-sts.yml --record 1 kubectl -n go-demo-3 get pods 1 kubectl get pv Configure Mongo If we want MongoDB to use the three instances as single dataplane, we have to tell it the dataplane members. First, we shell into one of the db containers via exec . 1 kubectl -n go-demo-3 exec -it db-0 -- sh Then, make sure we're talking with MongoDB, via its REPL, with mongo . 1 mongo And finally we explain MongoDB what we want to do. 1 2 3 4 5 6 7 8 rs.initiate ( { _id : rs0 , members: [ { _id: 0 , host: db-0.db:27017 } , { _id: 1 , host: db-1.db:27017 } , { _id: 2 , host: db-2.db:27017 } ] }) Let's confirm with MongoDB before we exit the container. 1 rs.status () And now you're free to exit the MongoDB REPL and the container via ctrl+d (so twice). 1 kubectl -n go-demo-3 get pods Observe update process 1 diff sts/go-demo-3-sts.yml sts/go-demo-3-sts-upd.yml 1 kubectl apply -f sts/go-demo-3-sts-upd.yml --record 1 kubectl -n go-demo-3 get pods Cleanup 1 kubectl delete ns go-demo-3","title":"Basics III"},{"location":"k8s/part3/#kubernetes-basics-part-iii","text":"Info This workshop segment expect you to be inside a cloned repository of k8s-specs . 1 2 git clone https://github.com/vfarcic/k8s-specs.git cd k8s-specs","title":"Kubernetes Basics - Part III"},{"location":"k8s/part3/#points-to-cover","text":"Persisting State Deploying Stateful Applications At Scale","title":"Points to cover"},{"location":"k8s/part3/#persisting-state","text":"","title":"Persisting State"},{"location":"k8s/part3/#without-state-preservation","text":"1 cat pv/jenkins-no-pv.yml 1 kubectl create -f pv/jenkins-no-pv.yml --record --save-config 1 kubectl -n jenkins get events 1 2 3 kubectl -n jenkins create secret generic jenkins-creds \\ --from-literal = jenkins-user = jdoe \\ --from-literal = jenkins-pass = incognito 1 kubectl -n jenkins rollout status deployment jenkins","title":"Without state preservation"},{"location":"k8s/part3/#retrieve-jenkins-address","text":"GKE 1 2 JENKINS_ADDR = $( kubectl -n jenkins get ing jenkins \\ -o jsonpath = {.status.loadBalancer.ingress[0].ip} ) EKS 1 2 JENKINS_ADDR = $( kubectl -n jenkins get ing jenkins \\ -o jsonpath = {.status.loadBalancer.ingress[0].hostname} ) Minikube 1 JENKINS_ADDR = $( minikube ip )","title":"Retrieve Jenkins Address"},{"location":"k8s/part3/#log-into-jenkins","text":"Warning If you're on windows, open probably doesn't work. Copy paste the url printed and open that in a browser. 1 2 echo $JENKINS_ADDR open http:// $JENKINS_ADDR /jenkins use jdoe as username and incognito as password create a job, doesn't matter what","title":"Log into Jenkins"},{"location":"k8s/part3/#kill-the-pod","text":"1 kubectl -n jenkins get pods --selector = app = jenkins -o json 1 2 POD_NAME = $( kubectl -n jenkins get pods --selector = app = jenkins \\ -o jsonpath = {.items[*].metadata.name} ) 1 echo $POD_NAME 1 kubectl -n jenkins exec -it $POD_NAME pkill java 1 2 echo http:// $JENKINS_ADDR /jenkins open http:// $JENKINS_ADDR /jenkins","title":"Kill the pod"},{"location":"k8s/part3/#create-volume","text":"","title":"Create Volume"},{"location":"k8s/part3/#gke","text":"1 CLUSTER_NAME = 1 2 gcloud compute instances list --filter = name:( ${ CLUSTER_NAME } ) \\ --format csv[no-heading](zone) | tee zones 1 AZ_1 = $( cat zones | head -n 1 ) 1 AZ_2 = $( cat zones | tail -n 2 | head -n 1 ) 1 AZ_3 = $( cat zones | tail -n 1 ) Replace ??? with your name to make sure the disk name is unique. 1 PREFIX = ??? 1 gcloud compute disks create ${ PREFIX } -disk1 --zone $AZ_1 1 gcloud compute disks create ${ PREFIX } -disk2 --zone $AZ_2 1 gcloud compute disks create ${ PREFIX } -disk3 --zone $AZ_3 Warning Later commands will depend on these variables. So stay in the same console session or make sure you recreate these! 1 2 3 VOLUME_ID_1 = ${ PREFIX } -disk1 VOLUME_ID_2 = ${ PREFIX } -disk2 VOLUME_ID_3 = ${ PREFIX } -disk3 1 gcloud compute disks describe VOLUME_ID_1","title":"GKE"},{"location":"k8s/part3/#eks","text":"...","title":"EKS"},{"location":"k8s/part3/#create-persistent-volume","text":"","title":"Create Persistent Volume"},{"location":"k8s/part3/#gke_1","text":"1 2 YAML = pv/pv-gke.yml cat $YAML 1 2 3 4 5 cat $YAML \\ | sed -e s@REPLACE_ME_1@ $VOLUME_ID_1 @g \\ | sed -e s@REPLACE_ME_2@ $VOLUME_ID_2 @g \\ | sed -e s@REPLACE_ME_3@ $VOLUME_ID_3 @g \\ | kubectl create -f - --save-config --record 1 kubectl get pv","title":"GKE"},{"location":"k8s/part3/#claim-persistent-volume","text":"1 cat pv/pvc.yml 1 kubectl create -f pv/pvc.yml --save-config --record 1 kubectl -n jenkins get pvc 1 kubectl get pv","title":"Claim Persistent Volume"},{"location":"k8s/part3/#attach-pvc","text":"1 cat pv/jenkins-pv.yml 1 kubectl apply -f pv/jenkins-pv.yml --record 1 kubectl -n jenkins rollout status deployment jenkins","title":"Attach PVC"},{"location":"k8s/part3/#demonstrate-the-persistent-part","text":"open Jenkins open \"http://$JENKINS_ADDR/jenkins\" create a job 1 2 POD_NAME = $( kubectl -n jenkins get pod --selector = app = jenkins \\ -o jsonpath = {.items[*].metadata.name} ) 1 kubectl -n jenkins exec -it $POD_NAME pkill java Let's delete Jenkins' deployment. 1 kubectl -n jenkins delete deploy jenkins Confirm it is gone. 1 kubectl get all -n jenkins Now, let's recreate Jenkins. 1 kubectl apply -f pv/jenkins-pv.yml --record 1 kubectl -n jenkins rollout status deployment jenkins confirm job is still there open Jenkins open \"http://$JENKINS_ADDR/jenkins\" 1 kubectl -n jenkins get pvc 1 kubectl get pv","title":"Demonstrate the persistent part"},{"location":"k8s/part3/#cleanup","text":"ALL 1 2 3 kubectl -n jenkins delete pvc jenkins kubectl get pv kubectl delete -f pv/pv.yml GKE 1 2 3 gcloud compute disks delete $VOLUME_ID_1 --zone $AZ_1 --quiet gcloud compute disks delete $VOLUME_ID_2 --zone $AZ_2 --quiet gcloud compute disks delete $VOLUME_ID_3 --zone $AZ_3 --quiet EKS 1 2 3 aws ec2 delete-volume --volume-id $VOLUME_ID_1 aws ec2 delete-volume --volume-id $VOLUME_ID_2 aws ec2 delete-volume --volume-id $VOLUME_ID_3","title":"Cleanup"},{"location":"k8s/part3/#storage-classes","text":"","title":"Storage Classes"},{"location":"k8s/part3/#view","text":"ALL - before others 1 kubectl get sc GKE 1 cat pv/jenkins-dynamic-gke.yml EKS 1 cat pv/jenkins-dynamic.yml","title":"View"},{"location":"k8s/part3/#create","text":"GKE 1 kubectl apply -f pv/jenkins-dynamic-gke.yml --record EKS 1 kubectl apply -f pv/jenkins-dynamic.yml --record ALL - after others 1 kubectl -n jenkins rollout status deployment jenkins","title":"Create"},{"location":"k8s/part3/#use","text":"ALL - before others 1 2 3 4 5 kubectl -n jenkins get events kubectl -n jenkins get pvc kubectl get pv GKE 1 2 PV_NAME = $( kubectl get pv -o jsonpath = {.items[0].metadata.name} ) gcloud compute disks list --filter = name:( $PV_NAME ) EKS 1 2 aws ec2 describe-volumes \\ --filters Name=tag-key,Values= kubernetes.io/created-for/pvc/name","title":"Use"},{"location":"k8s/part3/#use-2","text":"ALL - before others 1 2 3 kubectl -n jenkins delete deploy,pvc jenkins kubectl get pv GKE 1 gcloud compute disks list --filter = name:( $PV_NAME ) EKS 1 2 aws ec2 describe-volumes \\ --filters Name=tag-key,Values= kubernetes.io/created-for/pvc/name","title":"Use 2"},{"location":"k8s/part3/#use-default","text":"1 kubectl get sc 1 kubectl describe sc 1 cat pv/jenkins-default.yml 1 diff pv/jenkins-dynamic.yml pv/jenkins-default.yml 1 kubectl apply -f pv/jenkins-default.yml --record 1 kubectl get pv","title":"Use Default"},{"location":"k8s/part3/#prepare-alternative-sc","text":"All - before others 1 kubectl -n jenkins delete deploy,pvc jenkins GKE 1 YAML = sc-gke.yml EKS 1 YAML = sc.yml","title":"Prepare alternative SC"},{"location":"k8s/part3/#create-alternative-sc","text":"1 cat pv/ $YAML 1 kubectl create -f pv/ $YAML 1 kubectl get sc","title":"Create alternative SC"},{"location":"k8s/part3/#use-alternative-sc","text":"All - before others 1 2 cat pv/jenkins-sc.yml kubectl apply -f pv/jenkins-sc.yml --record EKS 1 2 aws ec2 describe-volumes \\ --filters Name=tag-key,Values= kubernetes.io/created-for/pvc/name GKE 1 2 PV_NAME = $( kubectl get pv -o jsonpath = {.items[0].metadata.name} ) gcloud compute disks list --filter = name:( $PV_NAME )","title":"Use alternative SC"},{"location":"k8s/part3/#cleanup_1","text":"1 kubectl delete ns jenkins 1 kubectl delete sc fast","title":"Cleanup"},{"location":"k8s/part3/#statefulset","text":"","title":"StatefulSet"},{"location":"k8s/part3/#create-statefulswet","text":"1 cat sts/jenkins.yml 1 kubectl apply -f sts/jenkins.yml --record 1 kubectl -n jenkins rollout status sts jenkins 1 kubectl -n jenkins get pvc 1 kubectl -n jenkins get pv","title":"Create StatefulSwet"},{"location":"k8s/part3/#get-jenkins-address","text":"GKE 1 2 JENKINS_ADDR = $( kubectl -n jenkins get ing jenkins \\ -o jsonpath = {.status.loadBalancer.ingress[0].ip} ) EKS 1 2 JENKINS_ADDR = $( kubectl -n jenkins get ing jenkins \\ -o jsonpath = {.status.loadBalancer.ingress[0].hostname} )","title":"Get Jenkins Address"},{"location":"k8s/part3/#use-jenkins","text":"1 open http:// $JENKINS_ADDR /jenkins 1 kubectl delete ns jenkins","title":"Use Jenkins"},{"location":"k8s/part3/#use-db-without-sts","text":"1 cat sts/go-demo-3-deploy.yml 1 kubectl apply -f sts/go-demo-3-deploy.yml --record 1 kubectl -n go-demo-3 rollout status deployment api 1 kubectl -n go-demo-3 get pods 1 2 DB_1 = $( kubectl -n go-demo-3 get pods -l app = db \\ -o jsonpath = {.items[0].metadata.name} ) 1 2 DB_2 = $( kubectl -n go-demo-3 get pods -l app = db \\ -o jsonpath = {.items[1].metadata.name} )","title":"Use DB without STS"},{"location":"k8s/part3/#investigate-problems","text":"1 kubectl -n go-demo-3 logs $DB_1 1 kubectl -n go-demo-3 logs $DB_2 1 kubectl get pv 1 kubectl delete ns go-demo-3","title":"Investigate problems"},{"location":"k8s/part3/#use-db-with-sts","text":"1 cat sts/go-demo-3-sts.yml 1 kubectl apply -f sts/go-demo-3-sts.yml --record 1 kubectl -n go-demo-3 get pods 1 kubectl get pv","title":"Use DB with STS"},{"location":"k8s/part3/#configure-mongo","text":"If we want MongoDB to use the three instances as single dataplane, we have to tell it the dataplane members. First, we shell into one of the db containers via exec . 1 kubectl -n go-demo-3 exec -it db-0 -- sh Then, make sure we're talking with MongoDB, via its REPL, with mongo . 1 mongo And finally we explain MongoDB what we want to do. 1 2 3 4 5 6 7 8 rs.initiate ( { _id : rs0 , members: [ { _id: 0 , host: db-0.db:27017 } , { _id: 1 , host: db-1.db:27017 } , { _id: 2 , host: db-2.db:27017 } ] }) Let's confirm with MongoDB before we exit the container. 1 rs.status () And now you're free to exit the MongoDB REPL and the container via ctrl+d (so twice). 1 kubectl -n go-demo-3 get pods","title":"Configure Mongo"},{"location":"k8s/part3/#observe-update-process","text":"1 diff sts/go-demo-3-sts.yml sts/go-demo-3-sts-upd.yml 1 kubectl apply -f sts/go-demo-3-sts-upd.yml --record 1 kubectl -n go-demo-3 get pods","title":"Observe update process"},{"location":"k8s/part3/#cleanup_2","text":"1 kubectl delete ns go-demo-3","title":"Cleanup"},{"location":"k8s/todo/","text":"Kubernetes basics Others The dev environment is set Never to receive promotions. New releases of our applications will not run there. The staging environment, on the other hand, is set to Auto promotion. What that means is that if a pipeline (defined in Jenkinsfile) has a command jx promote --all-auto, a new release will be deployed to all the environments with promotion set to Auto. Slide notes A few points I forgot, and that we should talk about: Gitops Cron-based Jenkins jobs: we have quite a few right now (I need to go over all our current jobs with you). Prefer event-based over scheduled. What if we have some jobs we really want to schedule? gcloud SDK (CLI). A few of us are used to it, but most people never used it. Because it\u2019s also our client for things like pubsub or GCS, it might be good to have a (small) part about google cloud, and how to use it. There is also the \u201cservice account\u201d part that I mentioned (in the \u201cpushing to GCR\u201d part), that is related. static envs vs dynamic envs. For the moment everybody is used to static envs. I think it\u2019s important to show the advantage of dynamic env. Could be a good intro to preview envs in the \u201cJenkins x\u201d part, it would be great if we can show an example of using updatebot. That\u2019s a great tool, and I love how the Jenkins x project is using it to automate the versions updates everywhere, and I\u2019d love to do the same thing. config maps labels annotations secrets containers are fun buuuuttttt, not enough to really solve problems in a better way crash course image vs container vs instance layers, from scratch regional vs. zonal rollback vs. rollforward namespaces purpose impact on DNS QoS: https://vfarcic.github.io/devops23/workshop-short.html#/29/24 Volumes volumes persistent volumes volume types persistent volume claims attaching persistent volume claims PersistendVolumeClaimTemplates Provisioners, such as NFS (incl. EFS and filestor support) storage Classes Declarative instead of Imperative Self-healing High-Availability (HA) Dynamic sizing Dynamic scaling Separate state from process 12-factor app/container Automatable Standard but Extensible Better utilization Serverless even better but more expensive for predictable load Dynamic Service Discovery Think Clusters of Cluster Static = fiction or dead Pets vs Cattle Self-Service On-Demand Curated instead of Fixed Layers of Abstraction to decouple create asynchronisity separate process state create fundamental building blocks but also provide predefined sets STS: teaches why STS type is needed cannot replicate database on the same data storage so create unique DB's via STS with PVC Templates see: https://vfarcic.github.io/devops23/workshop-short.html#/33/9 Ideas workshop gke does not give cluster-admin by default script for generating service account Docker build pre-requisite install docker (for windows/mac) dockerhub account cat a dockerfile single-stage multi-stage requires 17.5, not supported on GKE (yet) tag push Pod show imperative show declarative Slides Infrastructure As Code Declarative instead of Imperative Self-healing High-Availability (HA) Dynamic sizing Dynamic scaling Immutable Separate state from process 12-factor app/container Automatable Standard but Extensible Better utilization Serverless even better but more expensive for predictable load Dynamic Service Discovery Think Clusters of Cluster Static = fiction or dead Pets vs Cattle Self-Service On-Demand Curated instead of Fixed Layers of Abstraction to decouple create asynchronisity separate process state create fundamental building blocks but also provide predefined sets Building Docker Images Docker Docker Multi-stage BuildKit Docker Socket security issues Alternatives Kaniko Buildah IMG Others? Best Practices Smaller is usually better Optimise for short-running static links FROM scratch external state stay up-to-date with base images limit packages used use a (lightweight) process manager do not tie into a Runtime i.e. make the image suitable forn Swarm, Mesos, K8S","title":"Kubernetes basics"},{"location":"k8s/todo/#kubernetes-basics","text":"","title":"Kubernetes basics"},{"location":"k8s/todo/#others","text":"The dev environment is set Never to receive promotions. New releases of our applications will not run there. The staging environment, on the other hand, is set to Auto promotion. What that means is that if a pipeline (defined in Jenkinsfile) has a command jx promote --all-auto, a new release will be deployed to all the environments with promotion set to Auto.","title":"Others"},{"location":"k8s/todo/#slide-notes","text":"A few points I forgot, and that we should talk about: Gitops Cron-based Jenkins jobs: we have quite a few right now (I need to go over all our current jobs with you). Prefer event-based over scheduled. What if we have some jobs we really want to schedule? gcloud SDK (CLI). A few of us are used to it, but most people never used it. Because it\u2019s also our client for things like pubsub or GCS, it might be good to have a (small) part about google cloud, and how to use it. There is also the \u201cservice account\u201d part that I mentioned (in the \u201cpushing to GCR\u201d part), that is related. static envs vs dynamic envs. For the moment everybody is used to static envs. I think it\u2019s important to show the advantage of dynamic env. Could be a good intro to preview envs in the \u201cJenkins x\u201d part, it would be great if we can show an example of using updatebot. That\u2019s a great tool, and I love how the Jenkins x project is using it to automate the versions updates everywhere, and I\u2019d love to do the same thing. config maps labels annotations secrets containers are fun buuuuttttt, not enough to really solve problems in a better way crash course image vs container vs instance layers, from scratch regional vs. zonal rollback vs. rollforward namespaces purpose impact on DNS QoS: https://vfarcic.github.io/devops23/workshop-short.html#/29/24 Volumes volumes persistent volumes volume types persistent volume claims attaching persistent volume claims PersistendVolumeClaimTemplates Provisioners, such as NFS (incl. EFS and filestor support) storage Classes Declarative instead of Imperative Self-healing High-Availability (HA) Dynamic sizing Dynamic scaling Separate state from process 12-factor app/container Automatable Standard but Extensible Better utilization Serverless even better but more expensive for predictable load Dynamic Service Discovery Think Clusters of Cluster Static = fiction or dead Pets vs Cattle Self-Service On-Demand Curated instead of Fixed Layers of Abstraction to decouple create asynchronisity separate process state create fundamental building blocks but also provide predefined sets STS: teaches why STS type is needed cannot replicate database on the same data storage so create unique DB's via STS with PVC Templates see: https://vfarcic.github.io/devops23/workshop-short.html#/33/9","title":"Slide notes"},{"location":"k8s/todo/#ideas-workshop","text":"gke does not give cluster-admin by default script for generating service account","title":"Ideas workshop"},{"location":"k8s/todo/#docker-build","text":"pre-requisite install docker (for windows/mac) dockerhub account cat a dockerfile single-stage multi-stage requires 17.5, not supported on GKE (yet) tag push","title":"Docker build"},{"location":"k8s/todo/#pod","text":"show imperative show declarative","title":"Pod"},{"location":"k8s/todo/#slides","text":"Infrastructure As Code Declarative instead of Imperative Self-healing High-Availability (HA) Dynamic sizing Dynamic scaling Immutable Separate state from process 12-factor app/container Automatable Standard but Extensible Better utilization Serverless even better but more expensive for predictable load Dynamic Service Discovery Think Clusters of Cluster Static = fiction or dead Pets vs Cattle Self-Service On-Demand Curated instead of Fixed Layers of Abstraction to decouple create asynchronisity separate process state create fundamental building blocks but also provide predefined sets","title":"Slides"},{"location":"k8s/todo/#building-docker-images","text":"Docker Docker Multi-stage BuildKit Docker Socket security issues Alternatives Kaniko Buildah IMG Others? Best Practices Smaller is usually better Optimise for short-running static links FROM scratch external state stay up-to-date with base images limit packages used use a (lightweight) process manager do not tie into a Runtime i.e. make the image suitable forn Swarm, Mesos, K8S","title":"Building Docker Images"},{"location":"k8s/tools/","text":"Kubernetes Tools Kuard Kuard is a small demo application to show your cluster works. Also exposes some info you might want to see. 1 2 kubectl run --restart = Never --image = gcr.io/kuar-demo/kuard-amd64:blue kuard kubectl port-forward kuard 8080 :8080 Open your browser to http://localhost:8080 . Stern Stern allows you to tail multiple pods on Kubernetes and multiple containers within the pod. Each result is color coded for quicker debugging. 1 brew install stern Usage Imagine a build in Jenkins using more than one container in the Pod. You want to tail the logs of all containers... you can with stern. 1 stern maven- Kube Capacity Kube Capacity is a simple CLI that provides an overview of the resource requests, limits, and utilization in a Kubernetes cluster. 1 2 brew tap robscott/tap brew install robscott/tap/kube-capacity 1 kube-capacity 1 2 3 4 NODE CPU REQUESTS CPU LIMITS MEMORY REQUESTS MEMORY LIMITS * 560m ( 28 % ) 130m ( 7 % ) 572Mi ( 9 % ) 770Mi ( 13 % ) example-node-1 220m ( 22 % ) 10m ( 1 % ) 192Mi ( 6 % ) 360Mi ( 12 % ) example-node-2 340m ( 34 % ) 120m ( 12 % ) 380Mi ( 13 % ) 410Mi ( 14 % ) 1 kube-capacity --pods 1 2 3 4 5 6 7 8 9 10 NODE NAMESPACE POD CPU REQUESTS CPU LIMITS MEMORY REQUESTS MEMORY LIMITS * * * 560m ( 28 % ) 780m ( 38 % ) 572Mi ( 9 % ) 770Mi ( 13 % ) example-node-1 * * 220m ( 22 % ) 320m ( 32 % ) 192Mi ( 6 % ) 360Mi ( 12 % ) example-node-1 kube-system metrics-server-lwc6z 100m ( 10 % ) 200m ( 20 % ) 100Mi ( 3 % ) 200Mi ( 7 % ) example-node-1 kube-system coredns-7b5bcb98f8 120m ( 12 % ) 120m ( 12 % ) 92Mi ( 3 % ) 160Mi ( 5 % ) example-node-2 * * 340m ( 34 % ) 460m ( 46 % ) 380Mi ( 13 % ) 410Mi ( 14 % ) example-node-2 kube-system kube-proxy-3ki7 200m ( 20 % ) 280m ( 28 % ) 210Mi ( 7 % ) 210Mi ( 7 % ) example-node-2 tiller tiller-deploy 140m ( 14 % ) 180m ( 18 % ) 170Mi ( 5 % ) 200Mi ( 7 % ) 1 kube-capacity --util 1 2 3 4 NODE CPU REQUESTS CPU LIMITS CPU UTIL MEMORY REQUESTS MEMORY LIMITS MEMORY UTIL * 560m ( 28 % ) 130m ( 7 % ) 40m ( 2 % ) 572Mi ( 9 % ) 770Mi ( 13 % ) 470Mi ( 8 % ) example-node-1 220m ( 22 % ) 10m ( 1 % ) 10m ( 1 % ) 192Mi ( 6 % ) 360Mi ( 12 % ) 210Mi ( 7 % ) example-node-2 340m ( 34 % ) 120m ( 12 % ) 30m ( 3 % ) 380Mi ( 13 % ) 410Mi ( 14 % ) 260Mi ( 9 % ) 1 kube-capacity --pods --util Velero Velero RBAC Lookup RBAC Lookup Install bash 1 brew install reactiveops/tap/rbac-lookup Krew 1 kubectl krew install rbac-lookup Lookup user 1 rbac-lookup jvandergriendt -owide Lookup GKE user 1 rbac-lookup jvandergriendt --gke K9S K9S is a tool that gives you a console UI on your kubernetes cluster/namespace. Install 1 brew tap derailed/k9s brew install k9s Use By default is looks at a single namespace, and allows you to view elements of the pods running. 1 k9s -n cje Dive A tool for exploring a docker image, layer contents, and discovering ways to shrink your Docker image size. Dive is a tool for analyzing Docker images. Install Debian based 1 2 wget https://github.com/wagoodman/dive/releases/download/v0.7.1/dive_0.7.1_linux_amd64.deb sudo apt install ./dive_0.7.1_linux_amd64.deb RHEL based 1 2 curl -OL https://github.com/wagoodman/dive/releases/download/v0.7.1/dive_0.7.1_linux_amd64.rpm rpm -i dive_0.7.1_linux_amd64.rpm Homebrew 1 2 brew tap wagoodman/dive brew install dive Windows 1 go get github.com/wagoodman/dive Use Existing image 1 dive your-image-tag To be build image 1 dive build -t some-tag . For CI builds 1 CI = true dive your-image Kiali https://www.kiali.io/ Telepresence https://www.telepresence.io/","title":"Tools"},{"location":"k8s/tools/#kubernetes-tools","text":"","title":"Kubernetes Tools"},{"location":"k8s/tools/#kuard","text":"Kuard is a small demo application to show your cluster works. Also exposes some info you might want to see. 1 2 kubectl run --restart = Never --image = gcr.io/kuar-demo/kuard-amd64:blue kuard kubectl port-forward kuard 8080 :8080 Open your browser to http://localhost:8080 .","title":"Kuard"},{"location":"k8s/tools/#stern","text":"Stern allows you to tail multiple pods on Kubernetes and multiple containers within the pod. Each result is color coded for quicker debugging. 1 brew install stern","title":"Stern"},{"location":"k8s/tools/#usage","text":"Imagine a build in Jenkins using more than one container in the Pod. You want to tail the logs of all containers... you can with stern. 1 stern maven-","title":"Usage"},{"location":"k8s/tools/#kube-capacity","text":"Kube Capacity is a simple CLI that provides an overview of the resource requests, limits, and utilization in a Kubernetes cluster. 1 2 brew tap robscott/tap brew install robscott/tap/kube-capacity 1 kube-capacity 1 2 3 4 NODE CPU REQUESTS CPU LIMITS MEMORY REQUESTS MEMORY LIMITS * 560m ( 28 % ) 130m ( 7 % ) 572Mi ( 9 % ) 770Mi ( 13 % ) example-node-1 220m ( 22 % ) 10m ( 1 % ) 192Mi ( 6 % ) 360Mi ( 12 % ) example-node-2 340m ( 34 % ) 120m ( 12 % ) 380Mi ( 13 % ) 410Mi ( 14 % ) 1 kube-capacity --pods 1 2 3 4 5 6 7 8 9 10 NODE NAMESPACE POD CPU REQUESTS CPU LIMITS MEMORY REQUESTS MEMORY LIMITS * * * 560m ( 28 % ) 780m ( 38 % ) 572Mi ( 9 % ) 770Mi ( 13 % ) example-node-1 * * 220m ( 22 % ) 320m ( 32 % ) 192Mi ( 6 % ) 360Mi ( 12 % ) example-node-1 kube-system metrics-server-lwc6z 100m ( 10 % ) 200m ( 20 % ) 100Mi ( 3 % ) 200Mi ( 7 % ) example-node-1 kube-system coredns-7b5bcb98f8 120m ( 12 % ) 120m ( 12 % ) 92Mi ( 3 % ) 160Mi ( 5 % ) example-node-2 * * 340m ( 34 % ) 460m ( 46 % ) 380Mi ( 13 % ) 410Mi ( 14 % ) example-node-2 kube-system kube-proxy-3ki7 200m ( 20 % ) 280m ( 28 % ) 210Mi ( 7 % ) 210Mi ( 7 % ) example-node-2 tiller tiller-deploy 140m ( 14 % ) 180m ( 18 % ) 170Mi ( 5 % ) 200Mi ( 7 % ) 1 kube-capacity --util 1 2 3 4 NODE CPU REQUESTS CPU LIMITS CPU UTIL MEMORY REQUESTS MEMORY LIMITS MEMORY UTIL * 560m ( 28 % ) 130m ( 7 % ) 40m ( 2 % ) 572Mi ( 9 % ) 770Mi ( 13 % ) 470Mi ( 8 % ) example-node-1 220m ( 22 % ) 10m ( 1 % ) 10m ( 1 % ) 192Mi ( 6 % ) 360Mi ( 12 % ) 210Mi ( 7 % ) example-node-2 340m ( 34 % ) 120m ( 12 % ) 30m ( 3 % ) 380Mi ( 13 % ) 410Mi ( 14 % ) 260Mi ( 9 % ) 1 kube-capacity --pods --util","title":"Kube Capacity"},{"location":"k8s/tools/#velero","text":"Velero","title":"Velero"},{"location":"k8s/tools/#rbac-lookup","text":"RBAC Lookup","title":"RBAC Lookup"},{"location":"k8s/tools/#install","text":"bash 1 brew install reactiveops/tap/rbac-lookup Krew 1 kubectl krew install rbac-lookup","title":"Install"},{"location":"k8s/tools/#lookup-user","text":"1 rbac-lookup jvandergriendt -owide","title":"Lookup user"},{"location":"k8s/tools/#lookup-gke-user","text":"1 rbac-lookup jvandergriendt --gke","title":"Lookup GKE user"},{"location":"k8s/tools/#k9s","text":"K9S is a tool that gives you a console UI on your kubernetes cluster/namespace.","title":"K9S"},{"location":"k8s/tools/#install_1","text":"1 brew tap derailed/k9s brew install k9s","title":"Install"},{"location":"k8s/tools/#use","text":"By default is looks at a single namespace, and allows you to view elements of the pods running. 1 k9s -n cje","title":"Use"},{"location":"k8s/tools/#dive","text":"A tool for exploring a docker image, layer contents, and discovering ways to shrink your Docker image size. Dive is a tool for analyzing Docker images.","title":"Dive"},{"location":"k8s/tools/#install_2","text":"Debian based 1 2 wget https://github.com/wagoodman/dive/releases/download/v0.7.1/dive_0.7.1_linux_amd64.deb sudo apt install ./dive_0.7.1_linux_amd64.deb RHEL based 1 2 curl -OL https://github.com/wagoodman/dive/releases/download/v0.7.1/dive_0.7.1_linux_amd64.rpm rpm -i dive_0.7.1_linux_amd64.rpm Homebrew 1 2 brew tap wagoodman/dive brew install dive Windows 1 go get github.com/wagoodman/dive","title":"Install"},{"location":"k8s/tools/#use_1","text":"Existing image 1 dive your-image-tag To be build image 1 dive build -t some-tag . For CI builds 1 CI = true dive your-image","title":"Use"},{"location":"k8s/tools/#kiali","text":"https://www.kiali.io/","title":"Kiali"},{"location":"k8s/tools/#telepresence","text":"https://www.telepresence.io/","title":"Telepresence"},{"location":"other/alt-git-repo/","text":"Alternative Git Repository","title":"Alt Git Repo"},{"location":"other/alt-git-repo/#alternative-git-repository","text":"","title":"Alternative Git Repository"},{"location":"other/cluster-validation/","text":"Cluster Validation You can read this article from Viktor Farcic on how to confirm if your cluster is compliant with Jenkins X's requirements. Jenkins X has a binary, called jx , which includes some facilities from the Sonobuoy SDK to provide some validation capabilities. Run compliance check To run the compliance check, just use the jx command below. 1 jx compliance run Warning The compliance check will run for about one hour! Check Compliance run status 1 jx compliance status Check Compliance run logs 1 jx compliance logs -f See Compliance run results 1 jx compliance results Cleanup Once you're done with the compliance run, you can clean up any resources it created for the run. 1 jx compliance delete","title":"Cluster Validation"},{"location":"other/cluster-validation/#cluster-validation","text":"You can read this article from Viktor Farcic on how to confirm if your cluster is compliant with Jenkins X's requirements. Jenkins X has a binary, called jx , which includes some facilities from the Sonobuoy SDK to provide some validation capabilities.","title":"Cluster Validation"},{"location":"other/cluster-validation/#run-compliance-check","text":"To run the compliance check, just use the jx command below. 1 jx compliance run Warning The compliance check will run for about one hour!","title":"Run compliance check"},{"location":"other/cluster-validation/#check-compliance-run-status","text":"1 jx compliance status","title":"Check Compliance run status"},{"location":"other/cluster-validation/#check-compliance-run-logs","text":"1 jx compliance logs -f","title":"Check Compliance run logs"},{"location":"other/cluster-validation/#see-compliance-run-results","text":"1 jx compliance results","title":"See Compliance run results"},{"location":"other/cluster-validation/#cleanup","text":"Once you're done with the compliance run, you can clean up any resources it created for the run. 1 jx compliance delete","title":"Cleanup"},{"location":"other/custom-domain/","text":"Custom Domain","title":"Custom Domain"},{"location":"other/custom-domain/#custom-domain","text":"","title":"Custom Domain"},{"location":"other/issue-trackers/","text":"Issue Trackers Jenkins X can work with issue trackers such as GitHub issues and Jira. By default, Jenkins X will use GitHub for projects and issues. So if you haven't specified anything for either Git provider or Issue tracker, it will use GitHub for issues. GitHub 1 jx create issue -t lets make things more awesome 1 jx get issues Jira In order to configure Jenkins X to use Jira as issue tracker, you have to do three steps. create a tracker configuration jx create tracker server ${trackerName} https://mycompany.atlassian.net/ create a tracker login jx create tracker token -n ${trackerName} myEmailAddress configure your Jenkins X managed project to use this issue tracker instead jx edit config -k issues Info A file called jenkins-x.yml will be modified in your project source code which should be added to your git repository.","title":"Issue Trackers"},{"location":"other/issue-trackers/#issue-trackers","text":"Jenkins X can work with issue trackers such as GitHub issues and Jira. By default, Jenkins X will use GitHub for projects and issues. So if you haven't specified anything for either Git provider or Issue tracker, it will use GitHub for issues.","title":"Issue Trackers"},{"location":"other/issue-trackers/#github","text":"1 jx create issue -t lets make things more awesome 1 jx get issues","title":"GitHub"},{"location":"other/issue-trackers/#jira","text":"In order to configure Jenkins X to use Jira as issue tracker, you have to do three steps. create a tracker configuration jx create tracker server ${trackerName} https://mycompany.atlassian.net/ create a tracker login jx create tracker token -n ${trackerName} myEmailAddress configure your Jenkins X managed project to use this issue tracker instead jx edit config -k issues Info A file called jenkins-x.yml will be modified in your project source code which should be added to your git repository.","title":"Jira"},{"location":"other/security/","text":"Security features Secure Coding Jenkins X has direct support for some security analysis . Anchore image scanning The Anchore Engine is used to provide image security, by examining contents of containers either in pull request/review state, or on running containers. This was introduced in this blog post . Here is a video demonstrating it live. 1 jx create addon anchore To see if it found any problems in a specific environment: 1 jx get cve --environment = staging OWASP ZAP ZAP or Zed Attack Proxy allows you to scan the public surface of your application for any known vulnerability. 1 jx create addon owasp-zap","title":"Security"},{"location":"other/security/#security-features","text":"","title":"Security features"},{"location":"other/security/#secure-coding","text":"Jenkins X has direct support for some security analysis .","title":"Secure Coding"},{"location":"other/security/#anchore-image-scanning","text":"The Anchore Engine is used to provide image security, by examining contents of containers either in pull request/review state, or on running containers. This was introduced in this blog post . Here is a video demonstrating it live. 1 jx create addon anchore To see if it found any problems in a specific environment: 1 jx get cve --environment = staging","title":"Anchore image scanning"},{"location":"other/security/#owasp-zap","text":"ZAP or Zed Attack Proxy allows you to scan the public surface of your application for any known vulnerability. 1 jx create addon owasp-zap","title":"OWASP ZAP"},{"location":"preview/","text":"Preview Environments Two Charts We have two charts, one default chart and one for Preview environments only. Three Level Testing Static Validation Application Specific Tests System Validation Quote The first group of tests consists of those that do not rely on live applications. I'll call them static validation , and they can be unit tests, static analysis, or any other type that needs only code. Given that we do not need to install our application for those types of tests, we can run them as soon as we check out the code and before we even build our binaries. - Viktor Farcic Quote The second group is the application-specific tests. For those, we do need to deploy a new release first, but we do not need the whole system. Those tests tend to rely heavily on mocks and stubs. In some cases, that is not possible or practical, and we might need to deploy a few other applications to make the tests work. While I could argue that mocks should replace all \"real\" application dependencies in this phase, I am also aware that not all applications are designed to support that. - Viktor Farcic Quote The third group of tests is system-wide validations . We might want to check whether one live application integrates with other live applications. We might want to confirm that the performance of the system as a whole is within established thresholds. There can be many other things we might want to validate on the level of the whole system. What matters is that the tests in this phase are expensive. They tend to be slower than others, and they tend to need more resources. What we should not do while running system-wide validations is to repeat the checks we already did. We do not run the tests that already passed, and we try to keep those in this phase limited to what really matters (mostly integration and performance). - Viktor Farcic Making Changes Acceptable ways to make changes to source code. directly on Mainline ( trunk , master ), advocated by Trunk Based Development ( TBD ) via short lived feature branches, merging to Mainline quickly via an (semi-)automated process anything else that works for you - but don't expect me to support you in your endeavors this has been proven by Accelerate and State of DevOps Report to slow you down Automated Merging Process Jenkins X automates the merging process for short lived feature branches via PullRequests and ChatOps . It does so via the jx promote command. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 Promotes a version of an application to zero to many permanent environments. For more documentation see: https://jenkins-x.io/about/features/#promotion Examples: # Promote a version of the current application to staging # discovering the application name from the source code jx promote --version 1 .2.3 --env staging # Promote a version of the myapp application to production jx promote myapp --version 1 .2.3 --env production # To search for all the available charts for a given name use -f. # e.g. to find a redis chart to install jx promote -f redis # To promote a postgres chart using an alias jx promote -f postgres --alias mydb # To create or update a Preview Environment please see the jx preview command jx preview Options: --alias = : The optional alias used in the requirements.yaml file --all-auto = false: Promote to all automatic environments in order -a, --app = : The Application to promote --build = : The Build number which is used to update the PipelineActivity. If not specified its defaulted from the $BUILD_NUMBER environment variable -e, --env = : The Environment to promote to -f, --filter = : The search filter to find charts to promote -r, --helm-repo-name = releases : The name of the helm repository that contains the app -u, --helm-repo-url = : The Helm Repository URL to use for the App --ignore-local-file = false: Ignores the local file system when deducing the Git repository -n, --namespace = : The Namespace to promote to --no-helm-update = false: Allows the helm repo update command if you are sure your local helm cache is up to date with the version you wish to promote --no-merge = false: Disables automatic merge of promote Pull Requests --no-poll = false: Disables polling for Pull Request or Pipeline status --no-wait = false: Disables waiting for completing promotion after the Pull request is merged --pipeline = : The Pipeline string in the form folderName/repoName/branch which is used to update the PipelineActivity. If not specified its defaulted from the $BUILD_NUMBER environment variable --pull-request-poll-time = 20s : Poll time when waiting for a Pull Request to merge --release = : The name of the helm release -t, --timeout = 1h : The timeout to wait for the promotion to succeed in the underlying Environment. The command fails if the timeout is exceeded or the promotion does not complete -v, --version = : The Version to promote Usage: jx promote [ application ] [ flags ] [ options ] Use jx options for a list of global command-line options ( applies to all commands ) . Useful Commands 1 jx get previews 1 2 3 4 5 jx create pullrequest \\ --title My PR \\ --body This is the text that describes the PR and it can span multiple lines \\ --batch-mode 1 jx get issues -b Garbage Collection 1 kubectl get cronjobs 1 jx gc previews References https://medium.com/@MichalFoksa/jenkins-x-preview-environment-3bf2424a05e4 https://jenkins-x.io/docs/concepts/jenkins-x-pipelines/#customizing-the-pipelines https://jenkins-x.io/faq/develop/#how-do-i-add-other-services-into-a-preview https://jenkins-x.io/commands/jx_step_create_pullrequest_chart/ https://github.com/jasonwc-jenkinsx-example/yo-frontend/pull/3/files https://jenkins-x.io/about/features/#promotion","title":"Introduction"},{"location":"preview/#preview-environments","text":"","title":"Preview Environments"},{"location":"preview/#two-charts","text":"We have two charts, one default chart and one for Preview environments only.","title":"Two Charts"},{"location":"preview/#three-level-testing","text":"Static Validation Application Specific Tests System Validation Quote The first group of tests consists of those that do not rely on live applications. I'll call them static validation , and they can be unit tests, static analysis, or any other type that needs only code. Given that we do not need to install our application for those types of tests, we can run them as soon as we check out the code and before we even build our binaries. - Viktor Farcic Quote The second group is the application-specific tests. For those, we do need to deploy a new release first, but we do not need the whole system. Those tests tend to rely heavily on mocks and stubs. In some cases, that is not possible or practical, and we might need to deploy a few other applications to make the tests work. While I could argue that mocks should replace all \"real\" application dependencies in this phase, I am also aware that not all applications are designed to support that. - Viktor Farcic Quote The third group of tests is system-wide validations . We might want to check whether one live application integrates with other live applications. We might want to confirm that the performance of the system as a whole is within established thresholds. There can be many other things we might want to validate on the level of the whole system. What matters is that the tests in this phase are expensive. They tend to be slower than others, and they tend to need more resources. What we should not do while running system-wide validations is to repeat the checks we already did. We do not run the tests that already passed, and we try to keep those in this phase limited to what really matters (mostly integration and performance). - Viktor Farcic","title":"Three Level Testing"},{"location":"preview/#making-changes","text":"Acceptable ways to make changes to source code. directly on Mainline ( trunk , master ), advocated by Trunk Based Development ( TBD ) via short lived feature branches, merging to Mainline quickly via an (semi-)automated process anything else that works for you - but don't expect me to support you in your endeavors this has been proven by Accelerate and State of DevOps Report to slow you down","title":"Making Changes"},{"location":"preview/#automated-merging-process","text":"Jenkins X automates the merging process for short lived feature branches via PullRequests and ChatOps . It does so via the jx promote command. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 Promotes a version of an application to zero to many permanent environments. For more documentation see: https://jenkins-x.io/about/features/#promotion Examples: # Promote a version of the current application to staging # discovering the application name from the source code jx promote --version 1 .2.3 --env staging # Promote a version of the myapp application to production jx promote myapp --version 1 .2.3 --env production # To search for all the available charts for a given name use -f. # e.g. to find a redis chart to install jx promote -f redis # To promote a postgres chart using an alias jx promote -f postgres --alias mydb # To create or update a Preview Environment please see the jx preview command jx preview Options: --alias = : The optional alias used in the requirements.yaml file --all-auto = false: Promote to all automatic environments in order -a, --app = : The Application to promote --build = : The Build number which is used to update the PipelineActivity. If not specified its defaulted from the $BUILD_NUMBER environment variable -e, --env = : The Environment to promote to -f, --filter = : The search filter to find charts to promote -r, --helm-repo-name = releases : The name of the helm repository that contains the app -u, --helm-repo-url = : The Helm Repository URL to use for the App --ignore-local-file = false: Ignores the local file system when deducing the Git repository -n, --namespace = : The Namespace to promote to --no-helm-update = false: Allows the helm repo update command if you are sure your local helm cache is up to date with the version you wish to promote --no-merge = false: Disables automatic merge of promote Pull Requests --no-poll = false: Disables polling for Pull Request or Pipeline status --no-wait = false: Disables waiting for completing promotion after the Pull request is merged --pipeline = : The Pipeline string in the form folderName/repoName/branch which is used to update the PipelineActivity. If not specified its defaulted from the $BUILD_NUMBER environment variable --pull-request-poll-time = 20s : Poll time when waiting for a Pull Request to merge --release = : The name of the helm release -t, --timeout = 1h : The timeout to wait for the promotion to succeed in the underlying Environment. The command fails if the timeout is exceeded or the promotion does not complete -v, --version = : The Version to promote Usage: jx promote [ application ] [ flags ] [ options ] Use jx options for a list of global command-line options ( applies to all commands ) .","title":"Automated Merging Process"},{"location":"preview/#useful-commands","text":"1 jx get previews 1 2 3 4 5 jx create pullrequest \\ --title My PR \\ --body This is the text that describes the PR and it can span multiple lines \\ --batch-mode 1 jx get issues -b","title":"Useful Commands"},{"location":"preview/#garbage-collection","text":"1 kubectl get cronjobs 1 jx gc previews","title":"Garbage Collection"},{"location":"preview/#references","text":"https://medium.com/@MichalFoksa/jenkins-x-preview-environment-3bf2424a05e4 https://jenkins-x.io/docs/concepts/jenkins-x-pipelines/#customizing-the-pipelines https://jenkins-x.io/faq/develop/#how-do-i-add-other-services-into-a-preview https://jenkins-x.io/commands/jx_step_create_pullrequest_chart/ https://github.com/jasonwc-jenkinsx-example/yo-frontend/pull/3/files https://jenkins-x.io/about/features/#promotion","title":"References"},{"location":"preview/database/","text":"Preview Env Database","title":"Database"},{"location":"preview/database/#preview-env-database","text":"","title":"Preview Env Database"},{"location":"preview/dependencies/","text":"Preview Env Dependencies Remarks agreed. one cheat to minimise having the front end preview to know the latest backend version and vice versa is for the previews of the front + back ends to use latest image verisons; we generally recommend always using real versions - but for these kinds of front+back end previews it can be handy (then never using latest in real releases post a merge) Direct Service Link Add a Kubernetes Service resource in charts/preview/templates/ . The application launched in the preview environment can then call the dependency \"locally\", where the service has a reference to an instance running elsewhere. 1 Example 1 2 3 4 5 6 7 8 9 10 kind : Service apiVersion : v1 metadata : name : mysql spec : type : ExternalName # Target service DNS name externalName : mysql.jx-staging.svc.cluster.local ports : - port : 3306 Dependency Instance You can add dependencies in the charts/preview/requirements.yaml as for any other chart. Due to how Jenkins X creates dependencies, you will have to change something somewhere. Either your values.yaml needs to change how the direct dependency is called - to avoid the preview prefix - or you have to change your application's configuration to be able to point to the de Preview instance of your dependency. MongoDB Install Directly 1 2 helm repo add bitnami https://charts.bitnami.com helm install bitnami/mongodb --version 7.4.5 Via Jenkins X 1 2 3 4 # requirements.yaml - name : mongodb repository : https://charts.bitnami.com version : 7.4.5 1 2 3 4 5 # values.yaml mongodb : mongodbUsername : someusername mongodbPassword : somepassword mongodbDatabase : somedatabase References Jenkins X Preview Environment - Michal Foksa","title":"Dependencies"},{"location":"preview/dependencies/#preview-env-dependencies","text":"","title":"Preview Env Dependencies"},{"location":"preview/dependencies/#remarks","text":"agreed. one cheat to minimise having the front end preview to know the latest backend version and vice versa is for the previews of the front + back ends to use latest image verisons; we generally recommend always using real versions - but for these kinds of front+back end previews it can be handy (then never using latest in real releases post a merge)","title":"Remarks"},{"location":"preview/dependencies/#direct-service-link","text":"Add a Kubernetes Service resource in charts/preview/templates/ . The application launched in the preview environment can then call the dependency \"locally\", where the service has a reference to an instance running elsewhere. 1 Example 1 2 3 4 5 6 7 8 9 10 kind : Service apiVersion : v1 metadata : name : mysql spec : type : ExternalName # Target service DNS name externalName : mysql.jx-staging.svc.cluster.local ports : - port : 3306","title":"Direct Service Link"},{"location":"preview/dependencies/#dependency-instance","text":"You can add dependencies in the charts/preview/requirements.yaml as for any other chart. Due to how Jenkins X creates dependencies, you will have to change something somewhere. Either your values.yaml needs to change how the direct dependency is called - to avoid the preview prefix - or you have to change your application's configuration to be able to point to the de Preview instance of your dependency.","title":"Dependency Instance"},{"location":"preview/dependencies/#mongodb","text":"","title":"MongoDB"},{"location":"preview/dependencies/#install-directly","text":"1 2 helm repo add bitnami https://charts.bitnami.com helm install bitnami/mongodb --version 7.4.5","title":"Install Directly"},{"location":"preview/dependencies/#via-jenkins-x","text":"1 2 3 4 # requirements.yaml - name : mongodb repository : https://charts.bitnami.com version : 7.4.5 1 2 3 4 5 # values.yaml mongodb : mongodbUsername : someusername mongodbPassword : somepassword mongodbDatabase : somedatabase","title":"Via Jenkins X"},{"location":"preview/dependencies/#references","text":"Jenkins X Preview Environment - Michal Foksa","title":"References"},{"location":"preview/pipeline/","text":"Jenkins X Pipeline Create Step 1 jx create step --help 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 Creates a step in the Jenkins X Pipeline Aliases: step, steps Examples: # Create a new step in the Jenkins X Pipeline interactively jx create step # Creates a step on the command line: adding a post step to the release build lifecycle jx create step -sh echo hello world # Creates a step on the command line: adding a pre step to the pullRequest promote lifecycle jx create step -p pullrequest -l promote -m pre -c echo before promote Options: -d, --dir = : The root project directory. Defaults to the current dir -l, --lifecycle = : The lifecycle stage to add your step. Possible values: setup, setversion, prebuild, build, postbuild, promote -m, --mode = : The create mode for the new step. Possible values: pre, post, replace -p, --pipeline = : The pipeline kind to add your step. Possible values: release, pullrequest, feature -c, --sh = : The command to invoke for the new step Usage: jx create step [ flags ] [ options ] Create Step For PRs Simple Example 1 2 3 4 5 jx create step \\ --pipeline pullrequest \\ --lifecycle promote \\ --mode post \\ --sh ls -lath Multiline Wait 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 pipelineConfig : pipelines : pullRequest : build : preSteps : # This was modified - name : unit-tests command : make unittest promote : steps : # This is new - name : rollout command : | NS=\\`echo cd-\\$REPO_OWNER-go-demo-6-\\$BRANCH_NAME | tr [:upper:] [:lower:] \\` sleep 15 kubectl -n \\$NS rollout status deployment preview-preview --timeout 3m # This was modified - name : functional-tests command : ADDRESS=\\`jx get preview --current 2 1\\` make functest Validate Current Pipeline Configuration 1 jx step syntax validate pipeline Add SonarQube Scan 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 pipelineConfig : pipelines : overrides : - name : mvn-deploy pipeline : release stage : build step : name : sonar command : sonar-scanner image : fabiopotame/sonar-scanner-cli # newtmitch/sonar-scanner for JDK 10+? dir : /workspace/source/ args : - -Dsonar.projectName=jx-qs-spring-boot - -Dsonar.projectKey=jx-qs-spring-boot - -Dsonar.organization=joostvdg-github - -Dsonar.sources=./src/main/java/ - -Dsonar.language=java - -Dsonar.java.binaries=./target/classes - -Dsonar.host.url=https://sonarcloud.io - -Dsonar.login=bebe633ad6599cbf52f7e0b9ee1bc2bbd3cd9c80 type : after Pipeline Schema 1 jx step syntax schema Build Pack Schema 1 jx step syntax schema --buildpack 1 jx step syntax validate buildpacks","title":"Pipeline"},{"location":"preview/pipeline/#jenkins-x-pipeline","text":"","title":"Jenkins X Pipeline"},{"location":"preview/pipeline/#create-step","text":"1 jx create step --help 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 Creates a step in the Jenkins X Pipeline Aliases: step, steps Examples: # Create a new step in the Jenkins X Pipeline interactively jx create step # Creates a step on the command line: adding a post step to the release build lifecycle jx create step -sh echo hello world # Creates a step on the command line: adding a pre step to the pullRequest promote lifecycle jx create step -p pullrequest -l promote -m pre -c echo before promote Options: -d, --dir = : The root project directory. Defaults to the current dir -l, --lifecycle = : The lifecycle stage to add your step. Possible values: setup, setversion, prebuild, build, postbuild, promote -m, --mode = : The create mode for the new step. Possible values: pre, post, replace -p, --pipeline = : The pipeline kind to add your step. Possible values: release, pullrequest, feature -c, --sh = : The command to invoke for the new step Usage: jx create step [ flags ] [ options ]","title":"Create Step"},{"location":"preview/pipeline/#create-step-for-prs","text":"","title":"Create Step For PRs"},{"location":"preview/pipeline/#simple-example","text":"1 2 3 4 5 jx create step \\ --pipeline pullrequest \\ --lifecycle promote \\ --mode post \\ --sh ls -lath","title":"Simple Example"},{"location":"preview/pipeline/#multiline-wait","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 pipelineConfig : pipelines : pullRequest : build : preSteps : # This was modified - name : unit-tests command : make unittest promote : steps : # This is new - name : rollout command : | NS=\\`echo cd-\\$REPO_OWNER-go-demo-6-\\$BRANCH_NAME | tr [:upper:] [:lower:] \\` sleep 15 kubectl -n \\$NS rollout status deployment preview-preview --timeout 3m # This was modified - name : functional-tests command : ADDRESS=\\`jx get preview --current 2 1\\` make functest","title":"Multiline &amp; Wait"},{"location":"preview/pipeline/#validate-current-pipeline-configuration","text":"1 jx step syntax validate pipeline","title":"Validate Current Pipeline Configuration"},{"location":"preview/pipeline/#add-sonarqube-scan","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 pipelineConfig : pipelines : overrides : - name : mvn-deploy pipeline : release stage : build step : name : sonar command : sonar-scanner image : fabiopotame/sonar-scanner-cli # newtmitch/sonar-scanner for JDK 10+? dir : /workspace/source/ args : - -Dsonar.projectName=jx-qs-spring-boot - -Dsonar.projectKey=jx-qs-spring-boot - -Dsonar.organization=joostvdg-github - -Dsonar.sources=./src/main/java/ - -Dsonar.language=java - -Dsonar.java.binaries=./target/classes - -Dsonar.host.url=https://sonarcloud.io - -Dsonar.login=bebe633ad6599cbf52f7e0b9ee1bc2bbd3cd9c80 type : after","title":"Add SonarQube Scan"},{"location":"preview/pipeline/#pipeline-schema","text":"1 jx step syntax schema","title":"Pipeline Schema"},{"location":"preview/pipeline/#build-pack-schema","text":"1 jx step syntax schema --buildpack 1 jx step syntax validate buildpacks","title":"Build Pack Schema"},{"location":"serverless/","text":"Serverless JX","title":"Intro"},{"location":"serverless/#serverless-jx","text":"","title":"Serverless JX"},{"location":"tips/","text":"Tips Tricks https://github.com/jenkinsci/kubernetes-credentials-provider-plugin volume storage with Heptio's Valerio debugging: https://jenkins-x.io/contribute/development/#debugging --gitops mode have to check this jx upgrade platform knative authentication --git-username --org are complimentary, --organisations is unrelated username = the user for the repo's (apps env) org = the organization for the repo's (apps env) e.g.: --git-username joostvdg --org demomon --organisations is used to query GitHub for Quickstarts (no need to specify unless you have alternatives) Jenkins configurations do not persist you can specify them in a ConfigMap though https://github.com/jenkins-x/charts/blob/jenkins/stable/jenkins/templates/config.yaml#L8 credentials used for config can be found here: ~/.jx/jenkinsAuth.yam PodTemplates (static Jenkins) are from the Jenkins Kubernetes Plugin cleanup GKE jx gc gke you can create separate teams with --no-tiller even if the installation was done with Tiller jx init to \"fix\" a outdated ~/.jx folder https://jenkins-x.io/commands/jx_step_credential/ don't do mono repo's why? - https://medium.com/@mattklein123/monorepos-please-dont-e9a279be011b but if you do, https://fuchsia.googlesource.com/jiri/ using a different git provider --git-provider-url .... --git-provider-kind bitbucketserver --git-username foo --git-api-token whatever https://jenkins-x.io/developing/git/#using-a-different-git-provider-for-environments jx start pipeline to manually trigger a pipeline (I assume static Jenkins only) enable GCS for chartmuseum backend https://github.com/jenkins-x/cloud-environments/blob/master/env-jx-infra/myvalues.yaml#L10-L17 jx wraps kubectx tool, so you can use jx ns namespace to change your context to a different namespace faq for diagnosing exposecontroller issues: https://jenkins-x.io/faq/issues/#how-can-i-diagnose-exposecontroller-issues controller is used to generate ingress resources https://github.com/lvlstudio/jenkins-x-builders/tree/master/builder-nodejs-mysql https://github.com/jenkins-x/jx/issues/2550 https://github.com/jenkins-x/jenkins-x-platform/issues/4768 Tillerless: https://jenkins-x.io/news/helm-without-tiller/ difference between jx create quickstart and selecting spring vs. jx create spring jx create spring is an interactive wizard that uses the spring initialiser https://start.spring.io/ jx create quickstart uses a configurable github org to list available existing quickstarts i.e. https://github.com/jenkins-x-quickstarts (edited) multi-cluster support: https://github.com/jenkins-x-charts/environment-controller https://github.com/jenkins-x/jx/issues/479 jx create user ? https://jenkins-x.io/commands/jx_create_jenkins_token/ for problems with wild card certificates doing only one segment (i.e., *.example.com instead of *.*.example.com ) no - we can tweak that. It\u2019s easiest with wildcard - then any exposed service at svc.ns.domain just works - but you could register each namespace in DNS we\u2019ve not exposed that property to the jx install CLI yet - but you could try kubectl edit cm ingress-config urltemplate: \"{{.Service}}-{{.Namespace}}.{{.Domain}}\" and then jx upgrade ingress doesn't seem to work yet? (customization gets reverted) alternative, add dns entries to the ingress resources https://jenkins-x.io/getting-started/install-on-cluster/#installing-jenkins-x-on-premise Helm tips tricks for changing secrets https://github.com/helm/helm/blob/master/docs/charts_tips_and_tricks.md#user-content-automatically-roll-deployments-when-configmaps-or-secrets-change how to run integration tests I've answered this in a previous thread. Basically you use the helm chart (which includes the service dependencies as requirements). You create a preview but set the replicaCount for your service to 0 (that way, just the requirements are started). Then just run your tests against the requirements and delete the preview afterwards. Search the channel history for my messages and replicaCount. You should find it. https://jenkins-x.io/faq/develop/#how-do-i-add-other-services-into-a-preview managing static jenkins config has some issues https://github.com/jenkins-x/jx/issues/2991 https://github.com/jenkins-x/jx-docs/issues/1039 https://github.com/helm/charts/pull/9296/files we\u2019re super close from recommending folks use jx create cluster ... --vault --gitops which uses a git repository to store all the configuration changes + versions of stuff - and uses Vault to store all secrets we just merged the last few fixes so it should work for static jenkins servers with the latest jx binary - feb 7 Info It is Ready: jx create cluster gke --vault --gitops --no-tiller Multiple Micro-services if 1 microservice was 1 helm chart with a few different containers for example; you may have a few repos that just make binaries/docker images - then 1 repo which contains the helm chart of the microservice - you can also easily combine microservices together into an uber helm chart - James Strachan we prefer to use multiple repositories so that things are more microservice based. The problem with monorepos is everything gets released on every change; with separate repos its easier to manage change etc to handle changing versions of things across repositories we use updatebot ourselves to do \u2018CI/CD of dependencies\u2019 - we kinda think of it as promotion of dependencies like we promote microservices into environments - its PRs generated as part of the release process the main decision to make really is, , if you have, say, 3 microservices that are fairly tightly coupled; do you combine 3 versions of them all into 1 chart and then release that 1 chart when its all tested together; or do you release the 3 things totally independently - it depeends on coupling and team structure really you can switch from one to the other at any time really https://github.com/jenkins-x/updatebot 1 2 3 4 5 6 7 8 9 10 11 12 # Setup: $ jx create cluster gke -n team1 --default-environment-prefix team1 $ jx create quickstart -p app1 $ jx create quickstart -p app2 $ jx create quickstart -p app3 # The 5 repos: environment-team1-staging environment-team1-production app1 app2 app3","title":"Tips"},{"location":"tips/#tips-tricks","text":"https://github.com/jenkinsci/kubernetes-credentials-provider-plugin volume storage with Heptio's Valerio debugging: https://jenkins-x.io/contribute/development/#debugging --gitops mode have to check this jx upgrade platform knative authentication --git-username --org are complimentary, --organisations is unrelated username = the user for the repo's (apps env) org = the organization for the repo's (apps env) e.g.: --git-username joostvdg --org demomon --organisations is used to query GitHub for Quickstarts (no need to specify unless you have alternatives) Jenkins configurations do not persist you can specify them in a ConfigMap though https://github.com/jenkins-x/charts/blob/jenkins/stable/jenkins/templates/config.yaml#L8 credentials used for config can be found here: ~/.jx/jenkinsAuth.yam PodTemplates (static Jenkins) are from the Jenkins Kubernetes Plugin cleanup GKE jx gc gke you can create separate teams with --no-tiller even if the installation was done with Tiller jx init to \"fix\" a outdated ~/.jx folder https://jenkins-x.io/commands/jx_step_credential/ don't do mono repo's why? - https://medium.com/@mattklein123/monorepos-please-dont-e9a279be011b but if you do, https://fuchsia.googlesource.com/jiri/ using a different git provider --git-provider-url .... --git-provider-kind bitbucketserver --git-username foo --git-api-token whatever https://jenkins-x.io/developing/git/#using-a-different-git-provider-for-environments jx start pipeline to manually trigger a pipeline (I assume static Jenkins only) enable GCS for chartmuseum backend https://github.com/jenkins-x/cloud-environments/blob/master/env-jx-infra/myvalues.yaml#L10-L17 jx wraps kubectx tool, so you can use jx ns namespace to change your context to a different namespace faq for diagnosing exposecontroller issues: https://jenkins-x.io/faq/issues/#how-can-i-diagnose-exposecontroller-issues controller is used to generate ingress resources https://github.com/lvlstudio/jenkins-x-builders/tree/master/builder-nodejs-mysql https://github.com/jenkins-x/jx/issues/2550 https://github.com/jenkins-x/jenkins-x-platform/issues/4768 Tillerless: https://jenkins-x.io/news/helm-without-tiller/ difference between jx create quickstart and selecting spring vs. jx create spring jx create spring is an interactive wizard that uses the spring initialiser https://start.spring.io/ jx create quickstart uses a configurable github org to list available existing quickstarts i.e. https://github.com/jenkins-x-quickstarts (edited) multi-cluster support: https://github.com/jenkins-x-charts/environment-controller https://github.com/jenkins-x/jx/issues/479 jx create user ? https://jenkins-x.io/commands/jx_create_jenkins_token/ for problems with wild card certificates doing only one segment (i.e., *.example.com instead of *.*.example.com ) no - we can tweak that. It\u2019s easiest with wildcard - then any exposed service at svc.ns.domain just works - but you could register each namespace in DNS we\u2019ve not exposed that property to the jx install CLI yet - but you could try kubectl edit cm ingress-config urltemplate: \"{{.Service}}-{{.Namespace}}.{{.Domain}}\" and then jx upgrade ingress doesn't seem to work yet? (customization gets reverted) alternative, add dns entries to the ingress resources https://jenkins-x.io/getting-started/install-on-cluster/#installing-jenkins-x-on-premise Helm tips tricks for changing secrets https://github.com/helm/helm/blob/master/docs/charts_tips_and_tricks.md#user-content-automatically-roll-deployments-when-configmaps-or-secrets-change how to run integration tests I've answered this in a previous thread. Basically you use the helm chart (which includes the service dependencies as requirements). You create a preview but set the replicaCount for your service to 0 (that way, just the requirements are started). Then just run your tests against the requirements and delete the preview afterwards. Search the channel history for my messages and replicaCount. You should find it. https://jenkins-x.io/faq/develop/#how-do-i-add-other-services-into-a-preview managing static jenkins config has some issues https://github.com/jenkins-x/jx/issues/2991 https://github.com/jenkins-x/jx-docs/issues/1039 https://github.com/helm/charts/pull/9296/files we\u2019re super close from recommending folks use jx create cluster ... --vault --gitops which uses a git repository to store all the configuration changes + versions of stuff - and uses Vault to store all secrets we just merged the last few fixes so it should work for static jenkins servers with the latest jx binary - feb 7 Info It is Ready: jx create cluster gke --vault --gitops --no-tiller","title":"Tips &amp; Tricks"},{"location":"tips/#multiple-micro-services","text":"if 1 microservice was 1 helm chart with a few different containers for example; you may have a few repos that just make binaries/docker images - then 1 repo which contains the helm chart of the microservice - you can also easily combine microservices together into an uber helm chart - James Strachan we prefer to use multiple repositories so that things are more microservice based. The problem with monorepos is everything gets released on every change; with separate repos its easier to manage change etc to handle changing versions of things across repositories we use updatebot ourselves to do \u2018CI/CD of dependencies\u2019 - we kinda think of it as promotion of dependencies like we promote microservices into environments - its PRs generated as part of the release process the main decision to make really is, , if you have, say, 3 microservices that are fairly tightly coupled; do you combine 3 versions of them all into 1 chart and then release that 1 chart when its all tested together; or do you release the 3 things totally independently - it depeends on coupling and team structure really you can switch from one to the other at any time really https://github.com/jenkins-x/updatebot 1 2 3 4 5 6 7 8 9 10 11 12 # Setup: $ jx create cluster gke -n team1 --default-environment-prefix team1 $ jx create quickstart -p app1 $ jx create quickstart -p app2 $ jx create quickstart -p app3 # The 5 repos: environment-team1-staging environment-team1-production app1 app2 app3","title":"Multiple Micro-services"},{"location":"tips/commands/","text":"Useful Commands JX Shell Create a sub shell so that changes to the Kubernetes context, namespace or environment remain local to the shell 1 2 # create a new shell using a specific named context jx shell prod-cluster JX Prompt Generate the command line prompt for the current team and environment current 1 2 # Generate the current prompt jx prompt bash 1 2 # Enable the prompt for bash PS1 = [\\u@\\h \\W \\$(jx prompt)]\\$ zsh 1 2 # Enable the prompt for zsh PROMPT = $(jx prompt) $PROMPT JX Helm As of this writing, April 2019, Tiller is no longer installed by default. This means Helm cannot find it's releases. To interact with Helm as you're used to, use jx step helm instead. 1 jx step helm list For more details, read the docs . Single Issue commands Get URLS 1 jx get urls","title":"Command Overview"},{"location":"tips/commands/#useful-commands","text":"","title":"Useful Commands"},{"location":"tips/commands/#jx-shell","text":"Create a sub shell so that changes to the Kubernetes context, namespace or environment remain local to the shell 1 2 # create a new shell using a specific named context jx shell prod-cluster","title":"JX Shell"},{"location":"tips/commands/#jx-prompt","text":"Generate the command line prompt for the current team and environment current 1 2 # Generate the current prompt jx prompt bash 1 2 # Enable the prompt for bash PS1 = [\\u@\\h \\W \\$(jx prompt)]\\$ zsh 1 2 # Enable the prompt for zsh PROMPT = $(jx prompt) $PROMPT","title":"JX Prompt"},{"location":"tips/commands/#jx-helm","text":"As of this writing, April 2019, Tiller is no longer installed by default. This means Helm cannot find it's releases. To interact with Helm as you're used to, use jx step helm instead. 1 jx step helm list For more details, read the docs .","title":"JX Helm"},{"location":"tips/commands/#single-issue-commands","text":"","title":"Single Issue commands"},{"location":"tips/commands/#get-urls","text":"1 jx get urls","title":"Get URLS"},{"location":"tips/gitops/","text":"Jenkins X - GitOps flag","title":"GitOps"},{"location":"tips/gitops/#jenkins-x-gitops-flag","text":"","title":"Jenkins X  - GitOps flag"},{"location":"tips/vault/","text":"Vault addon","title":"Vault"},{"location":"tips/vault/#vault-addon","text":"","title":"Vault addon"},{"location":"xfiles/","text":"The X-Files Demo The X-Files is a workshop created for Jenkins X by Vincent Behar . There's three parts: Mulder : backend Scully : frontend Integration : ??? If you want to cheat , the workshop with all direction can be found here . For a good practice, follow the instructions on this site, as they force you to solve the puzzles yourself.","title":"Overview"},{"location":"xfiles/#the-x-files-demo","text":"The X-Files is a workshop created for Jenkins X by Vincent Behar . There's three parts: Mulder : backend Scully : frontend Integration : ??? If you want to cheat , the workshop with all direction can be found here . For a good practice, follow the instructions on this site, as they force you to solve the puzzles yourself.","title":"The X-Files Demo"},{"location":"xfiles/integration/","text":"XFiles - Integration","title":"Integration"},{"location":"xfiles/integration/#xfiles-integration","text":"","title":"XFiles - Integration"},{"location":"xfiles/mulder/","text":"Mulder Steps to follow fork clone import into jx fix redis config fix health check endpoint add unit tests to pipeline add integration tests to pipeline test PR test staging production Fork Clone Go to github.com/the-jenkins-x-files/mulder and fork the repository to your own account. Then, go to the directory you want the source code to be. 1 GH_USER = ? 1 2 git clone https://github.com/ ${ GH_USER } /mulder cd mulder JX Import 1 jx import Hints Application parameters For the parameters the application takes for configuration, take a look at the README.md . https://github.com/the-jenkins-x-files/mulder","title":"Mulder"},{"location":"xfiles/mulder/#mulder","text":"","title":"Mulder"},{"location":"xfiles/mulder/#steps-to-follow","text":"fork clone import into jx fix redis config fix health check endpoint add unit tests to pipeline add integration tests to pipeline test PR test staging production","title":"Steps to follow"},{"location":"xfiles/mulder/#fork-clone","text":"Go to github.com/the-jenkins-x-files/mulder and fork the repository to your own account. Then, go to the directory you want the source code to be. 1 GH_USER = ? 1 2 git clone https://github.com/ ${ GH_USER } /mulder cd mulder","title":"Fork &amp; Clone"},{"location":"xfiles/mulder/#jx-import","text":"1 jx import","title":"JX Import"},{"location":"xfiles/mulder/#hints","text":"","title":"Hints"},{"location":"xfiles/mulder/#application-parameters","text":"For the parameters the application takes for configuration, take a look at the README.md . https://github.com/the-jenkins-x-files/mulder","title":"Application parameters"},{"location":"xfiles/scully/","text":"Scully Steps fork clone jx import fix Dockerfile add Mulder as dependency helm repo (see hints below if you doubt) fix url to Mulder Fork Clone Go to github.com/the-jenkins-x-files/scully and fork the repository to your own account. Then, go to the directory you want the source code to be. 1 GH_USER = ? 1 2 git clone https://github.com/ ${ GH_USER } /scully cd mulder JX Import 1 jx import Hints Helm repository You might need to know the urls for the Helm repository. Internal For interaction within the cluster. 1 kubectl get svc -n jx You should see something like this: 1 2 3 4 5 6 7 8 9 10 NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE heapster ClusterIP 10 .23.241.52 none 8082 /TCP 12d jenkins ClusterIP 10 .23.253.164 none 8080 /TCP 12d jenkins-agent ClusterIP 10 .23.249.153 none 50000 /TCP 12d jenkins-x-chartmuseum ClusterIP 10 .23.252.63 none 8080 /TCP 12d jenkins-x-docker-registry ClusterIP 10 .23.250.118 none 5000 /TCP 12d jenkins-x-mongodb ClusterIP 10 .23.250.218 none 27017 /TCP 12d jenkins-x-monocular-api ClusterIP 10 .23.249.13 none 80 /TCP 12d jenkins-x-monocular-prerender ClusterIP 10 .23.242.95 none 80 /TCP 12d jenkins-x-monocular-ui ClusterIP 10 .23.243.161 none 80 /TCP 12d This name and the port number, is the chart repository you can use for Mulder internally. External For testing locally, to make sure the mulder chart exists in your ChartMuseum repository. 1 jx get urls Replace the ? below, with the url of ChartMusem you got via jx get urls . 1 CM_ADDR = ? 1 2 3 helm repo add jx-workshop $CM_ADDR helm repo update helm search mulder Build Run To build and run the application, you can take a look at the README.md . https://github.com/the-jenkins-x-files/scully You can also use the npm tools. 1 2 npm install npm run build 1 2 npm install -g serve # might have to be run with SUDO serve -s build Get a quote in the UI Click on Scully's \"voice box\" to get a quote. Use environment variables For the Mulder server url, you can set an environment variable. There are several places you can set this, in values.yaml or in templates/deployment.yaml . Cannot find module '../lib/utils/unsupported.js' If you run into this error: 1 2 3 4 5 6 internal/modules/cjs/loader.js:583 throw err ; ^ Error: Cannot find module ../lib/utils/unsupported.js at Function.Module._resolveFilename ( internal/modules/cjs/loader.js:581:15 ) The easiest resolution, is to reinstall node via homebrew (assuming MacOS). 1 2 sudo rm -rf /usr/local/lib/node_modules/npm brew reinstall node","title":"Scully"},{"location":"xfiles/scully/#scully","text":"","title":"Scully"},{"location":"xfiles/scully/#steps","text":"fork clone jx import fix Dockerfile add Mulder as dependency helm repo (see hints below if you doubt) fix url to Mulder","title":"Steps"},{"location":"xfiles/scully/#fork-clone","text":"Go to github.com/the-jenkins-x-files/scully and fork the repository to your own account. Then, go to the directory you want the source code to be. 1 GH_USER = ? 1 2 git clone https://github.com/ ${ GH_USER } /scully cd mulder","title":"Fork &amp; Clone"},{"location":"xfiles/scully/#jx-import","text":"1 jx import","title":"JX Import"},{"location":"xfiles/scully/#hints","text":"","title":"Hints"},{"location":"xfiles/scully/#helm-repository","text":"You might need to know the urls for the Helm repository.","title":"Helm repository"},{"location":"xfiles/scully/#internal","text":"For interaction within the cluster. 1 kubectl get svc -n jx You should see something like this: 1 2 3 4 5 6 7 8 9 10 NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE heapster ClusterIP 10 .23.241.52 none 8082 /TCP 12d jenkins ClusterIP 10 .23.253.164 none 8080 /TCP 12d jenkins-agent ClusterIP 10 .23.249.153 none 50000 /TCP 12d jenkins-x-chartmuseum ClusterIP 10 .23.252.63 none 8080 /TCP 12d jenkins-x-docker-registry ClusterIP 10 .23.250.118 none 5000 /TCP 12d jenkins-x-mongodb ClusterIP 10 .23.250.218 none 27017 /TCP 12d jenkins-x-monocular-api ClusterIP 10 .23.249.13 none 80 /TCP 12d jenkins-x-monocular-prerender ClusterIP 10 .23.242.95 none 80 /TCP 12d jenkins-x-monocular-ui ClusterIP 10 .23.243.161 none 80 /TCP 12d This name and the port number, is the chart repository you can use for Mulder internally.","title":"Internal"},{"location":"xfiles/scully/#external","text":"For testing locally, to make sure the mulder chart exists in your ChartMuseum repository. 1 jx get urls Replace the ? below, with the url of ChartMusem you got via jx get urls . 1 CM_ADDR = ? 1 2 3 helm repo add jx-workshop $CM_ADDR helm repo update helm search mulder","title":"External"},{"location":"xfiles/scully/#build-run","text":"To build and run the application, you can take a look at the README.md . https://github.com/the-jenkins-x-files/scully You can also use the npm tools. 1 2 npm install npm run build 1 2 npm install -g serve # might have to be run with SUDO serve -s build","title":"Build &amp; Run"},{"location":"xfiles/scully/#get-a-quote-in-the-ui","text":"Click on Scully's \"voice box\" to get a quote.","title":"Get a quote in the UI"},{"location":"xfiles/scully/#use-environment-variables","text":"For the Mulder server url, you can set an environment variable. There are several places you can set this, in values.yaml or in templates/deployment.yaml .","title":"Use environment variables"},{"location":"xfiles/scully/#cannot-find-module-libutilsunsupportedjs","text":"If you run into this error: 1 2 3 4 5 6 internal/modules/cjs/loader.js:583 throw err ; ^ Error: Cannot find module ../lib/utils/unsupported.js at Function.Module._resolveFilename ( internal/modules/cjs/loader.js:581:15 ) The easiest resolution, is to reinstall node via homebrew (assuming MacOS). 1 2 sudo rm -rf /usr/local/lib/node_modules/npm brew reinstall node","title":"Cannot find module '../lib/utils/unsupported.js'"}]}